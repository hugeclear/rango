#!/usr/bin/env python3
"""
CFS-Chameleonç·åˆè©•ä¾¡å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Direction vectorsé©ç”¨æ¸ˆã¿ã‚·ã‚¹ãƒ†ãƒ ã®å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã›è©•ä¾¡
"""

import subprocess
import json
import time
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class CFSComprehensiveEvaluator:
    def __init__(self):
        # å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        self.hook_layers = ["model.layers.14.mlp", "model.layers.16.mlp", "model.layers.18.mlp"]
        self.alpha_personal_values = [0.05, 0.1, 0.2, 0.3]
        self.rank_reduction_values = [8, 16, 32]
        self.alpha_general = -0.05  # å›ºå®šå€¤
        self.config_path = "cfs_config.yaml"
        
        # çµæœä¿å­˜ç”¨
        self.results = []
        self.evaluation_count = 0
        self.total_conditions = len(self.hook_layers) * len(self.alpha_personal_values) * len(self.rank_reduction_values)
        
        logger.info(f"ğŸ§ª CFS-Chameleonç·åˆè©•ä¾¡é–‹å§‹")
        logger.info(f"ğŸ“Š ç·å®Ÿé¨“æ¡ä»¶æ•°: {self.total_conditions}")
        logger.info(f"ğŸ”§ Hook layers: {self.hook_layers}")
        logger.info(f"ğŸ“ˆ Alpha personal: {self.alpha_personal_values}")
        logger.info(f"ğŸ¯ Rank reduction: {self.rank_reduction_values}")
    
    def run_single_evaluation(self, hook_layer: str, alpha_p: float, rank_reduction: int) -> Dict[str, Any]:
        """å˜ä¸€æ¡ä»¶ã§ã®è©•ä¾¡å®Ÿè¡Œ"""
        self.evaluation_count += 1
        logger.info(f"ğŸ”„ å®Ÿé¨“ {self.evaluation_count}/{self.total_conditions}: hook={hook_layer}, Î±_p={alpha_p}, rank={rank_reduction}")
        
        # ã‚³ãƒãƒ³ãƒ‰æ§‹ç¯‰
        cmd = [
            "python", "lampqa_cfs_benchmark.py",
            "--compare_modes",
            "--use_collaboration", 
            f"--config={self.config_path}",
            f"--hook_layer={hook_layer}",
            f"--alpha_p={alpha_p}",
            f"--alpha_g={self.alpha_general}",
            f"--rank_reduction={rank_reduction}"
        ]
        
        try:
            # è©•ä¾¡å®Ÿè¡Œ
            start_time = time.time()
            env = dict(subprocess.os.environ)
            env["CUDA_VISIBLE_DEVICES"] = "0"
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=600,  # 10åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                shell=False,
                env=env
            )
            execution_time = time.time() - start_time
            
            if result.returncode != 0:
                raise subprocess.CalledProcessError(result.returncode, cmd, result.stdout, result.stderr)
            
            # æœ€æ–°çµæœãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
            results_dir = Path("lampqa_evaluation_results")
            if not results_dir.exists():
                raise FileNotFoundError("Results directory not found")
            
            latest_result_file = max(results_dir.glob("lampqa_comparison_*.json"), key=lambda x: x.stat().st_mtime)
            
            with open(latest_result_file, 'r') as f:
                evaluation_data = json.load(f)
            
            # çµæœæŠ½å‡º
            legacy = evaluation_data.get("legacy_chameleon", {})
            cfs = evaluation_data.get("cfs_chameleon", {})
            improvements = evaluation_data.get("improvement_metrics", {})
            significance = evaluation_data.get("statistical_significance", {})
            
            result_data = {
                # å®Ÿé¨“æ¡ä»¶
                "hook_layer": hook_layer,
                "alpha_p": alpha_p,
                "rank_reduction": rank_reduction,
                "execution_time_total": round(execution_time, 2),
                
                # Legacy Chameleonçµæœ
                "legacy_rouge_l": round(legacy.get("rouge_l", 0.0), 4),
                "legacy_bleu": round(legacy.get("bleu_score", 0.0), 4),
                "legacy_bert_score": round(legacy.get("bert_score_f1", 0.0), 4),
                "legacy_inference_time": round(legacy.get("inference_time", 0.0), 4),
                
                # CFS-Chameleonçµæœ
                "cfs_rouge_l": round(cfs.get("rouge_l", 0.0), 4),
                "cfs_bleu": round(cfs.get("bleu_score", 0.0), 4),
                "cfs_bert_score": round(cfs.get("bert_score_f1", 0.0), 4),
                "cfs_inference_time": round(cfs.get("inference_time", 0.0), 4),
                "cfs_pool_utilization": round(cfs.get("pool_utilization", 0.0) * 100, 2),  # %è¡¨ç¤º
                
                # æ”¹å–„æŒ‡æ¨™
                "rouge_l_improvement": round(improvements.get("rouge_l_improvement", 0.0), 4),
                "bleu_improvement": round(improvements.get("bleu_improvement", 0.0), 4),
                "bert_improvement": round(improvements.get("bert_improvement", 0.0), 4),
                "speed_improvement": round(improvements.get("speed_improvement", 0.0), 4),
                
                # çµ±è¨ˆçš„æœ‰æ„æ€§
                "p_value": round(significance.get("p_value", 1.0), 6),
                "is_significant": significance.get("is_significant", False),
                
                # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
                "status": "SUCCESS"
            }
            
            logger.info(f"âœ… å®Ÿé¨“å®Œäº†: ROUGE-L={result_data['cfs_rouge_l']}, BLEU={result_data['cfs_bleu']}, BERTScore={result_data['cfs_bert_score']}")
            return result_data
            
        except subprocess.TimeoutExpired:
            logger.error(f"âŒ å®Ÿé¨“ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {hook_layer}, Î±_p={alpha_p}, rank={rank_reduction}")
            return self._create_error_result(hook_layer, alpha_p, rank_reduction, "TIMEOUT")
            
        except Exception as e:
            logger.error(f"âŒ å®Ÿé¨“ã‚¨ãƒ©ãƒ¼: {hook_layer}, Î±_p={alpha_p}, rank={rank_reduction} - {e}")
            return self._create_error_result(hook_layer, alpha_p, rank_reduction, "ERROR")
    
    def _create_error_result(self, hook_layer: str, alpha_p: float, rank_reduction: int, status: str) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼æ™‚ã®çµæœãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
        return {
            "hook_layer": hook_layer,
            "alpha_p": alpha_p,
            "rank_reduction": rank_reduction,
            "execution_time_total": 0.0,
            "legacy_rouge_l": 0.0,
            "legacy_bleu": 0.0,
            "legacy_bert_score": 0.0,
            "legacy_inference_time": 0.0,
            "cfs_rouge_l": 0.0,
            "cfs_bleu": 0.0,
            "cfs_bert_score": 0.0,
            "cfs_inference_time": 0.0,
            "cfs_pool_utilization": 0.0,
            "rouge_l_improvement": 0.0,
            "bleu_improvement": 0.0,
            "bert_improvement": 0.0,
            "speed_improvement": 0.0,
            "p_value": 1.0,
            "is_significant": False,
            "status": status
        }
    
    def run_comprehensive_evaluation(self):
        """ç·åˆè©•ä¾¡å®Ÿè¡Œ"""
        logger.info("ğŸš€ ç·å½“ã‚Šå®Ÿé¨“é–‹å§‹")
        
        total_start_time = time.time()
        
        for hook_layer in self.hook_layers:
            for alpha_p in self.alpha_personal_values:
                for rank_reduction in self.rank_reduction_values:
                    result = self.run_single_evaluation(hook_layer, alpha_p, rank_reduction)
                    self.results.append(result)
                    
                    # é€”ä¸­çµŒéè¡¨ç¤º
                    if self.evaluation_count % 5 == 0:
                        elapsed = time.time() - total_start_time
                        remaining = (elapsed / self.evaluation_count) * (self.total_conditions - self.evaluation_count)
                        logger.info(f"ğŸ“Š é€²æ—: {self.evaluation_count}/{self.total_conditions} ({self.evaluation_count/self.total_conditions*100:.1f}%)")
                        logger.info(f"â±ï¸  çµŒéæ™‚é–“: {elapsed/60:.1f}åˆ†, æ¨å®šæ®‹ã‚Šæ™‚é–“: {remaining/60:.1f}åˆ†")
        
        total_execution_time = time.time() - total_start_time
        logger.info(f"ğŸ‰ å…¨å®Ÿé¨“å®Œäº†! ç·å®Ÿè¡Œæ™‚é–“: {total_execution_time/60:.1f}åˆ†")
        
        # çµæœä¿å­˜ã¨åˆ†æ
        self.save_results()
        self.analyze_results()
    
    def save_results(self):
        """çµæœä¿å­˜"""
        # DataFrameä½œæˆ
        df = pd.DataFrame(self.results)
        
        # CSVä¿å­˜
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        csv_path = f"cfs_comprehensive_evaluation_{timestamp}.csv"
        df.to_csv(csv_path, index=False)
        logger.info(f"ğŸ“ çµæœä¿å­˜: {csv_path}")
        
        # JSONä¿å­˜
        json_path = f"cfs_comprehensive_evaluation_{timestamp}.json"
        with open(json_path, 'w') as f:
            json.dump(self.results, f, indent=2)
        logger.info(f"ğŸ“ çµæœä¿å­˜: {json_path}")
        
        self.csv_path = csv_path
        self.json_path = json_path
    
    def analyze_results(self):
        """çµæœåˆ†æã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        df = pd.DataFrame(self.results)
        
        # æˆåŠŸã—ãŸå®Ÿé¨“ã®ã¿åˆ†æ
        success_df = df[df['status'] == 'SUCCESS'].copy()
        
        if len(success_df) == 0:
            logger.error("âŒ æˆåŠŸã—ãŸå®Ÿé¨“ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        logger.info("ğŸ“Š çµæœåˆ†æé–‹å§‹")
        
        # å„æŒ‡æ¨™ã§ã®æœ€è‰¯æ¡ä»¶
        best_rouge_l = success_df.loc[success_df['cfs_rouge_l'].idxmax()]
        best_bleu = success_df.loc[success_df['cfs_bleu'].idxmax()]
        best_bert_score = success_df.loc[success_df['cfs_bert_score'].idxmax()]
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ3æŒ‡æ¨™ã®å¹³å‡æ”¹å–„ç‡ï¼‰
        success_df['total_improvement'] = (
            success_df['rouge_l_improvement'] + 
            success_df['bleu_improvement'] + 
            success_df['bert_improvement']
        ) / 3
        
        best_overall = success_df.loc[success_df['total_improvement'].idxmax()]
        
        # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
        print("\n" + "="*80)
        print("ğŸ¯ CFS-CHAMELEON ç·åˆè©•ä¾¡çµæœãƒ¬ãƒãƒ¼ãƒˆ")
        print("="*80)
        
        print("\nğŸ“Š å„æŒ‡æ¨™ã§ã®æœ€è‰¯æ¡ä»¶:")
        print(f"ğŸ† ROUGE-Læœ€é«˜å€¤: {best_rouge_l['cfs_rouge_l']:.4f}")
        print(f"   æ¡ä»¶: hook={best_rouge_l['hook_layer']}, Î±_p={best_rouge_l['alpha_p']}, rank={best_rouge_l['rank_reduction']}")
        
        print(f"ğŸ† BLEUæœ€é«˜å€¤: {best_bleu['cfs_bleu']:.4f}")
        print(f"   æ¡ä»¶: hook={best_bleu['hook_layer']}, Î±_p={best_bleu['alpha_p']}, rank={best_bleu['rank_reduction']}")
        
        print(f"ğŸ† BERTScoreæœ€é«˜å€¤: {best_bert_score['cfs_bert_score']:.4f}")
        print(f"   æ¡ä»¶: hook={best_bert_score['hook_layer']}, Î±_p={best_bert_score['alpha_p']}, rank={best_bert_score['rank_reduction']}")
        
        print(f"\nğŸ¯ ç·åˆæœ€é©æ¡ä»¶ï¼ˆå¹³å‡æ”¹å–„ç‡æœ€å¤§ï¼‰:")
        print(f"   Hook Layer: {best_overall['hook_layer']}")
        print(f"   Alpha Personal: {best_overall['alpha_p']}")
        print(f"   Rank Reduction: {best_overall['rank_reduction']}")
        print(f"   å¹³å‡æ”¹å–„ç‡: {best_overall['total_improvement']:.4f}%")
        print(f"   ROUGE-L: {best_overall['cfs_rouge_l']:.4f}")
        print(f"   BLEU: {best_overall['cfs_bleu']:.4f}")
        print(f"   BERTScore: {best_overall['cfs_bert_score']:.4f}")
        print(f"   æ¨è«–æ™‚é–“: {best_overall['cfs_inference_time']:.4f}ç§’")
        print(f"   ãƒ—ãƒ¼ãƒ«åˆ©ç”¨ç‡: {best_overall['cfs_pool_utilization']:.2f}%")
        
        # çµ±è¨ˆã‚µãƒãƒªãƒ¼
        print(f"\nğŸ“ˆ çµ±è¨ˆã‚µãƒãƒªãƒ¼:")
        print(f"   æˆåŠŸå®Ÿé¨“æ•°: {len(success_df)}/{len(df)}")
        print(f"   ROUGE-Lå¹³å‡: {success_df['cfs_rouge_l'].mean():.4f} (Ïƒ={success_df['cfs_rouge_l'].std():.4f})")
        print(f"   BLEUå¹³å‡: {success_df['cfs_bleu'].mean():.4f} (Ïƒ={success_df['cfs_bleu'].std():.4f})")
        print(f"   BERTScoreå¹³å‡: {success_df['cfs_bert_score'].mean():.4f} (Ïƒ={success_df['cfs_bert_score'].std():.4f})")
        
        # è€ƒå¯Ÿ
        print(f"\nğŸ’¡ è€ƒå¯Ÿ:")
        hook_analysis = success_df.groupby('hook_layer')['total_improvement'].mean().sort_values(ascending=False)
        alpha_analysis = success_df.groupby('alpha_p')['total_improvement'].mean().sort_values(ascending=False)
        rank_analysis = success_df.groupby('rank_reduction')['total_improvement'].mean().sort_values(ascending=False)
        
        print(f"   æœ€è‰¯Hookå±¤: {hook_analysis.index[0]} (å¹³å‡æ”¹å–„ç‡: {hook_analysis.iloc[0]:.4f}%)")
        print(f"   æœ€è‰¯Alphaå€¤: {alpha_analysis.index[0]} (å¹³å‡æ”¹å–„ç‡: {alpha_analysis.iloc[0]:.4f}%)")
        print(f"   æœ€è‰¯Rankå€¤: {rank_analysis.index[0]} (å¹³å‡æ”¹å–„ç‡: {rank_analysis.iloc[0]:.4f}%)")
        
        print("="*80)

if __name__ == "__main__":
    evaluator = CFSComprehensiveEvaluator()
    evaluator.run_comprehensive_evaluation()