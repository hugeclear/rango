#!/usr/bin/env python3
"""
Chameleon Alpha Parameter Optimization
=====================================

Chameleonã®Î±å€¤ã‚’è‡ªå‹•çš„ã«æœ€é©åŒ–ã—ã€æœ€é«˜ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è¦‹ã¤ã‘ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
import time
import logging
import itertools
import subprocess
from pathlib import Path
from typing import Dict, List, Tuple, Any
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('alpha_optimization.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class AlphaOptimizer:
    """Chameleonã®Î±å€¤æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, output_dir: str = "optimization_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.results = []
        
    def generate_alpha_combinations(self) -> List[Tuple[float, float]]:
        """Î±å€¤ã®çµ„ã¿åˆã‚ã›ã‚’ç”Ÿæˆ"""
        
        # Phase 1: ç²—ã„æ¢ç´¢
        alpha_personal_coarse = [0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]
        alpha_neutral_coarse = [-0.1, -0.5, -0.8, -1.0, -1.5, -2.0, -2.5]
        
        # Phase 2: ç´°ã‹ã„æ¢ç´¢ï¼ˆæœ‰æœ›ãªç¯„å›²ï¼‰
        alpha_personal_fine = np.arange(1.0, 2.1, 0.2)  # 1.0-2.0ã‚’0.2åˆ»ã¿
        alpha_neutral_fine = np.arange(-1.5, -0.4, 0.2)  # -1.5--0.5ã‚’0.2åˆ»ã¿
        
        # Phase 3: è¶…ç´°ã‹ã„æ¢ç´¢ï¼ˆæœ€é©å€¤å‘¨è¾ºï¼‰
        alpha_personal_ultra = np.arange(1.4, 1.81, 0.1)  # 1.4-1.8ã‚’0.1åˆ»ã¿
        alpha_neutral_ultra = np.arange(-1.2, -0.69, 0.1)  # -1.2--0.7ã‚’0.1åˆ»ã¿
        
        combinations = []
        
        # Phase 1: ç²—ã„æ¢ç´¢
        for ap, an in itertools.product(alpha_personal_coarse, alpha_neutral_coarse):
            combinations.append((ap, an, "coarse"))
        
        # Phase 2: ç´°ã‹ã„æ¢ç´¢
        for ap, an in itertools.product(alpha_personal_fine, alpha_neutral_fine):
            combinations.append((round(ap, 2), round(an, 2), "fine"))
        
        # Phase 3: è¶…ç´°ã‹ã„æ¢ç´¢
        for ap, an in itertools.product(alpha_personal_ultra, alpha_neutral_ultra):
            combinations.append((round(ap, 2), round(an, 2), "ultra"))
        
        # é‡è¤‡ã‚’é™¤å»
        unique_combinations = []
        seen = set()
        for ap, an, phase in combinations:
            key = (round(ap, 2), round(an, 2))
            if key not in seen:
                seen.add(key)
                unique_combinations.append((ap, an, phase))
        
        logger.info(f"Generated {len(unique_combinations)} unique alpha combinations")
        return unique_combinations
    
    def modify_config(self, alpha_personal: float, alpha_neutral: float) -> str:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚çš„ã«å¤‰æ›´"""
        config_path = "temp_alpha_config.py"
        
        config_content = f'''
# Temporary Alpha Configuration for Optimization
CHAMELEON_CONFIG = {{
    "alpha_personal": {alpha_personal},
    "alpha_neutral": {alpha_neutral},
    "target_layers": ["model.layers.16"],
    "max_length": 50
}}

# LaMP evaluation config
LAMP_CONFIG = {{
    "mode": "demo",
    "skip_checks": True,
    "max_samples": 10  # é«˜é€ŸåŒ–ã®ãŸã‚å°‘æ•°ã‚µãƒ³ãƒ—ãƒ«
}}
'''
        
        with open(config_path, 'w') as f:
            f.write(config_content)
        
        return config_path
    
    def run_evaluation(self, alpha_personal: float, alpha_neutral: float) -> Dict[str, Any]:
        """æŒ‡å®šã—ãŸÎ±å€¤ã§è©•ä¾¡ã‚’å®Ÿè¡Œ"""
        
        logger.info(f"Testing Î±_p={alpha_personal:.2f}, Î±_n={alpha_neutral:.2f}")
        
        try:
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚å¤‰æ›´
            config_path = self.modify_config(alpha_personal, alpha_neutral)
            
            # è©•ä¾¡å®Ÿè¡Œ
            start_time = time.time()
            
            # run_evaluation.pyã‚’å®Ÿè¡Œï¼ˆÎ±å€¤ã‚’å¼•æ•°ã¨ã—ã¦æ¸¡ã™ï¼‰
            cmd = [
                "python", "run_evaluation.py", 
                "--mode", "demo", 
                "--skip-checks",
                "--alpha-personal", str(alpha_personal),
                "--alpha-neutral", str(alpha_neutral),
                "--output-dir", str(self.output_dir / f"alpha_p{alpha_personal}_n{alpha_neutral}")
            ]
            
            # ä»£æ›¿å®Ÿè¡Œæ–¹æ³•ï¼ˆå¼•æ•°å¯¾å¿œã—ã¦ã„ãªã„å ´åˆï¼‰
            result = self.run_evaluation_alternative(alpha_personal, alpha_neutral)
            
            execution_time = time.time() - start_time
            
            # çµæœã‚’è§£æ
            if result:
                result.update({
                    'alpha_personal': alpha_personal,
                    'alpha_neutral': alpha_neutral,
                    'execution_time': execution_time,
                    'timestamp': time.time()
                })
                
                logger.info(f"Î±_p={alpha_personal:.2f}, Î±_n={alpha_neutral:.2f} -> "
                          f"Accuracy: {result.get('accuracy', 0):.4f}, "
                          f"Improvement: {result.get('improvement_rate', 0):.1%}")
                
                return result
            else:
                logger.warning(f"Failed to get results for Î±_p={alpha_personal:.2f}, Î±_n={alpha_neutral:.2f}")
                return None
                
        except Exception as e:
            logger.error(f"Error in evaluation Î±_p={alpha_personal:.2f}, Î±_n={alpha_neutral:.2f}: {e}")
            return None
        
        finally:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if Path(config_path).exists():
                Path(config_path).unlink()
    
    def run_evaluation_alternative(self, alpha_personal: float, alpha_neutral: float) -> Dict[str, Any]:
        """ä»£æ›¿å®Ÿè¡Œæ–¹æ³•ï¼šç›´æ¥ChameleonEvaluatorã‚’å‘¼ã³å‡ºã—"""
        
        try:
            from chameleon_evaluator import ChameleonEvaluator
            
            # ä¸€æ™‚çš„ãªè¨­å®šã§è©•ä¾¡å™¨ã‚’åˆæœŸåŒ–
            evaluator = ChameleonEvaluator()
            
            # LaMP-2ãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã‚’ä½¿ã£ã¦é«˜é€Ÿè©•ä¾¡
            with open('chameleon_prime_personalization/data/raw/LaMP-2/merged.json', 'r') as f:
                data = json.load(f)
            
            # å°‘æ•°ã‚µãƒ³ãƒ—ãƒ«ã§é«˜é€Ÿãƒ†ã‚¹ãƒˆ
            test_samples = data[:5]  # æœ€åˆã®5ã‚µãƒ³ãƒ—ãƒ«ã®ã¿
            
            baseline_scores = []
            chameleon_scores = []
            
            for sample in test_samples:
                input_text = sample['input']
                
                # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç”Ÿæˆ
                baseline_response = evaluator.model.generate(
                    **evaluator.tokenizer(input_text, return_tensors="pt"),
                    max_new_tokens=20,
                    do_sample=False
                )
                baseline_text = evaluator.tokenizer.decode(baseline_response[0], skip_special_tokens=True)
                
                # Chameleonç”Ÿæˆ
                chameleon_text = evaluator.generate_with_chameleon(
                    input_text,
                    alpha_personal=alpha_personal,
                    alpha_neutral=alpha_neutral,
                    max_length=20
                )
                
                # ç°¡æ˜“ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆå®Ÿéš›ã®è©•ä¾¡æŒ‡æ¨™ã§ç½®ãæ›ãˆå¯èƒ½ï¼‰
                baseline_score = len(baseline_text.split())  # å˜èªæ•°ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“æŒ‡æ¨™
                chameleon_score = len(chameleon_text.split())
                
                baseline_scores.append(baseline_score)
                chameleon_scores.append(chameleon_score)
            
            # çµæœã®è¨ˆç®—
            baseline_avg = np.mean(baseline_scores)
            chameleon_avg = np.mean(chameleon_scores)
            improvement_rate = (chameleon_avg - baseline_avg) / baseline_avg if baseline_avg > 0 else 0
            
            return {
                'accuracy': chameleon_avg / (baseline_avg + chameleon_avg) if (baseline_avg + chameleon_avg) > 0 else 0,
                'baseline_score': baseline_avg,
                'chameleon_score': chameleon_avg,
                'improvement_rate': improvement_rate,
                'samples_tested': len(test_samples)
            }
            
        except Exception as e:
            logger.error(f"Alternative evaluation failed: {e}")
            return None
    
    def optimize(self, max_iterations: int = None, target_improvement: float = 0.25) -> Dict[str, Any]:
        """Î±å€¤ã®æœ€é©åŒ–ã‚’å®Ÿè¡Œ"""
        
        logger.info("ğŸš€ Starting Chameleon Alpha Optimization")
        logger.info(f"Target improvement: {target_improvement:.1%}")
        
        combinations = self.generate_alpha_combinations()
        
        if max_iterations:
            combinations = combinations[:max_iterations]
            logger.info(f"Limited to {max_iterations} iterations")
        
        best_result = None
        best_improvement = -float('inf')
        
        total_start_time = time.time()
        
        for i, (alpha_p, alpha_n, phase) in enumerate(combinations, 1):
            logger.info(f"[{i}/{len(combinations)}] Phase: {phase}")
            
            result = self.run_evaluation(alpha_p, alpha_n)
            
            if result:
                self.results.append(result)
                
                improvement = result.get('improvement_rate', 0)
                if improvement > best_improvement:
                    best_improvement = improvement
                    best_result = result
                    
                    logger.info(f"ğŸ¯ New best result! Î±_p={alpha_p:.2f}, Î±_n={alpha_n:.2f} -> {improvement:.1%}")
                    
                    # ç›®æ¨™é”æˆãƒã‚§ãƒƒã‚¯
                    if improvement >= target_improvement:
                        logger.info(f"ğŸ‰ Target improvement {target_improvement:.1%} achieved!")
                        break
            
            # é€²æ—ä¿å­˜
            if i % 10 == 0:
                self.save_intermediate_results()
        
        total_time = time.time() - total_start_time
        
        logger.info(f"âœ… Optimization completed in {total_time:.1f} seconds")
        logger.info(f"Best result: Î±_p={best_result['alpha_personal']:.2f}, "
                   f"Î±_n={best_result['alpha_neutral']:.2f} -> "
                   f"{best_result['improvement_rate']:.1%}")
        
        # æœ€çµ‚çµæœã‚’ä¿å­˜
        self.save_results(best_result, total_time)
        self.create_visualizations()
        
        return best_result
    
    def save_intermediate_results(self):
        """ä¸­é–“çµæœã‚’ä¿å­˜"""
        results_file = self.output_dir / "intermediate_results.json"
        with open(results_file, 'w') as f:
            json.dump(self.results, f, indent=2)
    
    def save_results(self, best_result: Dict[str, Any], total_time: float):
        """æœ€çµ‚çµæœã‚’ä¿å­˜"""
        
        summary = {
            'optimization_summary': {
                'best_result': best_result,
                'total_evaluations': len(self.results),
                'total_time': total_time,
                'timestamp': time.time()
            },
            'all_results': self.results
        }
        
        results_file = self.output_dir / "optimization_results.json"
        with open(results_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        logger.info(f"Results saved to {results_file}")
    
    def create_visualizations(self):
        """çµæœã®å¯è¦–åŒ–"""
        
        if not self.results:
            logger.warning("No results to visualize")
            return
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        alpha_p_values = [r['alpha_personal'] for r in self.results]
        alpha_n_values = [r['alpha_neutral'] for r in self.results]
        improvements = [r.get('improvement_rate', 0) for r in self.results]
        
        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆ
        plt.figure(figsize=(12, 8))
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒªãƒƒãƒ‰ã«æ•´ç†
        unique_alpha_p = sorted(set(alpha_p_values))
        unique_alpha_n = sorted(set(alpha_n_values))
        
        heatmap_data = np.full((len(unique_alpha_n), len(unique_alpha_p)), np.nan)
        
        for i, result in enumerate(self.results):
            p_idx = unique_alpha_p.index(result['alpha_personal'])
            n_idx = unique_alpha_n.index(result['alpha_neutral'])
            heatmap_data[n_idx, p_idx] = result.get('improvement_rate', 0) * 100  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆè¡¨ç¤º
        
        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æç”»
        plt.subplot(2, 2, 1)
        sns.heatmap(
            heatmap_data, 
            xticklabels=[f"{x:.1f}" for x in unique_alpha_p],
            yticklabels=[f"{x:.1f}" for x in unique_alpha_n],
            annot=True, 
            fmt='.1f',
            cmap='RdYlGn',
            center=0,
            cbar_kws={'label': 'Improvement (%)'}
        )
        plt.xlabel('Alpha Personal')
        plt.ylabel('Alpha Neutral')
        plt.title('Improvement Rate Heatmap')
        
        # æ•£å¸ƒå›³
        plt.subplot(2, 2, 2)
        scatter = plt.scatter(alpha_p_values, alpha_n_values, c=improvements, cmap='RdYlGn', s=50)
        plt.colorbar(scatter, label='Improvement Rate')
        plt.xlabel('Alpha Personal')
        plt.ylabel('Alpha Neutral')
        plt.title('Alpha Parameters vs Improvement')
        
        # æ”¹å–„ç‡åˆ†å¸ƒ
        plt.subplot(2, 2, 3)
        plt.hist(improvements, bins=20, alpha=0.7, color='skyblue')
        plt.xlabel('Improvement Rate')
        plt.ylabel('Frequency')
        plt.title('Distribution of Improvement Rates')
        plt.axvline(x=0.25, color='red', linestyle='--', label='Target (25%)')
        plt.legend()
        
        # ä¸Šä½çµæœ
        plt.subplot(2, 2, 4)
        top_results = sorted(self.results, key=lambda x: x.get('improvement_rate', 0), reverse=True)[:10]
        top_improvements = [r.get('improvement_rate', 0) * 100 for r in top_results]
        top_labels = [f"({r['alpha_personal']:.1f}, {r['alpha_neutral']:.1f})" for r in top_results]
        
        plt.barh(range(len(top_improvements)), top_improvements)
        plt.yticks(range(len(top_improvements)), top_labels)
        plt.xlabel('Improvement (%)')
        plt.title('Top 10 Alpha Combinations')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'optimization_results.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"Visualizations saved to {self.output_dir / 'optimization_results.png'}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    print("ğŸ¦ Chameleon Alpha Parameter Optimization")
    print("=" * 50)
    
    # æœ€é©åŒ–å™¨ã‚’åˆæœŸåŒ–
    optimizer = AlphaOptimizer()
    
    # æœ€é©åŒ–å®Ÿè¡Œ
    best_result = optimizer.optimize(
        max_iterations=100,  # æœ€å¤§100å›ã®ãƒ†ã‚¹ãƒˆ
        target_improvement=0.25  # 25%æ”¹å–„ã‚’ç›®æ¨™
    )
    
    if best_result:
        print("\nğŸ‰ Optimization Results:")
        print("=" * 50)
        print(f"Best Alpha Personal: {best_result['alpha_personal']:.2f}")
        print(f"Best Alpha Neutral: {best_result['alpha_neutral']:.2f}")
        print(f"Improvement Rate: {best_result['improvement_rate']:.1%}")
        print(f"Accuracy: {best_result.get('accuracy', 0):.4f}")
        print("\nğŸ“ Results saved in: optimization_results/")
        print("ğŸ“Š Visualizations: optimization_results/optimization_results.png")
    else:
        print("âŒ Optimization failed to find good parameters")

if __name__ == "__main__":
    main()
