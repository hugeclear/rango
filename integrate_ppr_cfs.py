#!/usr/bin/env python3
"""
integrate_ppr_cfs.py

GraphRAG Step-4: PPRå‡ºåŠ›ã‚’CFS-Chameleonå”èª¿é‡ã¿ä»˜ã‘ã«çµ±åˆ

PPRã‚¹ã‚³ã‚¢ (ppr_topk.parquet) ã¨ãƒ¦ãƒ¼ã‚¶åŸ‹ã‚è¾¼ã¿ (theta_p.npy/theta_n.npy) ã‹ã‚‰
å”èª¿ãƒ¦ãƒ¼ã‚¶ãƒ—ãƒ¼ãƒ«ã®é‡ã¿ä»˜ããƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚

Usage:
    python integrate_ppr_cfs.py \
      --ppr ./ppr_results/ppr_topk.parquet \
      --id_map ./ppr_results/id_map.json \
      --theta_p ./chameleon_prime_personalization/processed/LaMP-2/theta_p.npy \
      --theta_n ./chameleon_prime_personalization/processed/LaMP-2/theta_n.npy \
      --out ./graphrag_cfs_weights \
      --gamma 1.0 --beta 0.3 --min_ppr 1e-4 --min_cos 0.2 --topM 50

Input:
    - ppr_topk.parquet: PPRçµæœ (seed_id, node_id, score)
    - id_map.json: å¤–éƒ¨IDâ†”å†…éƒ¨ID ãƒãƒƒãƒ”ãƒ³ã‚°  
    - theta_p.npy/theta_n.npy: ãƒ¦ãƒ¼ã‚¶åŸ‹ã‚è¾¼ã¿
    
Output:
    - cfs_pool.parquet: (user_id, neighbor_id, weight)
    - meta.json: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š

Algorithm:
    weight = (ppr_score^gamma) * (cos_sim(u, v)^beta)
    ãƒ•ã‚£ãƒ«ã‚¿: ppr_score >= min_ppr, cos_sim >= min_cos
    åˆ¶é™: å„ãƒ¦ãƒ¼ã‚¶æœ€å¤§ topM è¿‘å‚

Author: Claude Code Assistant
Date: 2025-08-10
"""

import argparse
import json
import logging
import os
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class PPRCFSIntegrator:
    """PPRå‡ºåŠ›ã‚’CFSå”èª¿é‡ã¿ä»˜ã‘ã«çµ±åˆã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, 
                 gamma: float = 1.0,
                 beta: float = 0.3,
                 min_ppr: float = 1e-4,
                 min_cos: float = 0.2,
                 top_m: int = 50,
                 diversity_enabled: bool = False,
                 max_per_cluster: int = 10,
                 random_seed: int = 42):
        """
        Args:
            gamma: PPRã‚¹ã‚³ã‚¢ã®æŒ‡æ•° weight = ppr_score^gamma
            beta: ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®æŒ‡æ•°  
            min_ppr: PPRã‚¹ã‚³ã‚¢æœ€å°é–¾å€¤
            min_cos: ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦æœ€å°é–¾å€¤
            top_m: å„ãƒ¦ãƒ¼ã‚¶ã®æœ€å¤§è¿‘å‚æ•°
            diversity_enabled: å¤šæ§˜æ€§æ­£å‰‡åŒ–ã®æœ‰åŠ¹åŒ–
            max_per_cluster: ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®æœ€å¤§è¿‘å‚æ•°
            random_seed: ä¹±æ•°ã‚·ãƒ¼ãƒ‰
        """
        self.gamma = gamma
        self.beta = beta
        self.min_ppr = min_ppr
        self.min_cos = min_cos
        self.top_m = top_m
        self.diversity_enabled = diversity_enabled
        self.max_per_cluster = max_per_cluster
        self.random_seed = random_seed
        
        # ä¹±æ•°ã‚·ãƒ¼ãƒ‰è¨­å®š
        np.random.seed(random_seed)
        
        logger.info(f"ğŸ”§ PPRCFSIntegratoråˆæœŸåŒ–å®Œäº†")
        logger.info(f"   gamma={gamma}, beta={beta}")
        logger.info(f"   min_ppr={min_ppr}, min_cos={min_cos}")
        logger.info(f"   top_m={top_m}, diversity={diversity_enabled}")
        
    def load_ppr_data(self, ppr_path: str, id_map: Dict[int, str]) -> pd.DataFrame:
        """PPRãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€æ–‡å­—åˆ—IDã‚’å†…éƒ¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°"""
        logger.info(f"ğŸ“‚ PPRãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {ppr_path}")
        
        if ppr_path.endswith('.parquet'):
            ppr_df = pd.read_parquet(ppr_path)
        else:
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å ´åˆã€ppr_topk.parquetã‚’æ¢ã™
            ppr_file = Path(ppr_path) / "ppr_topk.parquet"
            if not ppr_file.exists():
                raise FileNotFoundError(f"PPRãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {ppr_file}")
            ppr_df = pd.read_parquet(ppr_file)
        
        # å¿…é ˆã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯
        required_cols = ['seed_id', 'node_id', 'score']
        missing_cols = [col for col in required_cols if col not in ppr_df.columns]
        if missing_cols:
            raise ValueError(f"å¿…é ˆã‚«ãƒ©ãƒ ãŒä¸è¶³: {missing_cols}")
        
        # IDå¤‰æ›ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆï¼ˆå¤–éƒ¨IDæ–‡å­—åˆ— â†’ å†…éƒ¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰
        reverse_id_map = {v: k for k, v in id_map.items()} if id_map else {}
        
        def extract_user_id(id_str):
            """æ–‡å­—åˆ—IDã‹ã‚‰user_idã‚’æŠ½å‡º"""
            try:
                id_dict = eval(id_str)
                if isinstance(id_dict, dict) and 'user_id' in id_dict:
                    return id_dict['user_id']
                return None
            except:
                return None
        
        def map_to_internal_id(id_str):
            """å¤–éƒ¨IDæ–‡å­—åˆ—ã‚’å†…éƒ¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°"""
            user_id = extract_user_id(id_str)
            if user_id is None:
                return None
            return reverse_id_map.get(user_id, None)
        
        logger.info("ğŸ”„ PPR IDå¤‰æ›ä¸­...")
        
        # IDå¤‰æ›
        ppr_df['seed_internal_id'] = ppr_df['seed_id'].apply(map_to_internal_id)
        ppr_df['node_internal_id'] = ppr_df['node_id'].apply(map_to_internal_id)
        
        # å¤‰æ›ã§ããªã‹ã£ãŸã‚¨ãƒ³ãƒˆãƒªã‚’é™¤å»
        original_size = len(ppr_df)
        ppr_df = ppr_df.dropna(subset=['seed_internal_id', 'node_internal_id'])
        ppr_df['seed_internal_id'] = ppr_df['seed_internal_id'].astype(int)
        ppr_df['node_internal_id'] = ppr_df['node_internal_id'].astype(int)
        id_mapped_size = len(ppr_df)
        
        # PPRã‚¹ã‚³ã‚¢ãƒ•ã‚£ãƒ«ã‚¿
        ppr_df = ppr_df[ppr_df['score'] >= self.min_ppr]
        filtered_size = len(ppr_df)
        
        logger.info(f"âœ… PPRãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
        logger.info(f"   ç·ã‚¨ãƒ³ãƒˆãƒªæ•°: {original_size:,}")
        logger.info(f"   IDå¤‰æ›å¾Œ: {id_mapped_size:,}")
        logger.info(f"   ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {filtered_size:,} (min_ppr={self.min_ppr})")
        logger.info(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯seed: {ppr_df['seed_internal_id'].nunique():,}")
        logger.info(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯node: {ppr_df['node_internal_id'].nunique():,}")
        logger.info(f"   ã‚¹ã‚³ã‚¢ç¯„å›²: [{ppr_df['score'].min():.6f}, {ppr_df['score'].max():.6f}]")
        
        return ppr_df
    
    def load_id_mapping(self, id_map_path: Optional[str]) -> Dict[int, str]:
        """ID ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’èª­ã¿è¾¼ã¿ï¼ˆå†…éƒ¨ID â†’ å¤–éƒ¨IDï¼‰"""
        if id_map_path and os.path.exists(id_map_path):
            logger.info(f"ğŸ“‚ IDãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿: {id_map_path}")
            with open(id_map_path, 'r') as f:
                id_map_data = json.load(f)
            
            # å½¢å¼ã«å¿œã˜ã¦å¤‰æ›
            if isinstance(id_map_data, dict):
                if 'id_to_idx' in id_map_data:
                    # LaMPå½¢å¼: {"id_to_idx": {"{'user_id': '100', 'n_docs': 1}": 0, ...}}
                    # å¤–éƒ¨IDæ–‡å­—åˆ—ã‹ã‚‰å®Ÿéš›ã®user_idã‚’æŠ½å‡º
                    id_map = {}
                    for external_id_str, internal_idx in id_map_data['id_to_idx'].items():
                        try:
                            # æ–‡å­—åˆ—ã‚’è©•ä¾¡ã—ã¦user_idã‚’æŠ½å‡º
                            id_dict = eval(external_id_str)
                            if isinstance(id_dict, dict) and 'user_id' in id_dict:
                                user_id = id_dict['user_id']
                                id_map[internal_idx] = user_id
                            else:
                                logger.warning(f"âš ï¸ IDãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¸æ­£: {external_id_str}")
                        except Exception as e:
                            logger.warning(f"âš ï¸ IDè§£æã‚¨ãƒ©ãƒ¼: {external_id_str} - {e}")
                            continue
                elif 'id_to_index' in id_map_data:
                    # æ¨™æº–å½¢å¼: {external_id: internal_index} â†’ {internal_index: external_id}
                    id_map = {v: k for k, v in id_map_data['id_to_index'].items()}
                else:
                    # ç›´æ¥è¾æ›¸å½¢å¼ï¼ˆæ•°å€¤ã‚­ãƒ¼ã‚’intã«å¤‰æ›ï¼‰
                    id_map = {}
                    for k, v in id_map_data.items():
                        try:
                            id_map[int(k)] = str(v)
                        except ValueError:
                            logger.warning(f"âš ï¸ æ•°å€¤å¤‰æ›ä¸å¯: {k}")
                            continue
            else:
                raise ValueError(f"IDãƒãƒƒãƒ”ãƒ³ã‚°å½¢å¼ãŒä¸æ­£: {type(id_map_data)}")
                
            logger.info(f"âœ… IDãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿å®Œäº†: {len(id_map):,}ã‚¨ãƒ³ãƒˆãƒª")
        else:
            logger.warning("âš ï¸ IDãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãªã— - å†…éƒ¨IDã‚’ãã®ã¾ã¾ä½¿ç”¨")
            id_map = {}
        
        return id_map
    
    def load_user_embeddings(self, user_embeddings_path: str) -> np.ndarray:
        """ãƒ¦ãƒ¼ã‚¶åŸ‹ã‚è¾¼ã¿ã‚’èª­ã¿è¾¼ã¿"""
        if not os.path.exists(user_embeddings_path):
            raise FileNotFoundError(f"ãƒ¦ãƒ¼ã‚¶åŸ‹ã‚è¾¼ã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {user_embeddings_path}")
        
        logger.info(f"ğŸ“‚ ãƒ¦ãƒ¼ã‚¶åŸ‹ã‚è¾¼ã¿èª­ã¿è¾¼ã¿: {user_embeddings_path}")
        embeddings = np.load(user_embeddings_path).astype(np.float32)
        
        logger.info(f"âœ… ãƒ¦ãƒ¼ã‚¶åŸ‹ã‚è¾¼ã¿èª­ã¿è¾¼ã¿å®Œäº†")
        logger.info(f"   å½¢çŠ¶: {embeddings.shape}")
        logger.info(f"   ãƒ‡ãƒ¼ã‚¿å‹: {embeddings.dtype}")
        logger.info(f"   å€¤ç¯„å›²: [{embeddings.min():.4f}, {embeddings.max():.4f}]")
        
        # 2æ¬¡å…ƒãƒã‚§ãƒƒã‚¯
        if len(embeddings.shape) != 2:
            raise ValueError(f"åŸ‹ã‚è¾¼ã¿ã¯2æ¬¡å…ƒã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: {embeddings.shape}")
        
        logger.info(f"   ãƒãƒ«ãƒ çµ±è¨ˆ: mean={np.linalg.norm(embeddings, axis=1).mean():.4f}")
        
        return embeddings
    
    def compute_collaborative_weights(self, 
                                    ppr_df: pd.DataFrame,
                                    embeddings: np.ndarray,
                                    id_map: Dict[int, str]) -> pd.DataFrame:
        """å”èª¿é‡ã¿è¨ˆç®—ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        logger.info("ğŸ§® å”èª¿é‡ã¿è¨ˆç®—é–‹å§‹")
        
        # ãƒ¦ãƒ¼ã‚¶ãƒªã‚¹ãƒˆå–å¾—ï¼ˆå†…éƒ¨IDã‚’ä½¿ç”¨ï¼‰
        unique_users = ppr_df['seed_internal_id'].unique()
        total_users = len(unique_users)
        
        logger.info(f"   å‡¦ç†ãƒ¦ãƒ¼ã‚¶æ•°: {total_users:,}")
        logger.info(f"   åŸ‹ã‚è¾¼ã¿æ•°: {len(embeddings):,}")
        
        # å¤šæ§˜æ€§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        cluster_labels = None
        if self.diversity_enabled:
            logger.info("ğŸ”„ å¤šæ§˜æ€§æ­£å‰‡åŒ–ç”¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ")
            n_clusters = min(50, len(embeddings) // 10)  # é©å¿œçš„ã‚¯ãƒ©ã‚¹ã‚¿æ•°
            kmeans = KMeans(n_clusters=n_clusters, random_state=self.random_seed, n_init=10)
            cluster_labels = kmeans.fit_predict(embeddings)
            logger.info(f"   ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {n_clusters}, max_per_cluster: {self.max_per_cluster}")
        
        # çµæœè“„ç©ç”¨ãƒªã‚¹ãƒˆ
        results = []
        
        # å„ãƒ¦ãƒ¼ã‚¶ã«å¯¾ã™ã‚‹å‡¦ç†
        for user_internal_id in tqdm(unique_users, desc="å”èª¿é‡ã¿è¨ˆç®—"):
            # è©²å½“ãƒ¦ãƒ¼ã‚¶ã®PPRã‚¨ãƒ³ãƒˆãƒªå–å¾—
            user_ppr = ppr_df[ppr_df['seed_internal_id'] == user_internal_id].copy()
            
            if len(user_ppr) == 0:
                logger.warning(f"âš ï¸ User {user_internal_id}: PPRã‚¨ãƒ³ãƒˆãƒªãªã—")
                continue
            
            # ãƒ¦ãƒ¼ã‚¶åŸ‹ã‚è¾¼ã¿å–å¾—ï¼ˆå¢ƒç•Œãƒã‚§ãƒƒã‚¯ï¼‰
            if user_internal_id >= len(embeddings):
                logger.warning(f"âš ï¸ User {user_internal_id}: åŸ‹ã‚è¾¼ã¿ç¯„å›²å¤– ({len(embeddings)})")
                continue
            
            user_embedding = embeddings[user_internal_id].reshape(1, -1)
            
            # è¿‘å‚åŸ‹ã‚è¾¼ã¿å–å¾—
            neighbor_internal_ids = user_ppr['node_internal_id'].values
            valid_neighbors = neighbor_internal_ids[neighbor_internal_ids < len(embeddings)]
            
            if len(valid_neighbors) == 0:
                logger.warning(f"âš ï¸ User {user_internal_id}: æœ‰åŠ¹ãªè¿‘å‚ãªã—")
                continue
            
            neighbor_embeddings = embeddings[valid_neighbors]
            
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
            cos_similarities = cosine_similarity(user_embedding, neighbor_embeddings).flatten()
            
            # ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
            valid_mask = (cos_similarities >= self.min_cos) & \
                        (user_ppr['node_internal_id'].isin(valid_neighbors))
            
            if not valid_mask.any():
                logger.warning(f"âš ï¸ User {user_internal_id}: ãƒ•ã‚£ãƒ«ã‚¿å¾Œã«è¿‘å‚ãªã—")
                continue
            
            # ãƒ•ã‚£ãƒ«ã‚¿å¾Œãƒ‡ãƒ¼ã‚¿
            filtered_ppr = user_ppr[valid_mask].copy()
            
            # å¯¾å¿œã™ã‚‹ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®å–å¾—
            neighbor_to_cos = dict(zip(valid_neighbors, cos_similarities))
            filtered_cos = np.array([neighbor_to_cos[nid] for nid in filtered_ppr['node_internal_id']])
            
            # é‡ã¿è¨ˆç®—: weight = (ppr_score^gamma) * (cos_sim^beta)
            ppr_weights = np.power(filtered_ppr['score'].values, self.gamma)
            cos_weights = np.power(filtered_cos, self.beta)
            final_weights = ppr_weights * cos_weights
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«è¿½åŠ 
            filtered_ppr = filtered_ppr.copy()
            filtered_ppr['cos_sim'] = filtered_cos
            filtered_ppr['weight'] = final_weights
            
            # å¤šæ§˜æ€§åˆ¶ç´„ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            if self.diversity_enabled and cluster_labels is not None:
                filtered_ppr = self._apply_diversity_constraint(
                    filtered_ppr, cluster_labels
                )
            
            # Top-Mé¸æŠ
            top_neighbors = filtered_ppr.nlargest(self.top_m, 'weight')
            
            # çµæœã«è¿½åŠ ï¼ˆå¤–éƒ¨IDå¤‰æ›ï¼‰
            for _, row in top_neighbors.iterrows():
                neighbor_internal_id = row['node_internal_id']
                external_user_id = id_map.get(user_internal_id, str(user_internal_id))
                external_neighbor_id = id_map.get(neighbor_internal_id, str(neighbor_internal_id))
                
                results.append({
                    'user_id': external_user_id,
                    'neighbor_id': external_neighbor_id,
                    'weight': row['weight'],
                    'ppr_score': row['score'],
                    'cos_sim': row['cos_sim']
                })
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ åŒ–
        result_df = pd.DataFrame(results)
        
        # ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–
        if len(result_df) > 0:
            result_df['weight'] = result_df['weight'].astype(np.float32)
            result_df['ppr_score'] = result_df['ppr_score'].astype(np.float32)
            result_df['cos_sim'] = result_df['cos_sim'].astype(np.float32)
        
        logger.info(f"âœ… å”èª¿é‡ã¿è¨ˆç®—å®Œäº†")
        logger.info(f"   å‡¦ç†æ¸ˆã¿ãƒ¦ãƒ¼ã‚¶: {result_df['user_id'].nunique():,}/{total_users:,}")
        logger.info(f"   ç·ã‚¨ãƒƒã‚¸æ•°: {len(result_df):,}")
        logger.info(f"   å¹³å‡è¿‘å‚æ•°/ãƒ¦ãƒ¼ã‚¶: {len(result_df)/result_df['user_id'].nunique():.1f}")
        
        if len(result_df) > 0:
            logger.info(f"   é‡ã¿çµ±è¨ˆ: [{result_df['weight'].min():.6f}, {result_df['weight'].max():.6f}]")
            logger.info(f"   é‡ã¿åˆ†å¸ƒ: median={result_df['weight'].median():.6f}, 95%={result_df['weight'].quantile(0.95):.6f}")
        
        return result_df
    
    def _apply_diversity_constraint(self, 
                                  neighbor_df: pd.DataFrame,
                                  cluster_labels: np.ndarray) -> pd.DataFrame:
        """å¤šæ§˜æ€§åˆ¶ç´„ã‚’é©ç”¨ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã«max_per_clusteråˆ¶é™ï¼‰"""
        if len(neighbor_df) <= self.max_per_cluster:
            return neighbor_df
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ©ãƒ™ãƒ«è¿½åŠ 
        neighbor_df = neighbor_df.copy()
        neighbor_df['cluster'] = cluster_labels[neighbor_df['node_internal_id'].values]
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã«Top-Né¸æŠ
        diverse_results = []
        for cluster_id in neighbor_df['cluster'].unique():
            cluster_neighbors = neighbor_df[neighbor_df['cluster'] == cluster_id]
            top_in_cluster = cluster_neighbors.nlargest(self.max_per_cluster, 'weight')
            diverse_results.append(top_in_cluster)
        
        result = pd.concat(diverse_results, ignore_index=True)
        return result.drop('cluster', axis=1)
    
    def save_results(self, 
                    result_df: pd.DataFrame,
                    output_dir: str,
                    meta_info: Dict) -> None:
        """çµæœã‚’ä¿å­˜"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ’¾ çµæœä¿å­˜é–‹å§‹: {output_path}")
        
        # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        cfs_pool_path = output_path / "cfs_pool.parquet"
        result_df.to_parquet(cfs_pool_path, index=False)
        logger.info(f"âœ… å”èª¿ãƒ—ãƒ¼ãƒ«ä¿å­˜: {cfs_pool_path}")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        meta_path = output_path / "meta.json"
        with open(meta_path, 'w') as f:
            json.dump(meta_info, f, indent=2, ensure_ascii=False)
        logger.info(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {meta_path}")
        
        logger.info(f"ğŸ¯ ä¿å­˜å®Œäº†: {len(result_df):,} ã‚¨ãƒ³ãƒˆãƒª")
    
    def run_integration(self,
                       ppr_path: str,
                       id_map_path: Optional[str],
                       user_embeddings_path: str,
                       output_dir: str) -> None:
        """çµ±åˆå‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
        start_time = time.time()
        
        logger.info("ğŸš€ PPR-CFSçµ±åˆå‡¦ç†é–‹å§‹")
        logger.info(f"   PPRãƒ‡ãƒ¼ã‚¿: {ppr_path}")
        logger.info(f"   IDãƒãƒƒãƒ—: {id_map_path}")
        logger.info(f"   ãƒ¦ãƒ¼ã‚¶åŸ‹ã‚è¾¼ã¿: {user_embeddings_path}")
        logger.info(f"   å‡ºåŠ›å…ˆ: {output_dir}")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆé †åºé‡è¦ï¼šID mapã‚’å…ˆã«èª­ã¿è¾¼ã‚“ã§ã‹ã‚‰PPRãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ï¼‰
            id_map = self.load_id_mapping(id_map_path)
            embeddings = self.load_user_embeddings(user_embeddings_path)
            ppr_df = self.load_ppr_data(ppr_path, id_map)
            
            # IDæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            max_ppr_internal_id = max(ppr_df['seed_internal_id'].max(), ppr_df['node_internal_id'].max())
            if max_ppr_internal_id >= len(embeddings):
                logger.warning(f"âš ï¸ IDç¯„å›²ä¸æ•´åˆ: PPRæœ€å¤§å†…éƒ¨ID={max_ppr_internal_id}, åŸ‹ã‚è¾¼ã¿æ•°={len(embeddings)}")
            
            # å”èª¿é‡ã¿è¨ˆç®—
            result_df = self.compute_collaborative_weights(ppr_df, embeddings, id_map)
            
            # ç©ºçµæœãƒã‚§ãƒƒã‚¯
            if len(result_df) == 0:
                raise RuntimeError("å”èª¿ãƒ—ãƒ¼ãƒ«ç”Ÿæˆå¤±æ•—: çµæœãŒç©ºã§ã™")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æº–å‚™
            meta_info = {
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'processing_time_sec': time.time() - start_time,
                'parameters': {
                    'gamma': self.gamma,
                    'beta': self.beta,
                    'min_ppr': self.min_ppr,
                    'min_cos': self.min_cos,
                    'top_m': self.top_m,
                    'diversity_enabled': self.diversity_enabled,
                    'max_per_cluster': self.max_per_cluster,
                    'random_seed': self.random_seed
                },
                'statistics': {
                    'total_users': int(result_df['user_id'].nunique()),
                    'total_edges': int(len(result_df)),
                    'avg_neighbors_per_user': float(len(result_df) / result_df['user_id'].nunique()),
                    'weight_min': float(result_df['weight'].min()),
                    'weight_max': float(result_df['weight'].max()),
                    'weight_median': float(result_df['weight'].median()),
                    'weight_95pct': float(result_df['weight'].quantile(0.95))
                }
            }
            
            # çµæœä¿å­˜
            self.save_results(result_df, output_dir, meta_info)
            
            execution_time = time.time() - start_time
            logger.info(f"ğŸ‰ PPR-CFSçµ±åˆå®Œäº†!")
            logger.info(f"   å®Ÿè¡Œæ™‚é–“: {execution_time:.1f}s")
            logger.info(f"   å‡¦ç†é€Ÿåº¦: {len(result_df)/execution_time:.0f} edges/sec")
            
        except Exception as e:
            logger.error(f"âŒ PPR-CFSçµ±åˆå¤±æ•—: {e}")
            raise


def parse_arguments():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°è§£æ"""
    parser = argparse.ArgumentParser(
        description="PPRå‡ºåŠ›ã‚’CFS-Chameleonå”èª¿é‡ã¿ä»˜ã‘ã«çµ±åˆ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
å®Ÿè¡Œä¾‹:
    python integrate_ppr_cfs.py \\
      --ppr ./ppr_results/ppr_topk.parquet \\
      --id_map ./ppr_results/id_map.json \\
      --user_embeddings ./embeddings/lamp2_user_embeddings.npy \\
      --out ./graphrag_cfs_weights \\
      --gamma 1.0 --beta 0.3 --min_ppr 1e-4 --min_cos 0.2 --topM 50

å‡ºåŠ›:
    ./graphrag_cfs_weights/
    â”œâ”€â”€ cfs_pool.parquet    # (user_id, neighbor_id, weight)
    â””â”€â”€ meta.json           # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        """
    )
    
    # å¿…é ˆå¼•æ•°
    parser.add_argument('--ppr', required=True,
                       help='PPRãƒ‡ãƒ¼ã‚¿ (ppr_topk.parquet ã¾ãŸã¯ãã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª)')
    parser.add_argument('--user_embeddings', required=True,
                       help='ãƒ¦ãƒ¼ã‚¶åŸ‹ã‚è¾¼ã¿ (*.npy)')
    parser.add_argument('--out', required=True,
                       help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³å¼•æ•°
    parser.add_argument('--id_map', default=None,
                       help='IDãƒãƒƒãƒ”ãƒ³ã‚° (id_map.json)')
    
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument('--gamma', type=float, default=1.0,
                       help='PPRã‚¹ã‚³ã‚¢æŒ‡æ•° (default: 1.0)')
    parser.add_argument('--beta', type=float, default=0.3,
                       help='ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦æŒ‡æ•° (default: 0.3)')
    parser.add_argument('--min_ppr', type=float, default=1e-4,
                       help='PPRã‚¹ã‚³ã‚¢æœ€å°é–¾å€¤ (default: 1e-4)')
    parser.add_argument('--min_cos', type=float, default=0.2,
                       help='ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦æœ€å°é–¾å€¤ (default: 0.2)')
    parser.add_argument('--topM', type=int, default=50,
                       help='å„ãƒ¦ãƒ¼ã‚¶ã®æœ€å¤§è¿‘å‚æ•° (default: 50)')
    
    # å¤šæ§˜æ€§åˆ¶ç´„
    parser.add_argument('--diversity', action='store_true',
                       help='å¤šæ§˜æ€§æ­£å‰‡åŒ–ã‚’æœ‰åŠ¹ã«ã™ã‚‹')
    parser.add_argument('--max_per_cluster', type=int, default=10,
                       help='ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®æœ€å¤§è¿‘å‚æ•° (default: 10)')
    
    # ãã®ä»–
    parser.add_argument('--seed', type=int, default=42,
                       help='ä¹±æ•°ã‚·ãƒ¼ãƒ‰ (default: 42)')
    parser.add_argument('--verbose', action='store_true',
                       help='è©³ç´°ãƒ­ã‚°å‡ºåŠ›')
    
    return parser.parse_args()


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    args = parse_arguments()
    
    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«èª¿æ•´
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # å¼•æ•°æ¤œè¨¼
    if not os.path.exists(args.ppr):
        raise FileNotFoundError(f"PPRãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.ppr}")
    
    if not os.path.exists(args.user_embeddings):
        raise FileNotFoundError(f"ãƒ¦ãƒ¼ã‚¶åŸ‹ã‚è¾¼ã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.user_embeddings}")
    
    # çµ±åˆå‡¦ç†å®Ÿè¡Œ
    integrator = PPRCFSIntegrator(
        gamma=args.gamma,
        beta=args.beta,
        min_ppr=args.min_ppr,
        min_cos=args.min_cos,
        top_m=args.topM,
        diversity_enabled=args.diversity,
        max_per_cluster=args.max_per_cluster,
        random_seed=args.seed
    )
    
    integrator.run_integration(
        ppr_path=args.ppr,
        id_map_path=args.id_map,
        user_embeddings_path=args.user_embeddings,
        output_dir=args.out
    )


if __name__ == "__main__":
    main()