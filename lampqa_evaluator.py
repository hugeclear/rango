#!/usr/bin/env python3
"""
CFS-Chameleonå‘ã‘LAMP-QAãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
å¾“æ¥ç‰ˆvsæ”¹å–„ç‰ˆã®å®šé‡æ¯”è¼ƒã«ã‚ˆã‚‹EM/F1/ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è©•ä¾¡
"""

import numpy as np
import torch
import json
import csv
import time
import re
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass
from pathlib import Path
import logging
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import matplotlib.pyplot as plt
import seaborn as sns

# å››å¤§æ”¹è‰¯ã‚·ã‚¹ãƒ†ãƒ 
from adaptive_fusion_cfs_integration import IntegratedChameleonSystem, IntegratedChameleonConfig
from adaptive_piece_fusion import fuse_pieces_adaptive, AdaptiveFusionConfig
from dual_direction_cfs_integration import DualDirectionChameleonEditor, DualDirectionConfig
from task_based_quality_evaluator import TaskBasedQualityEvaluator
from semantic_similarity_engine import SemanticSimilarityEngine

# CFS-Chameleoné–¢é€£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
try:
    from cfs_chameleon_extension import DirectionPiece, CollaborativeDirectionPool
    from chameleon_cfs_integrator import CollaborativeChameleonEditor
    CFS_AVAILABLE = True
except ImportError:
    print("âš ï¸ CFS modules not available. Using mock implementations.")
    CFS_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class LampQaItem:
    """LAMP-QA è³ªå•å¿œç­”ã‚¢ã‚¤ãƒ†ãƒ """
    question: str
    answer: str
    context: Optional[str] = None
    domain: Optional[str] = None
    difficulty: Optional[str] = None
    item_id: Optional[str] = None

@dataclass
class EvaluationResult:
    """è©•ä¾¡çµæœ"""
    em_score: float
    f1_score: float
    avg_latency: float
    total_samples: int
    success_examples: List[Dict[str, str]]
    failure_examples: List[Dict[str, str]]
    domain_breakdown: Dict[str, Dict[str, float]]

@dataclass
class LampQaEvalConfig:
    """LAMP-QAè©•ä¾¡è¨­å®š"""
    max_samples: int = 100           # è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«æ•°åˆ¶é™
    max_length: int = 128            # æœ€å¤§ç”Ÿæˆé•·
    timeout_per_sample: float = 30.0 # ã‚µãƒ³ãƒ—ãƒ«æ¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
    parallel_evaluation: bool = True  # ä¸¦åˆ—è©•ä¾¡
    max_workers: int = 4             # ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
    alpha_personal: float = 0.1      # ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«æ–¹å‘å¼·åº¦
    alpha_neutral: float = -0.05     # ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘å¼·åº¦
    sample_random_seed: int = 42     # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ä¹±æ•°ã‚·ãƒ¼ãƒ‰
    normalize_answers: bool = True    # å›ç­”æ­£è¦åŒ–ãƒ•ãƒ©ã‚°
    save_detailed_results: bool = True # è©³ç´°çµæœä¿å­˜

class LampQaDataLoader:
    """LAMP-QA ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼"""
    
    def __init__(self, data_path: str):
        """
        åˆæœŸåŒ–
        
        Args:
            data_path: LAMP-QAãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        self.data_path = Path(data_path)
        
        if not self.data_path.exists():
            raise FileNotFoundError(f"LAMP-QA data file not found: {data_path}")
        
        logger.info(f"âœ… LampQaDataLoader initialized: {data_path}")
    
    def load_data(self) -> List[LampQaItem]:
        """
        LAMP-QAãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        
        Returns:
            LampQaItemã®ãƒªã‚¹ãƒˆ
        """
        logger.info(f"ğŸ“Š Loading LAMP-QA data from {self.data_path}")
        
        try:
            if self.data_path.suffix.lower() == '.json':
                return self._load_json_data()
            elif self.data_path.suffix.lower() == '.csv':
                return self._load_csv_data()
            else:
                raise ValueError(f"Unsupported file format: {self.data_path.suffix}")
                
        except Exception as e:
            logger.error(f"âŒ Failed to load LAMP-QA data: {e}")
            raise
    
    def _load_json_data(self) -> List[LampQaItem]:
        """JSONå½¢å¼ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        with open(self.data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        items = []
        if isinstance(data, list):
            # ç›´æ¥ã®ãƒªã‚¹ãƒˆå½¢å¼
            for i, item in enumerate(data):
                items.append(self._parse_item(item, str(i)))
        elif isinstance(data, dict):
            # è¾æ›¸å½¢å¼ï¼ˆãƒã‚¹ãƒˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ï¼‰
            for key, value in data.items():
                if isinstance(value, list):
                    for i, item in enumerate(value):
                        items.append(self._parse_item(item, f"{key}_{i}"))
                else:
                    items.append(self._parse_item(value, key))
        
        logger.info(f"âœ… Loaded {len(items)} items from JSON")
        return items
    
    def _load_csv_data(self) -> List[LampQaItem]:
        """CSVå½¢å¼ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        items = []
        
        with open(self.data_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for i, row in enumerate(reader):
                items.append(self._parse_item(row, str(i)))
        
        logger.info(f"âœ… Loaded {len(items)} items from CSV")
        return items
    
    def _parse_item(self, raw_item: Dict[str, Any], item_id: str) -> LampQaItem:
        """ç”Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰LampQaItemã¸ã®å¤‰æ›"""
        # æŸ”è»Ÿãªã‚­ãƒ¼å¯¾å¿œ
        question_keys = ['question', 'q', 'query', 'input']
        answer_keys = ['answer', 'a', 'target', 'output', 'response']
        context_keys = ['context', 'passage', 'background', 'document']
        
        question = None
        for key in question_keys:
            if key in raw_item:
                question = str(raw_item[key]).strip()
                break
        
        answer = None
        for key in answer_keys:
            if key in raw_item:
                answer = str(raw_item[key]).strip()
                break
        
        context = None
        for key in context_keys:
            if key in raw_item and raw_item[key]:
                context = str(raw_item[key]).strip()
                break
        
        if not question or not answer:
            raise ValueError(f"Missing question or answer in item {item_id}")
        
        return LampQaItem(
            question=question,
            answer=answer,
            context=context,
            domain=raw_item.get('domain', 'general'),
            difficulty=raw_item.get('difficulty', 'medium'),
            item_id=item_id
        )

class QAMetricsCalculator:
    """è³ªå•å¿œç­”ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—å™¨"""
    
    @staticmethod
    def normalize_text(text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–"""
        # å°æ–‡å­—åŒ–
        text = text.lower()
        # ä½™åˆ†ãªç©ºç™½é™¤å»
        text = re.sub(r'\s+', ' ', text).strip()
        # å¥èª­ç‚¹æ­£è¦åŒ–
        text = re.sub(r'[^\w\s]', '', text)
        return text
    
    @staticmethod
    def compute_exact_match(prediction: str, reference: str, normalize: bool = True) -> float:
        """
        Exact Match (EM) ã‚¹ã‚³ã‚¢è¨ˆç®—
        
        Args:
            prediction: äºˆæ¸¬ãƒ†ã‚­ã‚¹ãƒˆ
            reference: å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ
            normalize: æ­£è¦åŒ–ãƒ•ãƒ©ã‚°
            
        Returns:
            EMã‚¹ã‚³ã‚¢ (0.0 or 1.0)
        """
        if normalize:
            pred_norm = QAMetricsCalculator.normalize_text(prediction)
            ref_norm = QAMetricsCalculator.normalize_text(reference)
            return 1.0 if pred_norm == ref_norm else 0.0
        else:
            return 1.0 if prediction.strip() == reference.strip() else 0.0
    
    @staticmethod
    def compute_f1_score(prediction: str, reference: str, normalize: bool = True) -> float:
        """
        F1ã‚¹ã‚³ã‚¢è¨ˆç®—
        
        Args:
            prediction: äºˆæ¸¬ãƒ†ã‚­ã‚¹ãƒˆ
            reference: å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ
            normalize: æ­£è¦åŒ–ãƒ•ãƒ©ã‚°
            
        Returns:
            F1ã‚¹ã‚³ã‚¢ (0.0-1.0)
        """
        if normalize:
            pred_tokens = QAMetricsCalculator.normalize_text(prediction).split()
            ref_tokens = QAMetricsCalculator.normalize_text(reference).split()
        else:
            pred_tokens = prediction.split()
            ref_tokens = reference.split()
        
        if not pred_tokens and not ref_tokens:
            return 1.0
        if not pred_tokens or not ref_tokens:
            return 0.0
        
        # å…±é€šãƒˆãƒ¼ã‚¯ãƒ³è¨ˆç®—
        common_tokens = set(pred_tokens) & set(ref_tokens)
        
        if not common_tokens:
            return 0.0
        
        # Precision, Recallè¨ˆç®—
        precision = len(common_tokens) / len(set(pred_tokens))
        recall = len(common_tokens) / len(set(ref_tokens))
        
        # F1è¨ˆç®—
        if precision + recall == 0:
            return 0.0
        
        f1 = 2 * precision * recall / (precision + recall)
        return f1
    
    @staticmethod
    def compute_metrics_batch(predictions: List[str], 
                            references: List[str],
                            normalize: bool = True) -> Dict[str, float]:
        """
        ãƒãƒƒãƒãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        
        Args:
            predictions: äºˆæ¸¬ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
            references: å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
            normalize: æ­£è¦åŒ–ãƒ•ãƒ©ã‚°
            
        Returns:
            ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¾æ›¸
        """
        if len(predictions) != len(references):
            raise ValueError("Predictions and references must have same length")
        
        em_scores = []
        f1_scores = []
        
        for pred, ref in zip(predictions, references):
            em_scores.append(QAMetricsCalculator.compute_exact_match(pred, ref, normalize))
            f1_scores.append(QAMetricsCalculator.compute_f1_score(pred, ref, normalize))
        
        return {
            "EM": np.mean(em_scores),
            "F1": np.mean(f1_scores),
            "samples": len(predictions)
        }

class LampQaEvaluator:
    """LAMP-QA ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡å™¨"""
    
    def __init__(self, 
                 improved_editor: Any = None,
                 baseline_editor: Any = None,
                 config: LampQaEvalConfig = None):
        """
        åˆæœŸåŒ–
        
        Args:
            improved_editor: æ”¹å–„ç‰ˆCFS-Chameleonã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼
            baseline_editor: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆå¾“æ¥ç‰ˆï¼‰ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼
            config: è©•ä¾¡è¨­å®š
        """
        self.improved_editor = improved_editor
        self.baseline_editor = baseline_editor
        self.config = config or LampQaEvalConfig()
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—å™¨
        self.metrics_calculator = QAMetricsCalculator()
        
        logger.info("âœ… LampQaEvaluator initialized")
        logger.info(f"   Max samples: {self.config.max_samples}")
        logger.info(f"   Parallel evaluation: {self.config.parallel_evaluation}")
    
    def create_prompt(self, item: LampQaItem) -> str:
        """
        è³ªå•å¿œç­”ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        
        Args:
            item: LAMP-QAã‚¢ã‚¤ãƒ†ãƒ 
            
        Returns:
            ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        prompt_parts = []
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Œã°å…ˆé ­ã«é…ç½®
        if item.context:
            prompt_parts.append(f"æ–‡è„ˆ: {item.context}")
        
        # è³ªå•ã‚’è¿½åŠ 
        prompt_parts.append(f"è³ªå•: {item.question}")
        prompt_parts.append("å›ç­”:")
        
        return "\n".join(prompt_parts)
    
    def evaluate_single_sample(self, 
                              item: LampQaItem, 
                              editor: Any,
                              editor_name: str) -> Dict[str, Any]:
        """
        å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«ã®è©•ä¾¡
        
        Args:
            item: LAMP-QAã‚¢ã‚¤ãƒ†ãƒ 
            editor: è©•ä¾¡å¯¾è±¡ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼
            editor_name: ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼å
            
        Returns:
            è©•ä¾¡çµæœè¾æ›¸
        """
        prompt = self.create_prompt(item)
        
        try:
            # ç”Ÿæˆå®Ÿè¡Œï¼ˆæ™‚é–“è¨ˆæ¸¬ï¼‰
            start_time = time.time()
            
            if hasattr(editor, 'generate_with_improved_collaboration'):
                # æ”¹å–„ç‰ˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ 
                generated = editor.generate_with_improved_collaboration(
                    prompt=prompt,
                    user_id="lamp_user",
                    alpha_personal=self.config.alpha_personal,
                    alpha_neutral=self.config.alpha_neutral,
                    max_length=self.config.max_length
                )
            elif hasattr(editor, 'generate_with_integrated_system'):
                # çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
                generated = editor.generate_with_integrated_system(
                    prompt=prompt,
                    user_context=item.question,
                    max_length=self.config.max_length
                )
            elif hasattr(editor, 'generate'):
                # åŸºæœ¬ç”Ÿæˆãƒ¡ã‚½ãƒƒãƒ‰
                generated = editor.generate(prompt, max_length=self.config.max_length)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                generated = f"Generated response for: {item.question[:30]}..."
            
            latency = time.time() - start_time
            
            # ç”Ÿæˆçµæœã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            generated = generated.strip()
            
            # "å›ç­”:" ä»¥é™ã®ã¿æŠ½å‡º
            if "å›ç­”:" in generated:
                generated = generated.split("å›ç­”:")[-1].strip()
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            em_score = self.metrics_calculator.compute_exact_match(
                generated, item.answer, self.config.normalize_answers
            )
            f1_score = self.metrics_calculator.compute_f1_score(
                generated, item.answer, self.config.normalize_answers
            )
            
            return {
                "item_id": item.item_id,
                "question": item.question,
                "reference": item.answer,
                "prediction": generated,
                "em_score": em_score,
                "f1_score": f1_score,
                "latency": latency,
                "domain": item.domain,
                "difficulty": item.difficulty,
                "editor": editor_name,
                "success": True
            }
            
        except Exception as e:
            logger.error(f"âŒ Evaluation error for item {item.item_id}: {e}")
            return {
                "item_id": item.item_id,
                "question": item.question,
                "reference": item.answer,
                "prediction": "",
                "em_score": 0.0,
                "f1_score": 0.0,
                "latency": self.config.timeout_per_sample,
                "domain": item.domain,
                "difficulty": item.difficulty,
                "editor": editor_name,
                "success": False,
                "error": str(e)
            }
    
    def evaluate_editor(self, 
                       items: List[LampQaItem], 
                       editor: Any,
                       editor_name: str) -> EvaluationResult:
        """
        ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ã®è©•ä¾¡å®Ÿè¡Œ
        
        Args:
            items: è©•ä¾¡ã‚¢ã‚¤ãƒ†ãƒ ãƒªã‚¹ãƒˆ
            editor: è©•ä¾¡å¯¾è±¡ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼
            editor_name: ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼å
            
        Returns:
            è©•ä¾¡çµæœ
        """
        logger.info(f"ğŸš€ Evaluating {editor_name} on {len(items)} samples")
        
        if not self.config.parallel_evaluation:
            # ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«è©•ä¾¡
            results = []
            for i, item in enumerate(items):
                logger.info(f"   Processing sample {i+1}/{len(items)}")
                result = self.evaluate_single_sample(item, editor, editor_name)
                results.append(result)
        else:
            # ä¸¦åˆ—è©•ä¾¡
            results = []
            with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
                futures = {
                    executor.submit(self.evaluate_single_sample, item, editor, editor_name): item
                    for item in items
                }
                
                completed = 0
                for future in as_completed(futures):
                    try:
                        result = future.result(timeout=self.config.timeout_per_sample)
                        results.append(result)
                        completed += 1
                        logger.info(f"   Completed {completed}/{len(items)} samples")
                    except Exception as e:
                        item = futures[future]
                        logger.error(f"Sample {item.item_id} failed: {e}")
                        results.append({
                            "item_id": item.item_id,
                            "em_score": 0.0,
                            "f1_score": 0.0,
                            "latency": self.config.timeout_per_sample,
                            "success": False,
                            "error": str(e)
                        })
        
        # çµæœé›†è¨ˆ
        return self._aggregate_results(results, editor_name)
    
    def _aggregate_results(self, results: List[Dict[str, Any]], editor_name: str) -> EvaluationResult:
        """è©•ä¾¡çµæœã®é›†è¨ˆ"""
        successful_results = [r for r in results if r.get('success', False)]
        
        if not successful_results:
            logger.warning(f"No successful results for {editor_name}")
            return EvaluationResult(
                em_score=0.0,
                f1_score=0.0,
                avg_latency=0.0,
                total_samples=len(results),
                success_examples=[],
                failure_examples=results[:5],  # å¤±æ•—ä¾‹ã®ä¸Šä½5ã¤
                domain_breakdown={}
            )
        
        # å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        em_scores = [r['em_score'] for r in successful_results]
        f1_scores = [r['f1_score'] for r in successful_results]
        latencies = [r['latency'] for r in successful_results]
        
        avg_em = np.mean(em_scores)
        avg_f1 = np.mean(f1_scores)
        avg_latency = np.mean(latencies)
        
        # æˆåŠŸä¾‹ãƒ»å¤±æ•—ä¾‹
        success_examples = [r for r in successful_results if r['em_score'] > 0.0][:3]
        failure_examples = [r for r in successful_results if r['em_score'] == 0.0][:3]
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥é›†è¨ˆ
        domain_breakdown = {}
        domains = set(r.get('domain', 'general') for r in successful_results)
        
        for domain in domains:
            domain_results = [r for r in successful_results if r.get('domain', 'general') == domain]
            if domain_results:
                domain_breakdown[domain] = {
                    "EM": np.mean([r['em_score'] for r in domain_results]),
                    "F1": np.mean([r['f1_score'] for r in domain_results]),
                    "Latency": np.mean([r['latency'] for r in domain_results]),
                    "Count": len(domain_results)
                }
        
        logger.info(f"âœ… {editor_name} evaluation completed")
        logger.info(f"   EM: {avg_em:.4f}, F1: {avg_f1:.4f}, Latency: {avg_latency:.3f}s")
        
        return EvaluationResult(
            em_score=avg_em,
            f1_score=avg_f1,
            avg_latency=avg_latency,
            total_samples=len(results),
            success_examples=success_examples,
            failure_examples=failure_examples,
            domain_breakdown=domain_breakdown
        )
    
    def compare_editors(self, items: List[LampQaItem]) -> Dict[str, EvaluationResult]:
        """
        è¤‡æ•°ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ã®æ¯”è¼ƒè©•ä¾¡
        
        Args:
            items: è©•ä¾¡ã‚¢ã‚¤ãƒ†ãƒ ãƒªã‚¹ãƒˆ
            
        Returns:
            ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼åˆ¥è©•ä¾¡çµæœ
        """
        logger.info(f"ğŸ”¬ Starting comparative evaluation on {len(items)} samples")
        
        results = {}
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡
        if self.baseline_editor:
            logger.info("ğŸ“Š Evaluating baseline editor...")
            results["baseline"] = self.evaluate_editor(items, self.baseline_editor, "baseline")
        
        # æ”¹å–„ç‰ˆè©•ä¾¡
        if self.improved_editor:
            logger.info("ğŸ¦ Evaluating improved editor...")
            results["improved"] = self.evaluate_editor(items, self.improved_editor, "improved")
        
        logger.info("âœ… Comparative evaluation completed")
        return results

class LampQaReportGenerator:
    """LAMP-QAè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå™¨"""
    
    @staticmethod
    def generate_markdown_report(results: Dict[str, EvaluationResult],
                               config: LampQaEvalConfig,
                               output_path: str) -> str:
        """
        Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        
        Args:
            results: ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼åˆ¥è©•ä¾¡çµæœ
            config: è©•ä¾¡è¨­å®š
            output_path: å‡ºåŠ›ãƒ‘ã‚¹
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸMarkdownãƒ†ã‚­ã‚¹ãƒˆ
        """
        report_lines = []
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        report_lines.extend([
            "# LAMP-QA ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ",
            "",
            f"**è©•ä¾¡æ—¥æ™‚**: {time.strftime('%Y-%m-%d %H:%M:%S')}",
            f"**è©•ä¾¡è¨­å®š**: æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°={config.max_samples}, æœ€å¤§ç”Ÿæˆé•·={config.max_length}",
            f"**æ­£è¦åŒ–**: {'æœ‰åŠ¹' if config.normalize_answers else 'ç„¡åŠ¹'}",
            ""
        ])
        
        # ç·åˆçµæœãƒ†ãƒ¼ãƒ–ãƒ«
        if len(results) >= 2:
            report_lines.extend([
                "## ğŸ“Š ç·åˆçµæœæ¯”è¼ƒ",
                "",
                "| Editor | EM Score | F1 Score | Avg Latency (s) | Samples |",
                "|--------|----------|----------|----------------|---------|"
            ])
            
            for editor_name, result in results.items():
                report_lines.append(
                    f"| {editor_name.title()} | {result.em_score:.4f} | {result.f1_score:.4f} | "
                    f"{result.avg_latency:.3f} | {result.total_samples} |"
                )
            
            report_lines.append("")
            
            # æ”¹å–„ç‡è¨ˆç®—
            if "baseline" in results and "improved" in results:
                baseline = results["baseline"]
                improved = results["improved"]
                
                em_improve = ((improved.em_score - baseline.em_score) / baseline.em_score * 100) if baseline.em_score > 0 else 0
                f1_improve = ((improved.f1_score - baseline.f1_score) / baseline.f1_score * 100) if baseline.f1_score > 0 else 0
                latency_change = ((improved.avg_latency - baseline.avg_latency) / baseline.avg_latency * 100) if baseline.avg_latency > 0 else 0
                
                report_lines.extend([
                    "### ğŸš€ æ”¹å–„åŠ¹æœ",
                    "",
                    f"- **EM Score æ”¹å–„**: {em_improve:+.1f}%",
                    f"- **F1 Score æ”¹å–„**: {f1_improve:+.1f}%",
                    f"- **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å¤‰åŒ–**: {latency_change:+.1f}%",
                    ""
                ])
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥çµæœ
        for editor_name, result in results.items():
            if result.domain_breakdown:
                report_lines.extend([
                    f"## ğŸ“‹ {editor_name.title()} ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥è©³ç´°",
                    "",
                    "| Domain | EM Score | F1 Score | Avg Latency (s) | Count |",
                    "|--------|----------|----------|----------------|-------|"
                ])
                
                for domain, metrics in result.domain_breakdown.items():
                    report_lines.append(
                        f"| {domain} | {metrics['EM']:.4f} | {metrics['F1']:.4f} | "
                        f"{metrics['Latency']:.3f} | {metrics['Count']} |"
                    )
                
                report_lines.append("")
        
        # æˆåŠŸä¾‹ãƒ»å¤±æ•—ä¾‹
        for editor_name, result in results.items():
            if result.success_examples:
                report_lines.extend([
                    f"## âœ… {editor_name.title()} æˆåŠŸä¾‹",
                    ""
                ])
                
                for i, example in enumerate(result.success_examples[:3], 1):
                    report_lines.extend([
                        f"### æˆåŠŸä¾‹ {i}",
                        f"**è³ªå•**: {example.get('question', 'N/A')}",
                        f"**æ­£è§£**: {example.get('reference', 'N/A')}",
                        f"**äºˆæ¸¬**: {example.get('prediction', 'N/A')}",
                        f"**EM**: {example.get('em_score', 0):.1f}, **F1**: {example.get('f1_score', 0):.3f}",
                        ""
                    ])
            
            if result.failure_examples:
                report_lines.extend([
                    f"## âŒ {editor_name.title()} å¤±æ•—ä¾‹",
                    ""
                ])
                
                for i, example in enumerate(result.failure_examples[:3], 1):
                    report_lines.extend([
                        f"### å¤±æ•—ä¾‹ {i}",
                        f"**è³ªå•**: {example.get('question', 'N/A')}",
                        f"**æ­£è§£**: {example.get('reference', 'N/A')}",
                        f"**äºˆæ¸¬**: {example.get('prediction', 'N/A')}",
                        f"**EM**: {example.get('em_score', 0):.1f}, **F1**: {example.get('f1_score', 0):.3f}",
                        ""
                    ])
        
        # ã‚°ãƒ©ãƒ•ç”ŸæˆæŒ‡ç¤º
        report_lines.extend([
            "## ğŸ“ˆ å¯è¦–åŒ–ã‚°ãƒ©ãƒ•",
            "",
            "### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã‚°ãƒ©ãƒ•",
            "```python",
            "# ä»¥ä¸‹ã®Pythonã‚³ãƒ¼ãƒ‰ã§æ¯”è¼ƒã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã§ãã¾ã™",
            "import matplotlib.pyplot as plt",
            "import numpy as np",
            "",
            "# ãƒ‡ãƒ¼ã‚¿è¨­å®š",
            "editors = ['Baseline', 'Improved']",
            f"em_scores = [{results.get('baseline', EvaluationResult(0,0,0,0,[],[],{})).em_score:.4f}, {results.get('improved', EvaluationResult(0,0,0,0,[],[],{})).em_score:.4f}]",
            f"f1_scores = [{results.get('baseline', EvaluationResult(0,0,0,0,[],[],{})).f1_score:.4f}, {results.get('improved', EvaluationResult(0,0,0,0,[],[],{})).f1_score:.4f}]",
            "",
            "# ã‚°ãƒ©ãƒ•ä½œæˆ",
            "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))",
            "ax1.bar(editors, em_scores, color=['skyblue', 'lightcoral'])",
            "ax1.set_title('EM Score Comparison')",
            "ax1.set_ylabel('EM Score')",
            "ax2.bar(editors, f1_scores, color=['skyblue', 'lightcoral'])",
            "ax2.set_title('F1 Score Comparison')",
            "ax2.set_ylabel('F1 Score') ",
            "plt.tight_layout()",
            "plt.savefig('lampqa_comparison.png')",
            "plt.show()",
            "```",
            ""
        ])
        
        # çµè«–
        report_lines.extend([
            "## ğŸ¯ çµè«–",
            "",
            "### ä¸»è¦ãªç™ºè¦‹",
        ])
        
        if "baseline" in results and "improved" in results:
            baseline = results["baseline"] 
            improved = results["improved"]
            
            if improved.em_score > baseline.em_score:
                report_lines.append("- âœ… æ”¹å–„ç‰ˆã¯å¾“æ¥ç‰ˆã‚ˆã‚Šã‚‚é«˜ã„EM Scoreã‚’é”æˆ")
            if improved.f1_score > baseline.f1_score:
                report_lines.append("- âœ… æ”¹å–„ç‰ˆã¯F1 Scoreã®å‘ä¸Šã‚’å®Ÿç¾")
            
            if improved.avg_latency < baseline.avg_latency:
                report_lines.append("- âš¡ æ”¹å–„ç‰ˆã¯ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã®çŸ­ç¸®ã‚‚é”æˆ")
            elif improved.avg_latency > baseline.avg_latency:
                report_lines.append("- âš ï¸ æ”¹å–„ç‰ˆã¯ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒå¢—åŠ ï¼ˆé«˜ç²¾åº¦ã¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰")
        
        report_lines.extend([
            "",
            "### æ¨å¥¨äº‹é …",
            "- LAMP-QAãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã®ç¶™ç¶šçš„è©•ä¾¡",
            "- ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¤œè¨",
            "- ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æœ€é©åŒ–ã®ç¶™ç¶šæ”¹å–„",
            "",
            "---",
            f"**ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ™‚åˆ»**: {time.strftime('%Y-%m-%d %H:%M:%S')}",
            f"**å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«**: {output_path}"
        ])
        
        markdown_content = "\n".join(report_lines)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        logger.info(f"âœ… Report generated: {output_path}")
        return markdown_content

def create_mock_lampqa_data() -> List[LampQaItem]:
    """ãƒ¢ãƒƒã‚¯LAMP-QAãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
    return [
        LampQaItem(
            question="æ—¥æœ¬ã®é¦–éƒ½ã¯ã©ã“ã§ã™ã‹ï¼Ÿ",
            answer="æ±äº¬",
            context="æ—¥æœ¬ã¯æ±ã‚¢ã‚¸ã‚¢ã«ä½ç½®ã™ã‚‹å›½å®¶ã§ã™ã€‚",
            domain="geography",
            difficulty="easy",
            item_id="mock_1"
        ),
        LampQaItem(
            question="Pythonã§ãƒªã‚¹ãƒˆã®é•·ã•ã‚’å–å¾—ã™ã‚‹é–¢æ•°ã¯ï¼Ÿ",
            answer="len()",
            context="Pythonã¯äººæ°—ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ã™ã€‚",
            domain="programming",
            difficulty="medium",
            item_id="mock_2"
        ),
        LampQaItem(
            question="1æ™‚é–“ã¯ä½•åˆ†ã§ã™ã‹ï¼Ÿ",
            answer="60åˆ†",
            domain="math",
            difficulty="easy",
            item_id="mock_3"
        ),
        LampQaItem(
            question="æ©Ÿæ¢°å­¦ç¿’ã«ãŠã‘ã‚‹éå­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            answer="è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«éåº¦ã«é©åˆã—ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æ±åŒ–æ€§èƒ½ãŒä½ä¸‹ã™ã‚‹ç¾è±¡",
            context="æ©Ÿæ¢°å­¦ç¿’ã§ã¯æ§˜ã€…ãªèª²é¡ŒãŒã‚ã‚Šã¾ã™ã€‚",
            domain="ai",
            difficulty="hard",
            item_id="mock_4"
        ),
        LampQaItem(
            question="å…‰ã®é€Ÿåº¦ã¯ç§’é€Ÿç´„ä½•ãƒ¡ãƒ¼ãƒˆãƒ«ã§ã™ã‹ï¼Ÿ",
            answer="300000000ãƒ¡ãƒ¼ãƒˆãƒ«",
            context="ç‰©ç†å­¦ã«ãŠã„ã¦å…‰ã¯é‡è¦ãªæ¦‚å¿µã§ã™ã€‚",
            domain="physics",
            difficulty="medium",
            item_id="mock_5"
        )
    ]

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="LAMP-QA Benchmark Evaluation for CFS-Chameleon")
    parser.add_argument("--lampqa-path", type=str, help="Path to LAMP-QA dataset file")
    parser.add_argument("--output", type=str, default="report_lampqa.md", help="Output report file path")
    parser.add_argument("--max-samples", type=int, default=50, help="Maximum number of samples to evaluate")
    parser.add_argument("--max-length", type=int, default=128, help="Maximum generation length")
    parser.add_argument("--parallel", action="store_true", help="Enable parallel evaluation")
    parser.add_argument("--alpha-personal", type=float, default=0.1, help="Personal direction strength")
    parser.add_argument("--alpha-neutral", type=float, default=-0.05, help="Neutral direction strength")
    parser.add_argument("--use-mock-data", action="store_true", help="Use mock LAMP-QA data for testing")
    
    args = parser.parse_args()
    
    print("ğŸ¦ LAMP-QA ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)
    
    # è©•ä¾¡è¨­å®š
    config = LampQaEvalConfig(
        max_samples=args.max_samples,
        max_length=args.max_length,
        parallel_evaluation=args.parallel,
        alpha_personal=args.alpha_personal,  
        alpha_neutral=args.alpha_neutral
    )
    
    try:
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        if args.use_mock_data or not args.lampqa_path:
            logger.info("ğŸ“‹ Using mock LAMP-QA data")
            lampqa_items = create_mock_lampqa_data()
        else:
            data_loader = LampQaDataLoader(args.lampqa_path)
            lampqa_items = data_loader.load_data()
        
        # ã‚µãƒ³ãƒ—ãƒ«åˆ¶é™
        if len(lampqa_items) > config.max_samples:
            logger.info(f"ğŸ“Š Sampling {config.max_samples} items from {len(lampqa_items)}")
            np.random.seed(config.sample_random_seed)
            lampqa_items = np.random.choice(lampqa_items, config.max_samples, replace=False).tolist()
        
        # ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼åˆæœŸåŒ–
        logger.info("ğŸ”§ Initializing editors...")
        
        # æ”¹å–„ç‰ˆã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ï¼ˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ ï¼‰
        improved_config = IntegratedChameleonConfig(
            integration_strategy="full",
            use_semantic_similarity=True,
            use_quality_evaluation=True,
            use_dual_direction=True
        )
        improved_editor = IntegratedChameleonSystem(improved_config)
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆç°¡å˜ãªå®Ÿè£…ï¼‰
        class MockBaselineEditor:
            def generate(self, prompt: str, max_length: int = 100) -> str:
                # éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
                if "é¦–éƒ½" in prompt:
                    return "æ±äº¬"
                elif "Python" in prompt and "len" in prompt:
                    return "len()é–¢æ•°"
                elif "æ™‚é–“" in prompt and "åˆ†" in prompt:
                    return "60åˆ†"
                elif "éå­¦ç¿’" in prompt:
                    return "ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«éåº¦ã«é©åˆã™ã‚‹ã“ã¨"
                elif "å…‰" in prompt and "é€Ÿåº¦" in prompt:
                    return "ç´„3å„„ãƒ¡ãƒ¼ãƒˆãƒ«æ¯ç§’"
                else:
                    return f"è³ªå•ã€Œ{prompt[:20]}...ã€ã¸ã®å›ç­”ã§ã™"
        
        baseline_editor = MockBaselineEditor()
        
        # è©•ä¾¡å®Ÿè¡Œ
        evaluator = LampQaEvaluator(
            improved_editor=improved_editor,
            baseline_editor=baseline_editor,
            config=config
        )
        
        # æ¯”è¼ƒè©•ä¾¡å®Ÿè¡Œ
        results = evaluator.compare_editors(lampqa_items)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        logger.info("ğŸ“ Generating evaluation report...")
        LampQaReportGenerator.generate_markdown_report(results, config, args.output)
        
        # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print(f"\nğŸ“Š Evaluation Summary:")
        print("-" * 40)
        for editor_name, result in results.items():
            print(f"{editor_name.title()}: EM={result.em_score:.4f}, F1={result.f1_score:.4f}, "
                  f"Latency={result.avg_latency:.3f}s")
        
        print(f"\nâœ… è©•ä¾¡å®Œäº†! ãƒ¬ãƒãƒ¼ãƒˆ: {args.output}")
        
    except Exception as e:
        logger.error(f"âŒ Evaluation failed: {e}")
        raise

if __name__ == "__main__":
    main()