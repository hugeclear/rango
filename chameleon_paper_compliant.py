#!/usr/bin/env python3
"""
è«–æ–‡CHAMELEONå®Œå…¨æº–æ‹ å®Ÿè£…
- A.3æº–æ‹ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ã‚ˆã‚‹å€‹äººåŒ–/ä¸­ç«‹ãƒ‡ãƒ¼ã‚¿ãƒšã‚¢ç”Ÿæˆ
- SVD+CCSã«ã‚ˆã‚‹Î¸P/Î¸Næ¨å®š  
- æŠ•å½±ç·¨é›†ã«ã‚ˆã‚‹è¡¨ç¾æ“ä½œ
- 15ç¨®LaMP-2ã‚¿ã‚°çµ±ä¸€
"""

import json
import os
import time
import yaml
import numpy as np
import torch
import torch.nn.functional as F
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
from transformers import AutoTokenizer, AutoModelForCausalLM
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, f1_score
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# LaMP-2å…¬å¼15ç¨®ã‚¿ã‚° (è«–æ–‡æº–æ‹ )
LAMP2_OFFICIAL_TAGS = [
    'sci-fi', 'based on a book', 'comedy', 'action', 'twist ending',
    'dystopia', 'dark comedy', 'classic', 'psychology', 'fantasy', 
    'romance', 'thought-provoking', 'social commentary', 'violence', 'true story'
]

# ã‚¿ã‚°â†’IDãƒãƒƒãƒ”ãƒ³ã‚° (ä¸ç¢ºå®Ÿæ€§æ¨å®šç”¨)
TAG_TO_ID = {tag: i for i, tag in enumerate(LAMP2_OFFICIAL_TAGS)}
ID_TO_TAG = {i: tag for i, tag in enumerate(LAMP2_OFFICIAL_TAGS)}

@dataclass
class ChameleonConfig:
    """è«–æ–‡æº–æ‹ è¨­å®š"""
    model_path: str
    device: str = 'cuda'
    max_length: int = 512
    batch_size: int = 4
    num_insights: int = 6  # 5-8 bullets recommended
    target_layers: List[str] = None  # CSSæå¤±ã§è‡ªå‹•é¸å®š
    projection_strength: float = 1.0  # æŠ•å½±å¼·åº¦

@dataclass  
class PersonalizedData:
    """å€‹äººåŒ–ãƒ‡ãƒ¼ã‚¿ãƒšã‚¢"""
    user_id: str
    query: str
    personalized_output: str
    neutral_output: str
    personalized_insight: str
    neutral_insight: str
    history: List[Dict]

class PaperCompliantChameleon:
    """è«–æ–‡CHAMELEONæº–æ‹ å®Ÿè£…"""
    
    def __init__(self, config: ChameleonConfig):
        self.config = config
        self.tokenizer = AutoTokenizer.from_pretrained(config.model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            config.model_path, 
            torch_dtype=torch.float32,
            device_map='auto'
        )
        self.model.eval()
        
        # Padding tokenè¨­å®š
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        # å±¤åˆ¥Î¸P/Î¸Nä¿å­˜
        self.theta_p_layers: Dict[str, torch.Tensor] = {}
        self.theta_n_layers: Dict[str, torch.Tensor] = {}
        self.active_layers: List[str] = []
        
        logger.info(f"PaperCompliantChameleon initialized with model: {config.model_path}")

    def generate_personalized_insight(self, history: List[Dict]) -> str:
        """A.3æº–æ‹ : å€‹äººåŒ–ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆï¼ˆè¶…å³æ ¼ãƒ©ãƒ™ãƒ«æ¼æ´©é˜²æ­¢ç‰ˆï¼‰"""
        
        system_prompt = """Extract user preferences from film descriptions using ONLY abstract storytelling concepts.
ABSOLUTE PROHIBITIONS:
- NO genre names (action, comedy, drama, etc.)
- NO category words (type, kind, style, genre, etc.)  
- NO story/narrative terminology
- NO specific film industry terms
REQUIRED FOCUS:
- Emotional resonance patterns
- Complexity preferences in plots
- Character relationship dynamics  
- Pacing and rhythm preferences
- Thematic depth interests
- Visual/aesthetic preferences
Format: Exactly 6 bullets starting with "TRAIT: ..." """

        # å±¥æ­´ã‚’ã•ã‚‰ã«æŠ½è±¡åŒ–
        abstract_patterns = []
        for item in history[:6]:  # ã•ã‚‰ã«åˆ¶é™
            desc = item.get('description', '')[:100]
            # ã‚ˆã‚ŠæŠ½è±¡çš„ãªè¡¨ç¾ã«å¤‰æ›
            abstract_patterns.append(f"Content: {desc}")
        
        content_str = "\n".join(abstract_patterns)
        
        user_prompt = f"""[CONTENT PATTERNS]
{content_str}

Analyze these content patterns for user preferences using ONLY these categories:
- Emotional engagement levels (intense vs subtle)
- Plot complexity preferences (straightforward vs intricate)  
- Character focus (individual vs ensemble)
- Tension patterns (steady vs escalating)
- Resolution styles (conclusive vs open-ended)
- Aesthetic preferences (minimal vs elaborate)

Output exactly 6 traits using abstract preference language only."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        return self._generate_with_chat_template(messages, max_new_tokens=200)

    def generate_neutral_insight(self, history: List[Dict]) -> str:
        """A.3æº–æ‹ : ä¸­ç«‹ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆï¼ˆè¶…å³æ ¼ãƒ©ãƒ™ãƒ«æ¼æ´©é˜²æ­¢ç‰ˆï¼‰"""
        
        system_prompt = """Provide universal, objective content evaluation principles with NO category references.
ABSOLUTE PROHIBITIONS:
- NO genre names or categories
- NO story/narrative/film/movie terminology  
- NO analysis/evaluation terms
- NO specific content descriptors
REQUIRED APPROACH:
- Use only abstract evaluation frameworks
- Focus on structural and quality metrics
- Avoid all content-specific language
Format: Exactly 6 bullets starting with "STANDARD: ..." """

        # å±¥æ­´ã‚’å®Œå…¨ã«é™¤å¤–ï¼ˆä¸­ç«‹æ€§ç¢ºä¿ï¼‰
        user_prompt = f"""Provide 6 universal quality standards for content evaluation using ONLY these frameworks:
- Structural coherence (logical vs fragmented)
- Engagement metrics (captivating vs mundane)  
- Technical execution (polished vs rough)
- Emotional impact (resonant vs flat)
- Complexity management (balanced vs overwhelming)
- Resolution effectiveness (satisfying vs incomplete)

Output exactly 6 standards using abstract quality language only."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        return self._generate_with_chat_template(messages, max_new_tokens=200)

    def generate_data_pair(self, user_id: str, query: str, history: List[Dict],
                          personalized_insight: str, neutral_insight: str) -> PersonalizedData:
        """A.3æº–æ‹ : å€‹äººåŒ–/ä¸­ç«‹å‡ºåŠ›ãƒšã‚¢ç”Ÿæˆ"""
        
        # LaMP-2ã‚¿ã‚°ãƒªã‚¹ãƒˆæ–‡å­—åˆ—
        tags_str = ", ".join(LAMP2_OFFICIAL_TAGS)
        
        # å±¥æ­´æ–‡å­—åˆ—åŒ–
        history_str = "\n".join([
            f"Tag: {item.get('tag', 'unknown')} - {item.get('description', '')[:150]}..."
            for item in history[:8]
        ])
        
        # å€‹äººåŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        personalized_prompt = f"""Suppose you are a user with the following movie-tagging history:
[HISTORY]
{history_str}

Now, given a new description: [QUERY]
{query}

Question: Which tag does this movie relate to among the following tags?
Just answer with only ONE tag name without further explanation.
tags: [{tags_str}]

You are a helpfully personalized assistant. The user prefers: {personalized_insight}.

Before answering, think step-by-step but do not reveal the reasoning.
Do not copy tags from the history; decide only from the [QUERY] and preferences.
Output must be exactly one tag string from the provided list.

Your answer:"""
        
        # ä¸­ç«‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        neutral_prompt = f"""Suppose you are analyzing the following movie description:
[QUERY]
{query}

Question: Which tag does this movie relate to among the following tags?
Just answer with only ONE tag name without further explanation.
tags: [{tags_str}]

You are a generic and impersonal assistant. Follow these neutral characteristics: {neutral_insight}.

Before answering, think step-by-step but do not reveal the reasoning.
Analyze only the [QUERY] content objectively.
Output must be exactly one tag string from the provided list.

Your answer:"""
        
        # ç”Ÿæˆå®Ÿè¡Œ
        personalized_output = self._generate_direct(personalized_prompt, max_new_tokens=10)
        neutral_output = self._generate_direct(neutral_prompt, max_new_tokens=10)
        
        return PersonalizedData(
            user_id=user_id,
            query=query,
            personalized_output=personalized_output.strip().lower(),
            neutral_output=neutral_output.strip().lower(),
            personalized_insight=personalized_insight,
            neutral_insight=neutral_insight,
            history=history
        )

    def estimate_theta_vectors_svd_ccs(self, data_pairs: List[PersonalizedData], 
                                      target_layers: List[str] = None) -> Dict[str, Dict[str, torch.Tensor]]:
        """SVD+CCSã«ã‚ˆã‚‹Î¸P/Î¸Næ¨å®š (è«–æ–‡A.4æº–æ‹ )"""
        
        if target_layers is None:
            target_layers = self._select_layers_by_css_loss()
        
        logger.info(f"Estimating Î¸ vectors for {len(target_layers)} layers with {len(data_pairs)} pairs")
        
        results = {}
        
        for layer_name in target_layers:
            logger.info(f"Processing layer: {layer_name}")
            
            # å„ãƒšã‚¢ã®åŸ‹ã‚è¾¼ã¿æŠ½å‡º
            personalized_embeds = []
            neutral_embeds = []
            
            for pair in data_pairs:
                # å€‹äººåŒ–å‡ºåŠ›ã®åŸ‹ã‚è¾¼ã¿
                p_embed = self._extract_layer_embedding(pair.personalized_output, layer_name)
                n_embed = self._extract_layer_embedding(pair.neutral_output, layer_name)
                
                if p_embed is not None and n_embed is not None:
                    personalized_embeds.append(p_embed.cpu().numpy())
                    neutral_embeds.append(n_embed.cpu().numpy())
            
            if len(personalized_embeds) < 2:
                logger.warning(f"Insufficient data for layer {layer_name}, skipping")
                continue
                
            # HP_l,u, HN_l,uæ§‹ç¯‰
            HP = np.array(personalized_embeds)  # [num_pairs, hidden_size]
            HN = np.array(neutral_embeds)
            
            # SVDã«ã‚ˆã‚‹Î¸Pæ¨å®š (ç¬¬ä¸€ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«)
            U_p, S_p, Vt_p = np.linalg.svd(HP, full_matrices=False)
            theta_p = torch.tensor(Vt_p[0], dtype=torch.float32, device=self.config.device)
            
            # CCSã«ã‚ˆã‚‹Î¸Næ¨å®š (å·®åˆ†ã®ä¸»æˆåˆ†)
            diff_matrix = HP - HN  # å€‹äººåŒ–-ä¸­ç«‹ã®å·®åˆ†
            U_diff, S_diff, Vt_diff = np.linalg.svd(diff_matrix, full_matrices=False)
            theta_n = torch.tensor(Vt_diff[0], dtype=torch.float32, device=self.config.device)
            
            # æ­£è¦åŒ–
            theta_p = F.normalize(theta_p, dim=0)
            theta_n = F.normalize(theta_n, dim=0)
            
            results[layer_name] = {
                'theta_p': theta_p,
                'theta_n': theta_n,
                'explained_variance_p': float(S_p[0] / S_p.sum()),
                'explained_variance_n': float(S_diff[0] / S_diff.sum())
            }
            
            logger.info(f"Layer {layer_name}: Î¸P var={results[layer_name]['explained_variance_p']:.3f}, "
                       f"Î¸N var={results[layer_name]['explained_variance_n']:.3f}")
        
        return results

    def apply_projection_editing(self, layer_name: str, x: torch.Tensor, 
                                theta_p: torch.Tensor, theta_n: torch.Tensor,
                                strength: float = 1.0, target_edit_ratio: float = 0.025,
                                edit_ratio_tolerance: float = 0.5) -> torch.Tensor:
        """è«–æ–‡æº–æ‹ : æŠ•å½±ã«ã‚ˆã‚‹ç·¨é›†ï¼ˆedit-ratioåˆ¶å¾¡ä»˜ãï¼‰"""
        
        # æŠ•å½±åŠ ç®—: xÌ‚_l â† x_l + (âŸ¨x_l, Î¸^P_l,uâŸ©/âŸ¨Î¸^P_l,u, Î¸^P_l,uâŸ©)Î¸^P_l,u
        dot_p = torch.sum(x * theta_p, dim=-1, keepdim=True)  # å†…ç©
        norm_p_sq = torch.sum(theta_p * theta_p)  # Î¸Pã®ãƒãƒ«ãƒ Â²
        projection_p = (dot_p / norm_p_sq) * theta_p.unsqueeze(0)  # æŠ•å½±
        
        x_hat = x + strength * projection_p
        
        # æŠ•å½±æ¸›ç®—: xÌ‚_l â† xÌ‚_l - (âŸ¨xÌ‚_l, Î¸^N_l,uâŸ©/âŸ¨Î¸^N_l,u, Î¸^N_l,uâŸ©)Î¸^N_l,u
        dot_n = torch.sum(x_hat * theta_n, dim=-1, keepdim=True)
        norm_n_sq = torch.sum(theta_n * theta_n)
        projection_n = (dot_n / norm_n_sq) * theta_n.unsqueeze(0)
        
        x_final = x_hat - strength * projection_n
        
        # Edit-ratioåˆ¶å¾¡: 2-3%ç¯„å›²ã«ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´ï¼ˆtarget_edit_ratioãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã®ã¿ï¼‰
        if target_edit_ratio is not None and target_edit_ratio > 0:
            edit_magnitude = torch.norm(x_final - x, dim=-1)
            original_magnitude = torch.norm(x, dim=-1)
            
            # ç·¨é›†æ¯”ç‡è¨ˆç®—ï¼ˆã‚¼ãƒ­é™¤ç®—å›é¿ï¼‰
            current_edit_ratio = edit_magnitude / (original_magnitude + 1e-8)
            avg_edit_ratio = current_edit_ratio.mean().item()
            
            # ç›®æ¨™ç¯„å›²(2-3%)å¤–ã®å ´åˆã€è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´
            min_ratio, max_ratio = target_edit_ratio * 0.8, target_edit_ratio * 1.2  # 2.0%, 3.0%
            
            if avg_edit_ratio < min_ratio or avg_edit_ratio > max_ratio:
                # ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•°è¨ˆç®—
                if avg_edit_ratio > 0:
                    scale_factor = target_edit_ratio / avg_edit_ratio
                    # éåº¦ãªèª¿æ•´ã‚’é˜²ããŸã‚ã€toleranceå†…ã§ã‚¯ãƒ©ãƒ³ãƒ—
                    scale_factor = torch.clamp(torch.tensor(scale_factor), 
                                             1.0 - edit_ratio_tolerance, 
                                             1.0 + edit_ratio_tolerance).item()
                else:
                    scale_factor = 1.0
                
                # ã‚¹ã‚±ãƒ¼ãƒ«é©ç”¨
                edit_vector = x_final - x
                x_final = x + scale_factor * edit_vector
                
                # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°
                final_edit_ratio = (torch.norm(x_final - x, dim=-1) / (original_magnitude + 1e-8)).mean().item()
                logger.debug(f"Layer {layer_name}: edit-ratio {avg_edit_ratio:.4f} -> {final_edit_ratio:.4f} "
                            f"(scale: {scale_factor:.3f})")
        
        return x_final

    def register_projection_hooks(self, theta_vectors: Dict[str, Dict[str, torch.Tensor]], 
                                 strength: float = 1.0, target_edit_ratio: float = 0.025,
                                 edit_ratio_tolerance: float = 0.5) -> List:
        """æŠ•å½±ç·¨é›†ãƒ•ãƒƒã‚¯ç™»éŒ²ï¼ˆedit-ratioåˆ¶å¾¡ä»˜ãï¼‰"""
        
        hooks = []
        
        for layer_name, vectors in theta_vectors.items():
            theta_p = vectors['theta_p']
            theta_n = vectors['theta_n']
            
            def make_hook(tp, tn, ln, ter, ert):
                def projection_hook(module, input, output):
                    if isinstance(output, tuple):
                        x = output[0]  # MLPå‡ºåŠ›
                    else:
                        x = output
                    
                    # æŠ•å½±ç·¨é›†é©ç”¨ï¼ˆedit-ratioåˆ¶å¾¡ä»˜ãï¼‰
                    x_edited = self.apply_projection_editing(
                        ln, x, tp, tn, strength, ter, ert
                    )
                    
                    if isinstance(output, tuple):
                        return (x_edited,) + output[1:]
                    else:
                        return x_edited
                
                return projection_hook
            
            # å±¤å–å¾—ã¨ãƒ•ãƒƒã‚¯ç™»éŒ²
            layer = self._get_layer_by_name(layer_name)
            hook = layer.register_forward_hook(make_hook(theta_p, theta_n, layer_name, 
                                                       target_edit_ratio, edit_ratio_tolerance))
            hooks.append(hook)
            
            logger.info(f"Registered projection hook on {layer_name} (target edit-ratio: {target_edit_ratio:.1%})")
        
        return hooks

    def _select_layers_by_css_loss(self) -> List[str]:
        """CSSæå¤±ã«ã‚ˆã‚‹å±¤é¸æŠ (ç°¡æ˜“ç‰ˆ)"""
        # å®Ÿè£…ã®ç°¡æ˜“åŒ–ã®ãŸã‚ã€ä¸­é–“å±¤ã‚’é¸æŠ
        # å®Ÿéš›ã®è«–æ–‡å®Ÿè£…ã§ã¯ã€å„å±¤ã§CSSæå¤±ã‚’è¨ˆç®—ã—ã¦æœ€å°ã®å±¤ã‚’é¸ã¶
        total_layers = len([n for n, _ in self.model.named_modules() if 'layers.' in n and '.mlp' in n])
        
        # ä¸­é–“ã®3å±¤ã‚’é¸æŠ (CSSæå¤±è¨ˆç®—ã®ä»£æ›¿)
        selected_indices = [
            total_layers // 4,      # 1/4ä½ç½®
            total_layers // 2,      # ä¸­å¤®
            3 * total_layers // 4   # 3/4ä½ç½®
        ]
        
        selected_layers = [f"model.layers.{i}.mlp" for i in selected_indices if i < total_layers]
        logger.info(f"Auto-selected layers by CSS proxy: {selected_layers}")
        
        return selected_layers

    def _get_layer_by_name(self, layer_name: str):
        """å±¤åã‹ã‚‰å®Ÿéš›ã®å±¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå–å¾—"""
        layer = self.model
        for part in layer_name.split('.'):
            if part.isdigit():
                layer = layer[int(part)]
            else:
                layer = getattr(layer, part)
        return layer

    def _extract_layer_embedding(self, text: str, layer_name: str) -> Optional[torch.Tensor]:
        """æŒ‡å®šå±¤ã®åŸ‹ã‚è¾¼ã¿æŠ½å‡º"""
        embeddings = []
        
        def extraction_hook(module, input, output):
            if isinstance(output, tuple):
                embeddings.append(output[0].detach().clone())
            else:
                embeddings.append(output.detach().clone())
        
        # ãƒ•ãƒƒã‚¯ç™»éŒ²
        layer = self._get_layer_by_name(layer_name)
        hook = layer.register_forward_hook(extraction_hook)
        
        try:
            # é †ä¼æ’­å®Ÿè¡Œ
            inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)
            inputs = {k: v.to(self.config.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                _ = self.model(**inputs)
            
            if embeddings:
                # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨
                return embeddings[0][:, -1, :].mean(dim=0)  # [batch_size, seq_len, hidden] -> [hidden]
            else:
                return None
                
        finally:
            hook.remove()

    def _generate_with_chat_template(self, messages: List[Dict], max_new_tokens: int = 100) -> str:
        """ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½¿ç”¨ç”Ÿæˆ"""
        try:
            formatted_prompt = self.tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
        except:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ç´”é€£çµ
            formatted_prompt = f"System: {messages[0]['content']}\nUser: {messages[1]['content']}\nAssistant:"
        
        return self._generate_direct(formatted_prompt, max_new_tokens)

    def estimate_tag_uncertainty(self, prompt: str, temperature: float = 1.0) -> float:
        """æ”¹è‰¯ã•ã‚ŒãŸã‚¿ã‚°å°¤åº¦ãƒ™ãƒ¼ã‚¹ä¸ç¢ºå®Ÿæ€§æ¨å®š"""
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’"Answer:"ã§çµ‚ã‚ã‚‹ã‚ˆã†ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        formatted_prompt = prompt.strip()
        if not formatted_prompt.endswith("Answer:"):
            formatted_prompt += " Answer:"
        
        inputs = self.tokenizer(formatted_prompt, return_tensors='pt', padding=True, truncation=True)
        inputs = {k: v.to(self.config.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            # ãƒ¢ãƒ‡ãƒ«ã®logitså–å¾—
            outputs = self.model(**inputs, return_dict=True)
            logits = outputs.logits[:, -1, :]  # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®äºˆæ¸¬åˆ†å¸ƒ
            
            # å„ã‚¿ã‚°ã®æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã®å°¤åº¦ã‚’åé›†
            tag_logprobs = []
            tag_names = []
            
            for tag in LAMP2_OFFICIAL_TAGS:
                # ã‚¿ã‚°ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆæœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ï¼‰
                tag_tokens = self.tokenizer.encode(" " + tag, add_special_tokens=False)  # ã‚¹ãƒšãƒ¼ã‚¹ä»˜ãã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
                if not tag_tokens:
                    tag_tokens = self.tokenizer.encode(tag, add_special_tokens=False)
                
                if tag_tokens:
                    tag_token_id = tag_tokens[0]
                    tag_logprob = F.log_softmax(logits / temperature, dim=-1)[0, tag_token_id].item()
                    tag_logprobs.append(tag_logprob)
                    tag_names.append(tag)
                else:
                    logger.warning(f"Tag '{tag}' could not be tokenized")
        
        if not tag_logprobs:
            logger.error("No valid tag tokens found")
            return 1.0  # æœ€å¤§ä¸ç¢ºå®Ÿæ€§
        
        # ã‚¿ã‚°é–“ã§ã®ã¿æ­£è¦åŒ–ï¼ˆå…¨vocabä¸Šã§ã¯ãªãï¼‰
        tag_logprobs = torch.tensor(tag_logprobs, dtype=torch.float32)
        tag_probs = F.softmax(tag_logprobs, dim=0)
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
        entropy = -(tag_probs * torch.log(tag_probs + 1e-8)).sum().item()
        max_entropy = np.log(len(tag_logprobs))  # å®Ÿéš›ã®ã‚¿ã‚°æ•°ã§ã®æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        
        # æ­£è¦åŒ–
        normalized_uncertainty = entropy / max_entropy if max_entropy > 0 else 1.0
        
        # æœ€é«˜ç¢ºç‡ã‚¿ã‚°ã¨ã®å·®ã‚’è€ƒæ…®ã—ãŸèª¿æ•´
        max_prob = tag_probs.max().item()
        confidence = max_prob  # æœ€é«˜ç¢ºç‡ = ä¿¡é ¼åº¦
        
        # ä¸ç¢ºå®Ÿæ€§ = ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ + (1 - ä¿¡é ¼åº¦)ã®é‡ã¿ä»˜ãå¹³å‡
        final_uncertainty = 0.7 * normalized_uncertainty + 0.3 * (1 - confidence)
        
        logger.debug(f"Tag uncertainty: entropy={normalized_uncertainty:.3f}, confidence={confidence:.3f}, final={final_uncertainty:.3f}")
        logger.debug(f"Top3 tags: {[(tag_names[i], tag_probs[i].item()) for i in tag_probs.topk(3)[1]]}")
        
        return final_uncertainty

    def generate_with_uncertainty_gating(self, prompt: str, user_profile: List[Dict],
                                       personalized_strength: float = 1.0,
                                       uncertainty_threshold: float = 0.6) -> Tuple[str, float]:
        """ä¸ç¢ºå®Ÿæ€§ã‚²ãƒ¼ãƒˆã¤ãç”Ÿæˆ"""
        
        # ä¸ç¢ºå®Ÿæ€§æ¨å®š
        uncertainty = self.estimate_tag_uncertainty(prompt)
        
        # ä¸ç¢ºå®Ÿæ€§ã«åŸºã¥ã„ãŸå€‹äººåŒ–å¼·åº¦èª¿æ•´
        if uncertainty > uncertainty_threshold:
            # é«˜ä¸ç¢ºå®Ÿæ€§ â†’ å¼·ã„å€‹äººåŒ–
            effective_strength = personalized_strength * 1.5
            logger.info(f"High uncertainty ({uncertainty:.3f}) â†’ strong personalization ({effective_strength:.1f})")
        else:
            # ä½ä¸ç¢ºå®Ÿæ€§ â†’ å¼±ã„å€‹äººåŒ–
            effective_strength = personalized_strength * 0.7
            logger.info(f"Low uncertainty ({uncertainty:.3f}) â†’ weak personalization ({effective_strength:.1f})")
        
        # èª¿æ•´ã•ã‚ŒãŸå¼·åº¦ã§ç”Ÿæˆ
        response = self._generate_direct(prompt, max_new_tokens=10)
        
        return response, uncertainty

    def _generate_direct(self, prompt: str, max_new_tokens: int = 100) -> str:
        """ç›´æ¥ç”Ÿæˆ"""
        inputs = self.tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)
        inputs = {k: v.to(self.config.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                pad_token_id=self.tokenizer.eos_token_id,
                temperature=None,
                top_p=None
            )
        
        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = generated[len(prompt):].strip()
        
        return response


def test_paper_compliant_implementation():
    """è«–æ–‡æº–æ‹ å®Ÿè£…ãƒ†ã‚¹ãƒˆ"""
    
    print("ğŸ§ª è«–æ–‡CHAMELEONæº–æ‹ å®Ÿè£…ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    # è¨­å®š
    config = ChameleonConfig(
        model_path="./chameleon_prime_personalization/models/base_model",
        device="cuda"
    )
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    chameleon = PaperCompliantChameleon(config)
    
    # ãƒ†ã‚¹ãƒˆç”¨å±¥æ­´
    test_history = [
        {"tag": "psychology", "description": "A petty criminal fakes insanity to serve his sentence in a mental ward rather than prison."},
        {"tag": "action", "description": "When a virus leaks from a top-secret facility, turning all resident researchers into ravenous zombies."},
        {"tag": "classic", "description": "Overwhelmed by her suffocating schedule, touring European princess Ann takes off for a night while in Rome."}
    ]
    
    # ãƒ†ã‚¹ãƒˆç”¨ã‚¯ã‚¨ãƒª
    test_query = "A young FBI cadet must confide in an incarcerated and manipulative killer to receive his help on catching another serial killer who skins his victims."
    
    print("\n1ï¸âƒ£ ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ")
    print("-" * 30)
    
    # å€‹äººåŒ–ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
    personalized_insight = chameleon.generate_personalized_insight(test_history)
    print(f"å€‹äººåŒ–ã‚¤ãƒ³ã‚µã‚¤ãƒˆ:\n{personalized_insight}")
    
    # ä¸­ç«‹ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
    neutral_insight = chameleon.generate_neutral_insight(test_history)
    print(f"\nä¸­ç«‹ã‚¤ãƒ³ã‚µã‚¤ãƒˆ:\n{neutral_insight}")
    
    print("\n2ï¸âƒ£ ãƒ‡ãƒ¼ã‚¿ãƒšã‚¢ç”Ÿæˆãƒ†ã‚¹ãƒˆ")
    print("-" * 30)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒšã‚¢ç”Ÿæˆ
    data_pair = chameleon.generate_data_pair(
        user_id="test_user",
        query=test_query,
        history=test_history,
        personalized_insight=personalized_insight,
        neutral_insight=neutral_insight
    )
    
    print(f"å€‹äººåŒ–å‡ºåŠ›: {data_pair.personalized_output}")
    print(f"ä¸­ç«‹å‡ºåŠ›: {data_pair.neutral_output}")
    
    print("\n3ï¸âƒ£ Î¸ãƒ™ã‚¯ãƒˆãƒ«æ¨å®šãƒ†ã‚¹ãƒˆ")  
    print("-" * 30)
    
    # è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ãƒšã‚¢ã§Î¸æ¨å®šã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ãŸã‚ã€åŒã˜ãƒšã‚¢ã‚’è¤‡è£½
    test_pairs = [data_pair] * 3  # æœ€å°é™ã®ãƒ†ã‚¹ãƒˆ
    
    # Î¸ãƒ™ã‚¯ãƒˆãƒ«æ¨å®š
    theta_vectors = chameleon.estimate_theta_vectors_svd_ccs(
        test_pairs, 
        target_layers=["model.layers.15.mlp", "model.layers.20.mlp"]
    )
    
    for layer_name, vectors in theta_vectors.items():
        print(f"Layer {layer_name}:")
        print(f"  Î¸P shape: {vectors['theta_p'].shape}, variance: {vectors['explained_variance_p']:.3f}")
        print(f"  Î¸N shape: {vectors['theta_n'].shape}, variance: {vectors['explained_variance_n']:.3f}")
    
    print("\nâœ… è«–æ–‡æº–æ‹ å®Ÿè£…ãƒ†ã‚¹ãƒˆå®Œäº†!")
    print("ğŸ“‹ å®Ÿè£…æ¸ˆã¿:")
    print("  - A.3æº–æ‹ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ")
    print("  - 15ç¨®LaMP-2ã‚¿ã‚°çµ±ä¸€")
    print("  - SVD+CCSã«ã‚ˆã‚‹Î¸æ¨å®š")
    print("  - æŠ•å½±ç·¨é›†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯")

if __name__ == "__main__":
    test_paper_compliant_implementation()