#!/usr/bin/env python3
"""
CFS-Chameleonå‘ã‘ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ãƒ”ãƒ¼ã‚¹ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«æ–¹å‘ã¨å¯¾ç…§çš„ãªãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒ”ãƒ¼ã‚¹åŒ–å®Ÿè£…
"""

import numpy as np
import torch
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import logging
import time
import json
from pathlib import Path

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from sentence_transformers import SentenceTransformer
    from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    print("âš ï¸ Transformers/SentenceTransformers not available. Using mock implementations.")
    TRANSFORMERS_AVAILABLE = False

# CFS-Chameleoné–¢é€£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
try:
    from cfs_chameleon_extension import DirectionPiece
    CFS_AVAILABLE = True
except ImportError:
    print("âš ï¸ CFS modules not available. Using mock implementations.")
    CFS_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class NeutralDirectionPiece:
    """ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ãƒ”ãƒ¼ã‚¹æ§‹é€ """
    u_component: np.ndarray      # å·¦ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«æˆåˆ†
    singular_value: float        # ç‰¹ç•°å€¤
    v_component: np.ndarray      # å³ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«æˆåˆ†
    importance: float            # é‡è¦åº¦ï¼ˆç‰¹ç•°å€¤ã®å¯„ä¸ç‡ï¼‰
    quality_score: float         # å“è³ªã‚¹ã‚³ã‚¢
    semantic_context: str        # æ„å‘³çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆneutral_ã‚¿ã‚°ä»˜ãï¼‰
    source_history_indices: List[int]  # å…ƒå±¥æ­´ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    creation_timestamp: float    # ä½œæˆæ™‚åˆ»
    direction_type: str = "neutral"  # æ–¹å‘ã‚¿ã‚¤ãƒ—è­˜åˆ¥

class NeutralDirectionGenerator:
    """ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ãƒ”ãƒ¼ã‚¹ç”Ÿæˆå™¨"""
    
    def __init__(self, 
                 llm_model_name: str = "meta-llama/Llama-3.2-3B-Instruct",
                 embedding_model_name: str = "all-MiniLM-L6-v2",
                 device: str = "cuda"):
        """
        åˆæœŸåŒ–
        
        Args:
            llm_model_name: ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«è¨€ã„æ›ãˆç”Ÿæˆç”¨LLMãƒ¢ãƒ‡ãƒ«å
            embedding_model_name: åŸ‹ã‚è¾¼ã¿ç”¨ãƒ¢ãƒ‡ãƒ«å
            device: è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹
        """
        self.device = device
        self.llm_model_name = llm_model_name
        self.embedding_model_name = embedding_model_name
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        self._initialize_models()
        
        logger.info(f"âœ… NeutralDirectionGenerator initialized")
        logger.info(f"   LLM: {llm_model_name}")
        logger.info(f"   Embedding: {embedding_model_name}")
        logger.info(f"   Device: {device}")
    
    def _initialize_models(self):
        """ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–"""
        if TRANSFORMERS_AVAILABLE:
            try:
                # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
                self.embedding_model = SentenceTransformer(self.embedding_model_name)
                logger.info(f"âœ… Embedding model loaded: {self.embedding_model_name}")
                
                # LLMãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«è¨€ã„æ›ãˆç”¨ï¼‰
                self.tokenizer = AutoTokenizer.from_pretrained(self.llm_model_name)
                self.llm_model = AutoModelForCausalLM.from_pretrained(
                    self.llm_model_name,
                    torch_dtype=torch.float16,
                    device_map="auto" if self.device == "cuda" else "cpu"
                )
                
                # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                logger.info(f"âœ… LLM model loaded: {self.llm_model_name}")
                
            except Exception as e:
                logger.error(f"âŒ Model initialization error: {e}")
                self.embedding_model = None
                self.llm_model = None
                self.tokenizer = None
        else:
            # ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«
            self.embedding_model = None
            self.llm_model = None
            self.tokenizer = None
            logger.warning("âš ï¸ Using mock models for demonstration")
    
    def generate_neutral_paraphrases(self, 
                                   original_text: str, 
                                   num_variants: int = 3) -> List[str]:
        """
        LLMã‚’ä½¿ç”¨ã—ã¦ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ãƒ»æ±ç”¨çš„ãªè¨€ã„æ›ãˆã‚’ç”Ÿæˆ
        
        Args:
            original_text: å…ƒãƒ†ã‚­ã‚¹ãƒˆ
            num_variants: ç”Ÿæˆã™ã‚‹è¨€ã„æ›ãˆæ•°
            
        Returns:
            ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«è¨€ã„æ›ãˆãƒªã‚¹ãƒˆ
        """
        if self.llm_model is None or self.tokenizer is None:
            # ãƒ¢ãƒƒã‚¯å®Ÿè£…
            return self._mock_neutral_paraphrase_generation(original_text, num_variants)
        
        try:
            neutral_paraphrases = []
            
            # ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ã®è¨€ã„æ›ãˆç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            neutral_prompt = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã€ã‚ˆã‚Šä¸­ç«‹çš„ã§å®¢è¦³çš„ãªè¡¨ç¾ã«è¨€ã„æ›ãˆã¦ãã ã•ã„ã€‚å€‹äººçš„ãªæ„Ÿæƒ…ã‚„ä¸»è¦³ã‚’å–ã‚Šé™¤ãã€ä¸€èˆ¬çš„ã§æ±ç”¨çš„ãªè¡¨ç¾ã«ã—ã¦ãã ã•ã„ã€‚{num_variants}ã¤ã®ç•°ãªã‚‹è¨€ã„æ›ãˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

å…ƒãƒ†ã‚­ã‚¹ãƒˆ: {original_text}

ä¸­ç«‹çš„ãªè¨€ã„æ›ãˆ:"""
            
            neutral_variants = self._generate_text_variants(neutral_prompt, num_variants)
            neutral_paraphrases.extend(neutral_variants)
            
            logger.debug(f"Generated {len(neutral_paraphrases)} neutral paraphrases")
            
            return neutral_paraphrases
            
        except Exception as e:
            logger.error(f"âŒ Neutral paraphrase generation error: {e}")
            return self._mock_neutral_paraphrase_generation(original_text, num_variants)
    
    def _generate_text_variants(self, prompt: str, num_variants: int) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆå¤‰ç¨®ç”Ÿæˆ"""
        variants = []
        
        for i in range(num_variants):
            try:
                inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
                inputs = {k: v.to(self.llm_model.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = self.llm_model.generate(
                        **inputs,
                        max_length=inputs['input_ids'].shape[1] + 60,
                        do_sample=True,
                        temperature=0.7,
                        top_p=0.9,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                
                generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                variant = generated[len(prompt):].strip()
                
                if variant and len(variant) > 10:  # æœ‰åŠ¹ãªç”Ÿæˆçµæœã®ã¿è¿½åŠ 
                    variants.append(variant)
                    
            except Exception as e:
                logger.warning(f"Generation attempt {i+1} failed: {e}")
                continue
        
        return variants
    
    def _mock_neutral_paraphrase_generation(self, original_text: str, num_variants: int) -> List[str]:
        """ãƒ¢ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«è¨€ã„æ›ãˆç”Ÿæˆï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰"""
        neutral_variants = [
            f"{original_text}ã¨ã„ã†çŠ¶æ³ãŒä¸€èˆ¬çš„ã«è¦³å¯Ÿã•ã‚Œã¾ã™",
            f"ä¸€èˆ¬çš„ã«{original_text}ã¨ã„ã†ã“ã¨ãŒè¨€ãˆã¾ã™",
            f"å®¢è¦³çš„ã«è¦‹ã‚‹ã¨{original_text}ã§ã™"
        ][:num_variants]
        
        return neutral_variants
    
    def compute_embeddings(self, texts: List[str]) -> np.ndarray:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿è¨ˆç®—"""
        if self.embedding_model is None:
            # ãƒ¢ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿
            return np.random.randn(len(texts), 384)  # MiniLMã®æ¬¡å…ƒæ•°
        
        try:
            embeddings = self.embedding_model.encode(texts, convert_to_numpy=True)
            return embeddings
        except Exception as e:
            logger.error(f"âŒ Embedding computation error: {e}")
            return np.random.randn(len(texts), 384)
    
    def compute_neutral_direction_vectors(self, 
                                        original_texts: List[str], 
                                        neutral_paraphrases_list: List[List[str]]) -> np.ndarray:
        """
        å…ƒãƒ†ã‚­ã‚¹ãƒˆãƒ»ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«è¨€ã„æ›ãˆã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—
        
        Args:
            original_texts: å…ƒãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
            neutral_paraphrases_list: å„ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«è¨€ã„æ›ãˆãƒªã‚¹ãƒˆ
            
        Returns:
            ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ã®è¡Œåˆ— (num_pairs, embedding_dim)
        """
        neutral_direction_vectors = []
        
        # å„ãƒ†ã‚­ã‚¹ãƒˆã¨ãã®ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«è¨€ã„æ›ãˆã‹ã‚‰æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—
        for i, (original_text, neutral_paraphrases) in enumerate(zip(original_texts, neutral_paraphrases_list)):
            
            # å…ƒãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿
            original_emb = self.compute_embeddings([original_text])[0]
            
            for neutral_text in neutral_paraphrases:
                # ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«è¨€ã„æ›ãˆã®åŸ‹ã‚è¾¼ã¿
                neutral_emb = self.compute_embeddings([neutral_text])[0]
                
                # ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ« = neutral - original
                # ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ã¸ã®ç§»å‹•ã‚’è¡¨ã™ï¼‰
                neutral_direction_vec = neutral_emb - original_emb
                
                # æ­£è¦åŒ–
                neutral_direction_vec = neutral_direction_vec / (np.linalg.norm(neutral_direction_vec) + 1e-8)
                
                neutral_direction_vectors.append(neutral_direction_vec)
        
        return np.array(neutral_direction_vectors)
    
    def perform_neutral_svd_decomposition(self, 
                                        neutral_direction_matrix: np.ndarray, 
                                        rank_reduction: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«è¡Œåˆ—ã®SVDåˆ†è§£
        
        Args:
            neutral_direction_matrix: ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«è¡Œåˆ—
            rank_reduction: ãƒ©ãƒ³ã‚¯å‰Šæ¸›æ•°
            
        Returns:
            (U, S, Vt) - SVDåˆ†è§£çµæœ
        """
        try:
            # SVDåˆ†è§£å®Ÿè¡Œ
            U, S, Vt = np.linalg.svd(neutral_direction_matrix, full_matrices=False)
            
            # ãƒ©ãƒ³ã‚¯å‰Šæ¸›
            rank = min(rank_reduction, len(S))
            U_reduced = U[:, :rank]
            S_reduced = S[:rank]
            Vt_reduced = Vt[:rank, :]
            
            logger.info(f"âœ… Neutral SVD decomposition completed")
            logger.info(f"   Original shape: {neutral_direction_matrix.shape}")
            logger.info(f"   Reduced rank: {rank}")
            logger.info(f"   Variance explained: {np.sum(S_reduced**2) / np.sum(S**2):.4f}")
            
            return U_reduced, S_reduced, Vt_reduced
            
        except Exception as e:
            logger.error(f"âŒ Neutral SVD decomposition error: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ä½è¡Œåˆ—
            dim = neutral_direction_matrix.shape[1]
            rank = min(rank_reduction, dim)
            U_reduced = np.eye(neutral_direction_matrix.shape[0], rank)
            S_reduced = np.ones(rank)
            Vt_reduced = np.eye(rank, dim)
            
            return U_reduced, S_reduced, Vt_reduced
    
    def create_neutral_direction_pieces(self, 
                                      U: np.ndarray, 
                                      S: np.ndarray, 
                                      Vt: np.ndarray,
                                      history_texts: List[str]) -> List[NeutralDirectionPiece]:
        """
        SVDçµæœã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ãƒ”ãƒ¼ã‚¹ã‚’ä½œæˆ
        
        Args:
            U, S, Vt: SVDåˆ†è§£çµæœ
            history_texts: å…ƒå±¥æ­´ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ãƒ”ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆ
        """
        pieces = []
        total_variance = np.sum(S**2)
        
        for i in range(len(S)):
            # é‡è¦åº¦è¨ˆç®—ï¼ˆç‰¹ç•°å€¤ã®å¯„ä¸ç‡ï¼‰
            importance = (S[i]**2) / total_variance
            
            # å“è³ªã‚¹ã‚³ã‚¢ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ã§ã¯ç‰¹ç•°å€¤ãƒ™ãƒ¼ã‚¹ï¼‰
            quality_score = importance * 0.8  # ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã¯è‹¥å¹²æ§ãˆã‚ã«è¨­å®š
            
            # æ„å‘³çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆneutral_ã‚¿ã‚°ä»˜ãï¼‰
            base_context = f"neutral_component_{i+1}"
            if i < len(history_texts):
                # å…ƒå±¥æ­´ã‹ã‚‰æ¨å®šã•ã‚Œã‚‹ä¸­ç«‹åŒ–ã‚«ãƒ†ã‚´ãƒª
                if "æ˜ ç”»" in history_texts[i] or "å¨¯æ¥½" in history_texts[i]:
                    base_context = "neutral_entertainment"
                elif "æ–™ç†" in history_texts[i] or "é£Ÿäº‹" in history_texts[i]:
                    base_context = "neutral_cooking"
                elif "æŠ€è¡“" in history_texts[i] or "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°" in history_texts[i]:
                    base_context = "neutral_technology"
                else:
                    base_context = "neutral_general"
            
            # å…ƒå±¥æ­´ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆã“ã®æˆåˆ†ã«æœ€ã‚‚å¯„ä¸ã™ã‚‹å±¥æ­´ã‚’ç‰¹å®šï¼‰
            source_indices = []
            if i < U.shape[1]:
                # Uæˆåˆ†ã§æœ€ã‚‚å¤§ããªå€¤ã‚’æŒã¤å±¥æ­´ã‚’ç‰¹å®š
                u_component = U[:, i]
                top_indices = np.argsort(np.abs(u_component))[-3:]  # ä¸Šä½3ã¤
                source_indices = top_indices.tolist()
            
            piece = NeutralDirectionPiece(
                u_component=U[:, i] if i < U.shape[1] else np.zeros(U.shape[0]),
                singular_value=S[i],
                v_component=Vt[i, :] if i < Vt.shape[0] else np.zeros(Vt.shape[1]),
                importance=importance,
                quality_score=quality_score,
                semantic_context=base_context,
                source_history_indices=source_indices,
                creation_timestamp=time.time(),
                direction_type="neutral"
            )
            
            pieces.append(piece)
        
        # é‡è¦åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        pieces.sort(key=lambda x: x.importance, reverse=True)
        
        logger.info(f"âœ… Created {len(pieces)} neutral direction pieces")
        logger.info(f"   Top piece importance: {pieces[0].importance:.4f}")
        logger.info(f"   Total variance covered: {sum(p.importance for p in pieces):.4f}")
        
        return pieces

def generate_neutral_direction_pieces(
    user_history_texts: List[str],
    llm_model: Any = None,
    embedding_model: Any = None,
    rank_reduction: int = 16
) -> List[Dict[str, Any]]:
    """
    ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ã‚’ãƒ”ãƒ¼ã‚¹åŒ–ã—ã¦è¿”ã™é–¢æ•°
    
    Args:
        user_history_texts: ãƒ¦ãƒ¼ã‚¶å±¥æ­´ã®ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
        llm_model: neutral è¨€ã„æ›ãˆç”Ÿæˆç”¨ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆæœªä½¿ç”¨ã€äº’æ›æ€§ã®ãŸã‚ï¼‰
        embedding_model: ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆæœªä½¿ç”¨ã€äº’æ›æ€§ã®ãŸã‚ï¼‰
        rank_reduction: SVD ã®ãƒ©ãƒ³ã‚¯å‰Šæ¸›æ•°
        
    Returns:
        List[Dict]: ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ã®ãƒ”ãƒ¼ã‚¹æƒ…å ±è¾æ›¸ãƒªã‚¹ãƒˆ
    """
    logger.info(f"ğŸš€ Starting neutral direction pieces generation")
    logger.info(f"   History texts: {len(user_history_texts)}")
    logger.info(f"   Rank reduction: {rank_reduction}")
    
    # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼åˆæœŸåŒ–
    generator = NeutralDirectionGenerator()
    
    all_neutral_paraphrases = []
    
    # å„å±¥æ­´ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«è¨€ã„æ›ãˆã‚’ç”Ÿæˆ
    for i, history_text in enumerate(user_history_texts):
        logger.info(f"ğŸ“ Processing history {i+1}/{len(user_history_texts)}: {history_text[:50]}...")
        
        try:
            # ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«è¨€ã„æ›ãˆç”Ÿæˆ
            neutral_paraphrases = generator.generate_neutral_paraphrases(
                history_text, num_variants=3
            )
            
            all_neutral_paraphrases.append(neutral_paraphrases)
            
        except Exception as e:
            logger.error(f"âŒ Error processing history {i+1}: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ¢ãƒƒã‚¯è¨€ã„æ›ãˆ
            all_neutral_paraphrases.append([f"ä¸­ç«‹çš„ãªè¡¨ç¾: {history_text}"])
            continue
    
    if not all_neutral_paraphrases or all(not paraphrases for paraphrases in all_neutral_paraphrases):
        logger.error("âŒ No neutral paraphrases generated")
        return []
    
    # ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—
    neutral_direction_matrix = generator.compute_neutral_direction_vectors(
        user_history_texts, all_neutral_paraphrases
    )
    
    logger.info(f"ğŸ“Š Neutral direction matrix shape: {neutral_direction_matrix.shape}")
    
    # SVDåˆ†è§£å®Ÿè¡Œ
    U, S, Vt = generator.perform_neutral_svd_decomposition(neutral_direction_matrix, rank_reduction)
    
    # ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ãƒ”ãƒ¼ã‚¹ä½œæˆ
    neutral_pieces = generator.create_neutral_direction_pieces(U, S, Vt, user_history_texts)
    
    # è¾æ›¸å½¢å¼ã«å¤‰æ›
    pieces_dict = []
    for piece in neutral_pieces:
        piece_dict = {
            "u_component": piece.u_component.tolist(),
            "singular_value": float(piece.singular_value),
            "v_component": piece.v_component.tolist(),
            "importance": float(piece.importance),
            "quality_score": float(piece.quality_score),
            "semantic_context": piece.semantic_context,
            "source_history_indices": piece.source_history_indices,
            "creation_timestamp": piece.creation_timestamp,
            "direction_type": piece.direction_type
        }
        pieces_dict.append(piece_dict)
    
    logger.info(f"ğŸ‰ Neutral direction pieces generation completed!")
    logger.info(f"   Generated {len(pieces_dict)} neutral pieces")
    
    return pieces_dict

def demonstrate_neutral_direction_generation():
    """ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ãƒ”ãƒ¼ã‚¹ç”Ÿæˆã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("ğŸ”„ ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ãƒ”ãƒ¼ã‚¹ç”Ÿæˆãƒ‡ãƒ¢")
    print("=" * 60)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    user_history = [
        "ä»Šæ—¥ã¯æ˜ ç”»ã‚’è¦‹ã«è¡ŒããŸã„æ°—åˆ†ã§ã™",
        "é¢ç™½ã„SFæ˜ ç”»ãŒå¤§å¥½ãã§ã™", 
        "æœ€è¿‘è¦‹ãŸæ˜ ç”»ã¯æœ¬å½“ã«ç´ æ™´ã‚‰ã—ã‹ã£ãŸ",
        "æ˜ ç”»é¤¨ã§å‹é”ã¨æ¥½ã—ã„æ™‚é–“ã‚’éã”ã—ã¾ã—ãŸ",
        "æ–°ã—ã„æ˜ ç”»ã®äºˆå‘Šç·¨ã‚’è¦‹ã¦ãƒ¯ã‚¯ãƒ¯ã‚¯ã—ã¦ã„ã¾ã™"
    ]
    
    # ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ãƒ”ãƒ¼ã‚¹ç”Ÿæˆå®Ÿè¡Œ
    start_time = time.time()
    neutral_pieces = generate_neutral_direction_pieces(
        user_history_texts=user_history,
        rank_reduction=8
    )
    execution_time = time.time() - start_time
    
    # çµæœè¡¨ç¤º
    print(f"\nğŸ“Š ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ãƒ”ãƒ¼ã‚¹ç”Ÿæˆçµæœ:")
    print(f"   å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’")
    print(f"   ç”Ÿæˆãƒ”ãƒ¼ã‚¹æ•°: {len(neutral_pieces)}")
    
    for i, piece in enumerate(neutral_pieces[:3]):  # ä¸Šä½3ã¤ã®ã¿è¡¨ç¤º
        print(f"\nğŸ”¸ ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ãƒ”ãƒ¼ã‚¹ {i+1}:")
        print(f"   ç‰¹ç•°å€¤: {piece['singular_value']:.4f}")
        print(f"   é‡è¦åº¦: {piece['importance']:.4f}")
        print(f"   å“è³ªã‚¹ã‚³ã‚¢: {piece['quality_score']:.4f}")
        print(f"   æ„å‘³çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {piece['semantic_context']}")
        print(f"   æ–¹å‘ã‚¿ã‚¤ãƒ—: {piece['direction_type']}")
        print(f"   Uæˆåˆ†æ¬¡å…ƒ: {len(piece['u_component'])}")
        print(f"   Væˆåˆ†æ¬¡å…ƒ: {len(piece['v_component'])}")
    
    # ãƒ‘ãƒ¼ã‚½ãƒŠãƒ« vs ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã®æ¯”è¼ƒèª¬æ˜
    print(f"\nğŸ”„ ãƒ‘ãƒ¼ã‚½ãƒŠãƒ« vs ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ãƒ”ãƒ¼ã‚¹æ¯”è¼ƒ:")
    print(f"   ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«æ–¹å‘: å€‹äººçš„ãƒ»æ„Ÿæƒ…çš„ãªè¡¨ç¾ã¸ã®æ–¹å‘")
    print(f"   ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘: ä¸­ç«‹çš„ãƒ»å®¢è¦³çš„ãªè¡¨ç¾ã¸ã®æ–¹å‘")
    print(f"   CFSé©ç”¨æ™‚: ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«å¼·èª¿(+Î±) + ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æŠ‘åˆ¶(-Î²)")
    
    print("\nğŸ‰ ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ãƒ”ãƒ¼ã‚¹ç”Ÿæˆãƒ‡ãƒ¢å®Œäº†!")

if __name__ == "__main__":
    demonstrate_neutral_direction_generation()