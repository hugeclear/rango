#!/usr/bin/env python3
"""
LaMP-2 è«–æ–‡æº–æ‹ å®Ÿè£…
- FlanT5-base æ—¢å®šã€FlanT5-XXL/GPT-3.5 ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆå¯¾å¿œ
- IPA/FiD ä¸¡æ–¹å¼å®Ÿè£…
- BM25/Contriever ãƒªãƒˆãƒªãƒ¼ãƒ
- PPEPâ†’AIP ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆTable 5 æº–æ‹ ï¼‰
- BERTScore è¿‘å‚å†™åƒ
- iU/iT åˆ†å‰²å¯¾å¿œ

å‚è€ƒ: LaMPè«–æ–‡ Table 5 (è¡Œç•ªå·47-52: LaMP-2 template specification)
"""

import json
import os
import logging
import argparse
import numpy as np
import torch
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional, Union
from dataclasses import dataclass, field
from transformers import (
    AutoTokenizer, AutoModelForSeq2SeqLM, 
    Trainer, TrainingArguments, DataCollatorForSeq2Seq,
    pipeline
)
from datasets import Dataset
from sklearn.metrics import accuracy_score, f1_score
from sentence_transformers import SentenceTransformer
import openai
from rank_bm25 import BM25Okapi
from bert_score import score as bert_score
import re
from collections import Counter

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# LaMP-2 15ã‚¯ãƒ©ã‚¹å›ºå®šï¼ˆè«–æ–‡æº–æ‹ ï¼‰
LAMP2_CLASSES = [
    'sci-fi', 'based on a book', 'comedy', 'action', 'twist ending',
    'dystopia', 'dark comedy', 'classic', 'psychology', 'fantasy', 
    'romance', 'thought-provoking', 'social commentary', 'violence', 'true story'
]

@dataclass
class LaMP2Config:
    """LaMP-2å®Ÿé¨“è¨­å®š"""
    model_name: str = "google/flan-t5-base"  # æ—¢å®š: FlanT5-base
    device: str = "cuda"
    data_path: str = "./data/LaMP-2"
    
    # ãƒªãƒˆãƒªãƒ¼ãƒè¨­å®š
    retriever_type: str = "bm25"  # bm25 | contriever
    retrieval_k: int = 8  # IPAç”¨æ—¢å®š
    fid_k: int = 16  # FiDç”¨æ—¢å®šï¼ˆè«–æ–‡æº–æ‹ ï¼‰
    
    # å­¦ç¿’è¨­å®š
    train_epochs: int = 20  # ç”Ÿæˆç³»æ—¢å®šï¼ˆè«–æ–‡è¨­è¨ˆï¼‰
    learning_rate: float = 5e-5
    batch_size: int = 8
    warmup_steps: int = 100
    
    # è©•ä¾¡è¨­å®š
    split_type: str = "iU"  # iU | iT
    method: str = "ipa"  # ipa | fid | non_personalized
    zero_shot: bool = False  # FlanT5-XXL/GPT-3.5ç”¨
    
    # å‡ºåŠ›å‡¦ç†
    use_bertscore_mapping: bool = True
    bertscore_threshold: float = 0.8

class LaMP2DataLoader:
    """LaMP-2ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼"""
    
    def __init__(self, config: LaMP2Config):
        self.config = config
        self.data_path = Path(config.data_path)
        
    def load_data(self) -> Dict[str, List[Dict]]:
        """iU/iTåˆ†å‰²ã§ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        logger.info(f"Loading LaMP-2 data with split: {self.config.split_type}")
        
        # å®Ÿè£…æ³¨: LaMPè«–æ–‡ã®ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ä»•æ§˜ï¼ˆiU: user-based, iT: time-basedï¼‰
        if self.config.split_type == "iU":
            train_file = self.data_path / "train_iU.json"
            dev_file = self.data_path / "dev_iU.json" 
            test_file = self.data_path / "test_iU.json"
        else:  # iT
            train_file = self.data_path / "train_iT.json"
            dev_file = self.data_path / "dev_iT.json"
            test_file = self.data_path / "test_iT.json"
        
        data = {}
        for split, file_path in [("train", train_file), ("dev", dev_file), ("test", test_file)]:
            if file_path.exists():
                with open(file_path, 'r') as f:
                    data[split] = json.load(f)
                logger.info(f"Loaded {len(data[split])} {split} samples")
            else:
                logger.warning(f"File not found: {file_path}")
                data[split] = []
        
        return data

class LaMP2Retriever:
    """LaMP-2ãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ï¼ˆBM25/Contrieverï¼‰"""
    
    def __init__(self, config: LaMP2Config):
        self.config = config
        self.retriever_type = config.retriever_type
        
        if self.retriever_type == "contriever":
            # Contrieveräº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
            self.encoder = SentenceTransformer('facebook-dpr-ctx_encoder-single-nq-base')
            logger.info("Initialized Contriever retriever")
        else:
            # BM25ç”¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆç°¡æ˜“ï¼‰
            logger.info("Initialized BM25 retriever")
    
    def build_index(self, profile_texts: List[str]):
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æ–‡æ›¸ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰"""
        if self.retriever_type == "bm25":
            # BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆPyseriniä»£æ›¿ã®ç°¡æ˜“å®Ÿè£…ï¼‰
            tokenized = [text.lower().split() for text in profile_texts]
            self.bm25 = BM25Okapi(tokenized)
            self.profile_texts = profile_texts
        else:
            # Contrieverã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°
            self.profile_embeddings = self.encoder.encode(profile_texts)
            self.profile_texts = profile_texts
    
    def retrieve(self, query: str, k: int) -> List[str]:
        """ä¸Šä½kä»¶ãƒªãƒˆãƒªãƒ¼ãƒãƒ«"""
        if self.retriever_type == "bm25":
            query_tokens = query.lower().split()
            scores = self.bm25.get_scores(query_tokens)
            top_k_indices = np.argsort(scores)[-k:][::-1]
            return [self.profile_texts[i] for i in top_k_indices]
        else:
            # Contrieverã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
            query_emb = self.encoder.encode([query])
            similarities = np.dot(query_emb, self.profile_embeddings.T)[0]
            top_k_indices = np.argsort(similarities)[-k:][::-1]
            return [self.profile_texts[i] for i in top_k_indices]

class LaMP2Prompter:
    """LaMP-2 PPEPâ†’AIPãƒ—ãƒ­ãƒ³ãƒ—ã‚¿ãƒ¼"""
    
    def __init__(self):
        # LaMPè«–æ–‡ Table 5æº–æ‹ ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆè¡Œç•ªå·47-52å‚ç…§ï¼‰
        # PPEP (Per-Profile Example Prompting): å±¥æ­´å´ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.ppep_template = 'the tag for the movie: "{description}" is "{tag}"'
        
        # AIP (Aggregated In-context Prompting): å…¥åŠ›å´çµ±åˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ  
        self.aip_prefix = ", and "
        
    def create_ppep(self, profile_item: Dict[str, str]) -> str:
        """å˜ä¸€ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«é …ç›®ã®PPEPç”Ÿæˆ"""
        return self.ppep_template.format(
            description=profile_item.get('description', ''),
            tag=profile_item.get('tag', '')
        )
    
    def create_aip(self, retrieved_items: List[Dict[str, str]], input_description: str) -> str:
        """AIPçµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆï¼ˆTable 5æº–æ‹ ï¼‰"""
        # è¤‡æ•°PPEPã‚’", and "ã§çµåˆ
        ppep_parts = [self.create_ppep(item) for item in retrieved_items]
        ppep_context = self.aip_prefix.join(ppep_parts)
        
        # æœ€çµ‚å…¥åŠ›å½¢å¼
        full_prompt = f'{ppep_context}{self.aip_prefix}the tag for the movie: "{input_description}" is "'
        
        return full_prompt
    
    def create_non_personalized_prompt(self, input_description: str) -> str:
        """éå€‹äººåŒ–ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³"""
        return f'the tag for the movie: "{input_description}" is "'

class LaMP2Model:
    """LaMP-2ãƒ¢ãƒ‡ãƒ«ï¼ˆIPA/FiDå¯¾å¿œï¼‰"""
    
    def __init__(self, config: LaMP2Config):
        self.config = config
        self.device = torch.device(config.device)
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        if config.zero_shot and "gpt-3.5" in config.model_name:
            # GPT-3.5 APIä½¿ç”¨
            self.model_type = "gpt"
            logger.info("Using GPT-3.5-turbo API")
        else:
            # FlanT5ç³»
            self.model_type = "flant5"
            self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(
                config.model_name,
                torch_dtype=torch.float32,
                device_map='auto'
            )
            logger.info(f"Loaded {config.model_name}")
    
    def generate_ipa(self, prompt: str, max_length: int = 10) -> str:
        """IPAæ¨è«–"""
        if self.model_type == "gpt":
            # GPT-3.5ç”Ÿæˆï¼ˆãƒã‚¹ãƒˆãƒ—ãƒ­ã‚»ã‚¹å¯¾å¿œï¼‰
            try:
                response = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_length,
                    temperature=0.0
                )
                return response.choices[0].message.content.strip()
            except Exception as e:
                logger.error(f"GPT-3.5 API error: {e}")
                return "unknown"
        else:
            # FlanT5ç”Ÿæˆ
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_length,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            return generated.strip()
    
    def generate_fid(self, retrieved_items: List[Dict[str, str]], input_description: str) -> str:
        """FiDæ¨è«–ï¼ˆ16ä»¶åˆ†é›¢ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰"""
        if self.model_type == "gpt":
            # GPT-3.5ã§ã¯FiDã‚’ä½¿ç”¨ã—ãªã„
            logger.warning("FiD not supported for GPT-3.5, falling back to IPA")
            prompter = LaMP2Prompter()
            prompt = prompter.create_aip(retrieved_items, input_description)
            return self.generate_ipa(prompt)
        
        # FlanT5-FiDå®Ÿè£…ï¼ˆå…¸å‹çš„ãªFiD: åˆ†é›¢ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰â†’ãƒ‡ã‚³ãƒ¼ãƒ€çµ±åˆï¼‰
        fid_prompts = []
        prompter = LaMP2Prompter()
        
        # å„æ–‡æ›¸ã‚’å€‹åˆ¥ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåŒ–
        for item in retrieved_items[:self.config.fid_k]:
            individual_prompt = prompter.create_aip([item], input_description)
            fid_prompts.append(individual_prompt)
        
        # ãƒãƒƒãƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        inputs = self.tokenizer(
            fid_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # FiDæ¨è«–ï¼ˆç°¡æ˜“ç‰ˆï¼šè¤‡æ•°å…¥åŠ›ã®å¹³å‡ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
        with torch.no_grad():
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼å‡ºåŠ›ã‚’å¹³å‡åŒ–
            encoder_outputs = self.model.encoder(**inputs)
            pooled_hidden = encoder_outputs.last_hidden_state.mean(dim=0, keepdim=True)
            
            # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã§ç”Ÿæˆ
            decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id]], device=self.device)
            outputs = self.model.generate(
                encoder_outputs=type('EncoderOutput', (), {'last_hidden_state': pooled_hidden})(),
                decoder_input_ids=decoder_input_ids,
                max_new_tokens=10,
                do_sample=False
            )
        
        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return generated.strip()

class LaMP2OutputProcessor:
    """å‡ºåŠ›æ­£è¦åŒ–ãƒ»BERTScoreè¿‘å‚å†™åƒ"""
    
    def __init__(self, config: LaMP2Config):
        self.config = config
        self.classes = LAMP2_CLASSES
        
        # åŒç¾©èªãƒãƒƒãƒ—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        self.synonym_map = {
            'scifi': 'sci-fi',
            'science fiction': 'sci-fi',
            'book': 'based on a book',
            'funny': 'comedy',
            'violent': 'violence',
            'fight': 'action',
            'romantic': 'romance'
        }
    
    def normalize_output(self, raw_output: str) -> str:
        """ç”Ÿæˆèªå‰å‡¦ç†"""
        # å°æ–‡å­—åŒ–ãƒ»è¨˜å·çµ±ä¸€
        normalized = raw_output.lower().strip()
        normalized = re.sub(r'[^\w\s-]', '', normalized)  # å¥èª­ç‚¹å‰Šé™¤
        normalized = re.sub(r'\s+', ' ', normalized)  # ç©ºç™½æ­£è¦åŒ–
        normalized = normalized.replace('_', '-')  # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢çµ±ä¸€
        
        # åŒç¾©èªå†™åƒ
        if normalized in self.synonym_map:
            normalized = self.synonym_map[normalized]
        
        return normalized
    
    def bertscore_mapping(self, normalized_output: str) -> Tuple[str, float]:
        """BERTScoreæœ€è¿‘å‚å†™åƒ"""
        if not self.config.use_bertscore_mapping:
            return normalized_output, 1.0
        
        # ã‚¯ãƒ©ã‚¹å†…ãƒã‚§ãƒƒã‚¯
        if normalized_output in self.classes:
            return normalized_output, 1.0
        
        # BERTScoreè¨ˆç®—ï¼ˆå„ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã¨ã®é¡ä¼¼åº¦ï¼‰
        try:
            _, _, f1_scores = bert_score([normalized_output] * len(self.classes), self.classes, lang="en")
            best_idx = f1_scores.argmax().item()
            best_score = f1_scores[best_idx].item()
            best_label = self.classes[best_idx]
            
            logger.debug(f"BERTScore mapping: '{normalized_output}' -> '{best_label}' (score: {best_score:.3f})")
            return best_label, best_score
        except Exception as e:
            logger.error(f"BERTScore error: {e}")
            return self.classes[0], 0.0  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    
    def process_output(self, raw_output: str) -> Dict[str, Any]:
        """å®Œå…¨å‡ºåŠ›å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
        normalized = self.normalize_output(raw_output)
        final_label, confidence = self.bertscore_mapping(normalized)
        
        return {
            'raw': raw_output,
            'normalized': normalized,
            'final': final_label,
            'confidence': confidence,
            'was_mapped': final_label != normalized
        }

class LaMP2Evaluator:
    """LaMP-2è©•ä¾¡å™¨"""
    
    def __init__(self, config: LaMP2Config):
        self.config = config
        self.data_loader = LaMP2DataLoader(config)
        self.retriever = LaMP2Retriever(config)
        self.model = LaMP2Model(config)
        self.prompter = LaMP2Prompter()
        self.output_processor = LaMP2OutputProcessor(config)
        
    def setup_retriever(self, train_data: List[Dict]):
        """ãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰"""
        logger.info("Building retriever index...")
        profile_texts = []
        for item in train_data:
            for profile_item in item.get('profile', []):
                desc = profile_item.get('description', '')
                if desc:
                    profile_texts.append(desc)
        
        self.retriever.build_index(profile_texts)
        self.profile_data = train_data  # ãƒªãƒˆãƒªãƒ¼ãƒãƒ«ç”¨
        logger.info(f"Built index with {len(profile_texts)} documents")
    
    def retrieve_for_sample(self, sample: Dict[str, Any], k: int) -> List[Dict[str, str]]:
        """ã‚µãƒ³ãƒ—ãƒ«ç”¨ãƒªãƒˆãƒªãƒ¼ãƒãƒ«"""
        query = sample.get('input', '')
        user_profile = sample.get('profile', [])
        
        if not user_profile:
            return []
        
        # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æ–‡æ›¸ã‹ã‚‰ãƒªãƒˆãƒªãƒ¼ãƒãƒ«
        profile_texts = [item.get('description', '') for item in user_profile]
        retrieved_texts = self.retriever.retrieve(query, min(k, len(profile_texts)))
        
        # å¯¾å¿œã™ã‚‹ã‚¿ã‚°æƒ…å ±ã‚’å¾©å…ƒ
        retrieved_items = []
        for text in retrieved_texts:
            for profile_item in user_profile:
                if profile_item.get('description', '') == text:
                    retrieved_items.append({
                        'description': text,
                        'tag': profile_item.get('tag', 'unknown')
                    })
                    break
        
        return retrieved_items
    
    def evaluate_method(self, test_data: List[Dict], method: str) -> Dict[str, Any]:
        """æŒ‡å®šæ‰‹æ³•ã§ã®è©•ä¾¡"""
        logger.info(f"Evaluating with method: {method}")
        
        predictions = []
        ground_truths = []
        processing_stats = {
            'total': 0,
            'mapped': 0,
            'ool_rate': 0.0,
            'confidence_scores': []
        }
        
        for i, sample in enumerate(test_data):
            if i % 50 == 0:
                logger.info(f"Processing {i+1}/{len(test_data)}")
            
            input_desc = sample.get('input', '')
            ground_truth = sample.get('output', 'unknown')
            
            # æ‰‹æ³•åˆ¥ç”Ÿæˆ
            if method == "non_personalized":
                prompt = self.prompter.create_non_personalized_prompt(input_desc)
                prediction = self.model.generate_ipa(prompt)
                
            elif method == "ipa":
                k = self.config.retrieval_k
                retrieved = self.retrieve_for_sample(sample, k)
                prompt = self.prompter.create_aip(retrieved, input_desc)
                prediction = self.model.generate_ipa(prompt)
                
            elif method == "fid":
                k = self.config.fid_k
                retrieved = self.retrieve_for_sample(sample, k)
                prediction = self.model.generate_fid(retrieved, input_desc)
                
            else:
                raise ValueError(f"Unknown method: {method}")
            
            # å‡ºåŠ›å‡¦ç†
            processed = self.output_processor.process_output(prediction)
            
            predictions.append(processed['final'])
            ground_truths.append(ground_truth)
            
            # çµ±è¨ˆåé›†
            processing_stats['total'] += 1
            if processed['was_mapped']:
                processing_stats['mapped'] += 1
            processing_stats['confidence_scores'].append(processed['confidence'])
        
        # OOLç‡è¨ˆç®—
        processing_stats['ool_rate'] = processing_stats['mapped'] / processing_stats['total']
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        accuracy = accuracy_score(ground_truths, predictions)
        f1_macro = f1_score(ground_truths, predictions, average='macro', zero_division=0)
        
        # BERTScoreåˆ†å¸ƒçµ±è¨ˆ
        conf_scores = processing_stats['confidence_scores']
        bertscore_stats = {
            'mean': np.mean(conf_scores),
            'std': np.std(conf_scores),
            'min': np.min(conf_scores),
            'max': np.max(conf_scores)
        }
        
        return {
            'method': method,
            'split': self.config.split_type,
            'retriever': self.config.retriever_type,
            'k': self.config.fid_k if method == 'fid' else self.config.retrieval_k,
            'accuracy': accuracy,
            'f1_macro': f1_macro,
            'ool_rate': processing_stats['ool_rate'],
            'bertscore_stats': bertscore_stats,
            'predictions': predictions,
            'ground_truths': ground_truths,
            'sample_count': len(test_data)
        }
    
    def run_full_evaluation(self) -> Dict[str, Any]:
        """å®Œå…¨è©•ä¾¡å®Ÿè¡Œ"""
        logger.info("Starting LaMP-2 paper-compliant evaluation")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        data = self.data_loader.load_data()
        train_data = data['train']
        dev_data = data['dev'] 
        test_data = data['test']
        
        if not test_data:
            logger.error("No test data found")
            return {}
        
        # ãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        self.setup_retriever(train_data)
        
        # å„æ‰‹æ³•ã§ã®è©•ä¾¡
        results = {}
        methods = ["non_personalized", "ipa", "fid"]
        
        for method in methods:
            if self.config.zero_shot and method == "fid":
                logger.info(f"Skipping FiD for zero-shot evaluation")
                continue
                
            try:
                result = self.evaluate_method(test_data, method)
                results[method] = result
            except Exception as e:
                logger.error(f"Error evaluating {method}: {e}")
                results[method] = {'error': str(e)}
        
        return results

def print_evaluation_results(results: Dict[str, Any]):
    """è©•ä¾¡çµæœè¡¨ç¤º"""
    print("\n" + "=" * 80)
    print("ğŸ“Š LaMP-2 Paper-Compliant Evaluation Results")
    print("=" * 80)
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼
    print(f"{'Method':<20} | {'Accuracy':<10} | {'F1-Macro':<10} | {'OOL Rate':<10} | {'Samples':<8}")
    print("-" * 80)
    
    # å„æ‰‹æ³•çµæœ
    for method, result in results.items():
        if 'error' in result:
            print(f"{method:<20} | {'ERROR':<10} | {'ERROR':<10} | {'ERROR':<10} | {'N/A':<8}")
        else:
            print(f"{method:<20} | {result['accuracy']:<10.3f} | {result['f1_macro']:<10.3f} | "
                  f"{result['ool_rate']:<10.3f} | {result['sample_count']:<8}")
    
    # è©³ç´°çµ±è¨ˆ
    print(f"\nğŸ“‹ Configuration:")
    if results:
        first_result = next(iter(results.values()))
        if 'split' in first_result:
            print(f"  Split: {first_result['split']}")
            print(f"  Retriever: {first_result['retriever']}")
            
    # BERTScoreåˆ†å¸ƒ
    print(f"\nğŸ“Š BERTScore Mapping Statistics:")
    for method, result in results.items():
        if 'bertscore_stats' in result:
            stats = result['bertscore_stats']
            print(f"  {method}: mean={stats['mean']:.3f}, std={stats['std']:.3f}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    parser = argparse.ArgumentParser(description="LaMP-2 Paper-Compliant Evaluation")
    parser.add_argument("--model", default="google/flan-t5-base", 
                       help="Model name (flan-t5-base/flan-t5-xxl/gpt-3.5-turbo)")
    parser.add_argument("--split", choices=["iU", "iT"], default="iU",
                       help="Data split type")
    parser.add_argument("--retriever", choices=["bm25", "contriever"], default="bm25",
                       help="Retriever type")
    parser.add_argument("--method", choices=["non_personalized", "ipa", "fid", "all"], 
                       default="all", help="Evaluation method")
    parser.add_argument("--zero-shot", action="store_true",
                       help="Zero-shot evaluation (FlanT5-XXL/GPT-3.5)")
    parser.add_argument("--retrieval-k", type=int, default=8,
                       help="Retrieval k for IPA")
    parser.add_argument("--fid-k", type=int, default=16,
                       help="FiD document count")
    
    args = parser.parse_args()
    
    # è¨­å®šç”Ÿæˆ
    config = LaMP2Config(
        model_name=args.model,
        split_type=args.split,
        retriever_type=args.retriever,
        zero_shot=args.zero_shot,
        retrieval_k=args.retrieval_k,
        fid_k=args.fid_k
    )
    
    logger.info(f"Starting evaluation with model: {args.model}")
    logger.info(f"Configuration: split={args.split}, retriever={args.retriever}, zero_shot={args.zero_shot}")
    
    # è©•ä¾¡å®Ÿè¡Œ
    evaluator = LaMP2Evaluator(config)
    results = evaluator.run_full_evaluation()
    
    # çµæœè¡¨ç¤º
    print_evaluation_results(results)
    
    print("\nğŸ‰ LaMP-2 Paper-Compliant Evaluation Complete!")
    print("âœ… Implementation features:")
    print("  - FlanT5-base default with XXL/GPT-3.5 zero-shot support")
    print("  - IPA/FiD methods with BM25/Contriever retrievers") 
    print("  - PPEPâ†’AIP prompting (Table 5 compliant)")
    print("  - BERTScore nearest neighbor mapping")
    print("  - iU/iT split support")
    print("  - 15-class LaMP-2 evaluation")

if __name__ == "__main__":
    main()