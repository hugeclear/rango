#!/usr/bin/env python3
"""
CFS-Chameleonå‘ã‘ã‚¿ã‚¹ã‚¯é©å¿œåŒ–ãƒ”ãƒ¼ã‚¹çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
å®Ÿéš›ã®ã‚¿ã‚¹ã‚¯æ€§èƒ½ã«åŸºã¥ãå‹•çš„é‡ã¿ä»˜ã‘ã«ã‚ˆã‚‹æœ€é©ãªæ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«èåˆ
"""

import numpy as np
import torch
from typing import List, Dict, Any, Optional, Tuple, Callable, Union
import logging
import time
import json
from pathlib import Path
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed
import functools

# æ—¢å­˜ã®å“è³ªè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
from task_based_quality_evaluator import (
    TaskBasedQualityEvaluator,
    QualityEvaluationConfig,
    calculate_improved_quality_score
)

# CFS-Chameleoné–¢é€£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
try:
    from cfs_chameleon_extension import DirectionPiece, CollaborativeDirectionPool
    from chameleon_cfs_integrator import CollaborativeChameleonEditor
    CFS_AVAILABLE = True
except ImportError:
    print("âš ï¸ CFS modules not available. Using mock implementations.")
    CFS_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class AdaptiveFusionConfig:
    """ã‚¿ã‚¹ã‚¯é©å¿œåŒ–çµ±åˆè¨­å®š"""
    # é‡ã¿è¨ˆç®—æ–¹å¼
    weight_method: str = "softmax"  # softmax, linear, learned
    temperature: float = 1.0        # softmaxæ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    
    # è©•ä¾¡è¨­å®š
    eval_sample_size: int = 20      # è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«æ•°
    metrics: List[str] = None       # ä½¿ç”¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    parallel_evaluation: bool = True # ä¸¦åˆ—è©•ä¾¡ãƒ•ãƒ©ã‚°
    max_workers: int = 4            # ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
    
    # çµ±åˆè¨­å®š
    min_weight_threshold: float = 0.01  # æœ€å°é‡ã¿é–¾å€¤
    normalize_vectors: bool = True      # ãƒ™ã‚¯ãƒˆãƒ«æ­£è¦åŒ–ãƒ•ãƒ©ã‚°
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
    cache_evaluations: bool = True      # è©•ä¾¡çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
    cache_max_size: int = 1000         # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€å¤§ã‚µã‚¤ã‚º
    
    def __post_init__(self):
        if self.metrics is None:
            self.metrics = ["rouge", "bertscore"]

class PiecePerformanceEvaluator:
    """ãƒ”ãƒ¼ã‚¹å˜ä½“æ€§èƒ½è©•ä¾¡ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, config: AdaptiveFusionConfig = None):
        """
        åˆæœŸåŒ–
        
        Args:
            config: é©å¿œåŒ–çµ±åˆè¨­å®š
        """
        self.config = config or AdaptiveFusionConfig()
        
        # å“è³ªè©•ä¾¡å™¨ã®åˆæœŸåŒ–
        self.quality_evaluator = TaskBasedQualityEvaluator()
        
        # è©•ä¾¡ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.evaluation_cache = {} if self.config.cache_evaluations else None
        
        logger.info("âœ… PiecePerformanceEvaluator initialized")
        logger.info(f"   Weight method: {self.config.weight_method}")
        logger.info(f"   Metrics: {self.config.metrics}")
        logger.info(f"   Parallel evaluation: {self.config.parallel_evaluation}")
    
    def _get_cache_key(self, piece_id: str, eval_data_hash: str) -> str:
        """è©•ä¾¡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        return f"{piece_id}_{eval_data_hash}_{'-'.join(self.config.metrics)}"
    
    def _hash_eval_dataset(self, eval_dataset: List[Tuple[str, str]]) -> str:
        """è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒãƒƒã‚·ãƒ¥å€¤è¨ˆç®—"""
        import hashlib
        data_str = json.dumps(eval_dataset, sort_keys=True)
        return hashlib.md5(data_str.encode()).hexdigest()[:8]
    
    def evaluate_single_piece(self, 
                            piece: Any,
                            eval_dataset: List[Tuple[str, str]],
                            generate_with_piece: Callable[[str, Any], str]) -> float:
        """
        å˜ä¸€ãƒ”ãƒ¼ã‚¹ã®æ€§èƒ½è©•ä¾¡
        
        Args:
            piece: è©•ä¾¡å¯¾è±¡ã®DirectionPiece
            eval_dataset: è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ [(input, reference), ...]
            generate_with_piece: (prompt, piece) -> ç”Ÿæˆæ–‡ ã®é–¢æ•°
            
        Returns:
            æ€§èƒ½ã‚¹ã‚³ã‚¢ (0.0-1.0)
        """
        piece_id = getattr(piece, 'piece_id', str(id(piece)))
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if self.evaluation_cache is not None:
            data_hash = self._hash_eval_dataset(eval_dataset)
            cache_key = self._get_cache_key(piece_id, data_hash)
            
            if cache_key in self.evaluation_cache:
                logger.debug(f"Cache hit for piece {piece_id}")
                return self.evaluation_cache[cache_key]
        
        try:
            # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™
            eval_sample = eval_dataset[:self.config.eval_sample_size]
            
            logger.debug(f"ğŸ” Evaluating piece {piece_id} on {len(eval_sample)} samples")
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ¥ã‚¹ã‚³ã‚¢è¨ˆç®—
            total_scores = []
            
            for input_text, reference_text in eval_sample:
                try:
                    # ãƒ”ãƒ¼ã‚¹ã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆ
                    generated_text = generate_with_piece(input_text, piece)
                    
                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
                    sample_scores = []
                    
                    if "rouge" in self.config.metrics:
                        rouge_score = self.quality_evaluator.compute_rouge_score(
                            generated_text, reference_text
                        )
                        sample_scores.append(rouge_score)
                    
                    if "bleu" in self.config.metrics:
                        bleu_score = self.quality_evaluator.compute_bleu_score(
                            generated_text, reference_text
                        )
                        sample_scores.append(bleu_score)
                    
                    if "bertscore" in self.config.metrics:
                        bert_score = self.quality_evaluator.compute_bert_score(
                            generated_text, reference_text
                        )
                        sample_scores.append(bert_score)
                    
                    # ã‚µãƒ³ãƒ—ãƒ«å¹³å‡ã‚¹ã‚³ã‚¢
                    if sample_scores:
                        sample_avg = np.mean(sample_scores)
                        total_scores.append(sample_avg)
                    
                except Exception as e:
                    logger.warning(f"Sample evaluation error: {e}")
                    continue
            
            # å…¨ä½“å¹³å‡ã‚¹ã‚³ã‚¢
            if total_scores:
                performance_score = np.mean(total_scores)
            else:
                logger.warning(f"No valid scores for piece {piece_id}, using default")
                performance_score = 0.1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½ã‚¹ã‚³ã‚¢
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            if self.evaluation_cache is not None:
                if len(self.evaluation_cache) >= self.config.cache_max_size:
                    # å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤ï¼ˆç°¡å˜ãªå®Ÿè£…ï¼‰
                    oldest_key = next(iter(self.evaluation_cache))
                    del self.evaluation_cache[oldest_key]
                
                self.evaluation_cache[cache_key] = performance_score
            
            logger.debug(f"âœ… Piece {piece_id} performance: {performance_score:.4f}")
            return float(performance_score)
            
        except Exception as e:
            logger.error(f"âŒ Piece evaluation error for {piece_id}: {e}")
            return 0.1
    
    def evaluate_pieces_batch(self,
                            pieces: List[Any],
                            eval_dataset: List[Tuple[str, str]],
                            generate_with_piece: Callable[[str, Any], str]) -> List[float]:
        """
        è¤‡æ•°ãƒ”ãƒ¼ã‚¹ã®ãƒãƒƒãƒæ€§èƒ½è©•ä¾¡
        
        Args:
            pieces: è©•ä¾¡å¯¾è±¡ã®DirectionPieceãƒªã‚¹ãƒˆ
            eval_dataset: è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            generate_with_piece: ç”Ÿæˆé–¢æ•°
            
        Returns:
            å„ãƒ”ãƒ¼ã‚¹ã®æ€§èƒ½ã‚¹ã‚³ã‚¢ãƒªã‚¹ãƒˆ
        """
        logger.info(f"ğŸš€ Batch evaluating {len(pieces)} pieces on {len(eval_dataset)} samples")
        
        if not self.config.parallel_evaluation or len(pieces) == 1:
            # ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«è©•ä¾¡
            performance_scores = []
            for i, piece in enumerate(pieces):
                logger.info(f"   Evaluating piece {i+1}/{len(pieces)}")
                score = self.evaluate_single_piece(piece, eval_dataset, generate_with_piece)
                performance_scores.append(score)
            
        else:
            # ä¸¦åˆ—è©•ä¾¡
            performance_scores = [0.0] * len(pieces)
            
            with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
                # ã‚¿ã‚¹ã‚¯æŠ•å…¥
                future_to_index = {}
                for i, piece in enumerate(pieces):
                    future = executor.submit(
                        self.evaluate_single_piece, piece, eval_dataset, generate_with_piece
                    )
                    future_to_index[future] = i
                
                # çµæœåé›†
                completed = 0
                for future in as_completed(future_to_index):
                    index = future_to_index[future]
                    try:
                        score = future.result()
                        performance_scores[index] = score
                        completed += 1
                        logger.info(f"   Completed {completed}/{len(pieces)} evaluations")
                    except Exception as e:
                        logger.error(f"Parallel evaluation error for piece {index}: {e}")
                        performance_scores[index] = 0.1
        
        logger.info(f"âœ… Batch evaluation completed")
        logger.info(f"   Score range: {min(performance_scores):.4f} - {max(performance_scores):.4f}")
        logger.info(f"   Average score: {np.mean(performance_scores):.4f}")
        
        return performance_scores

class AdaptiveWeightCalculator:
    """é©å¿œçš„é‡ã¿è¨ˆç®—å™¨"""
    
    def __init__(self, config: AdaptiveFusionConfig = None):
        """
        åˆæœŸåŒ–
        
        Args:
            config: é©å¿œåŒ–çµ±åˆè¨­å®š
        """
        self.config = config or AdaptiveFusionConfig()
        
        logger.info("âœ… AdaptiveWeightCalculator initialized")
        logger.info(f"   Weight method: {self.config.weight_method}")
        logger.info(f"   Temperature: {self.config.temperature}")
    
    def compute_softmax_weights(self, performance_scores: List[float]) -> np.ndarray:
        """
        Softmaxé‡ã¿è¨ˆç®—
        
        Args:
            performance_scores: æ€§èƒ½ã‚¹ã‚³ã‚¢ãƒªã‚¹ãƒˆ
            
        Returns:
            æ­£è¦åŒ–ã•ã‚ŒãŸé‡ã¿é…åˆ—
        """
        scores = np.array(performance_scores)
        
        # æ¸©åº¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaled_scores = scores / self.config.temperature
        
        # Softmaxè¨ˆç®—
        exp_scores = np.exp(scaled_scores - np.max(scaled_scores))  # æ•°å€¤å®‰å®šåŒ–
        weights = exp_scores / np.sum(exp_scores)
        
        logger.debug(f"Softmax weights computed: {weights}")
        return weights
    
    def compute_linear_weights(self, performance_scores: List[float]) -> np.ndarray:
        """
        ç·šå½¢é‡ã¿è¨ˆç®—
        
        Args:
            performance_scores: æ€§èƒ½ã‚¹ã‚³ã‚¢ãƒªã‚¹ãƒˆ
            
        Returns:
            æ­£è¦åŒ–ã•ã‚ŒãŸé‡ã¿é…åˆ—
        """
        scores = np.array(performance_scores)
        
        # æ­£ã®å€¤ã«èª¿æ•´
        min_score = np.min(scores)
        if min_score < 0:
            scores = scores - min_score
        
        # æ­£è¦åŒ–
        total_score = np.sum(scores)
        if total_score > 0:
            weights = scores / total_score
        else:
            # å…¨ã‚¹ã‚³ã‚¢ãŒ0ã®å ´åˆã¯å‡ç­‰é‡ã¿
            weights = np.ones(len(scores)) / len(scores)
        
        logger.debug(f"Linear weights computed: {weights}")
        return weights
    
    def compute_learned_weights(self, 
                              performance_scores: List[float],
                              pieces: List[Any] = None) -> np.ndarray:
        """
        å­¦ç¿’ãƒ™ãƒ¼ã‚¹é‡ã¿è¨ˆç®—ï¼ˆç°¡å˜ãªç·šå½¢å›å¸°ï¼‰
        
        Args:
            performance_scores: æ€§èƒ½ã‚¹ã‚³ã‚¢ãƒªã‚¹ãƒˆ
            pieces: DirectionPieceãƒªã‚¹ãƒˆï¼ˆç‰¹å¾´æŠ½å‡ºç”¨ï¼‰
            
        Returns:
            å­¦ç¿’ã•ã‚ŒãŸé‡ã¿é…åˆ—
        """
        # ç°¡å˜ãªå®Ÿè£…: æ€§èƒ½ã‚¹ã‚³ã‚¢ + ãƒ”ãƒ¼ã‚¹ç‰¹å¾´ã®ç·šå½¢çµåˆ
        scores = np.array(performance_scores)
        
        if pieces is not None:
            # ãƒ”ãƒ¼ã‚¹ç‰¹å¾´ã®æŠ½å‡ºï¼ˆé‡è¦åº¦ã€å“è³ªã‚¹ã‚³ã‚¢ç­‰ï¼‰
            piece_features = []
            for piece in pieces:
                features = []
                
                # é‡è¦åº¦ç‰¹å¾´
                if hasattr(piece, 'importance'):
                    features.append(piece.importance)
                elif isinstance(piece, dict) and 'importance' in piece:
                    features.append(piece['importance'])
                else:
                    features.append(0.5)
                
                # å“è³ªã‚¹ã‚³ã‚¢ç‰¹å¾´  
                if hasattr(piece, 'quality_score'):
                    features.append(piece.quality_score)
                elif isinstance(piece, dict) and 'quality_score' in piece:
                    features.append(piece['quality_score'])
                else:
                    features.append(0.5)
                
                piece_features.append(features)
            
            piece_features = np.array(piece_features)
            
            # ç°¡å˜ãªç‰¹å¾´é‡ã¿ä»˜ã‘
            feature_weights = np.array([0.3, 0.2])  # importance, quality_score
            piece_scores = np.dot(piece_features, feature_weights)
            
            # æ€§èƒ½ã‚¹ã‚³ã‚¢ã¨ç‰¹å¾´ã‚¹ã‚³ã‚¢ã®çµåˆ
            combined_scores = 0.7 * scores + 0.3 * piece_scores
        else:
            combined_scores = scores
        
        # Softmaxé‡ã¿è¨ˆç®—
        weights = self.compute_softmax_weights(combined_scores.tolist())
        
        logger.debug(f"Learned weights computed: {weights}")
        return weights
    
    def compute_weights(self, 
                       performance_scores: List[float],
                       pieces: List[Any] = None) -> np.ndarray:
        """
        è¨­å®šã«åŸºã¥ãé‡ã¿è¨ˆç®—
        
        Args:
            performance_scores: æ€§èƒ½ã‚¹ã‚³ã‚¢ãƒªã‚¹ãƒˆ
            pieces: DirectionPieceãƒªã‚¹ãƒˆ
            
        Returns:
            è¨ˆç®—ã•ã‚ŒãŸé‡ã¿é…åˆ—
        """
        if self.config.weight_method == "softmax":
            weights = self.compute_softmax_weights(performance_scores)
        elif self.config.weight_method == "linear":
            weights = self.compute_linear_weights(performance_scores)
        elif self.config.weight_method == "learned":
            weights = self.compute_learned_weights(performance_scores, pieces)
        else:
            logger.warning(f"Unknown weight method: {self.config.weight_method}, using softmax")
            weights = self.compute_softmax_weights(performance_scores)
        
        # æœ€å°é‡ã¿é–¾å€¤ã®é©ç”¨
        weights = np.maximum(weights, self.config.min_weight_threshold)
        weights = weights / np.sum(weights)  # å†æ­£è¦åŒ–
        
        return weights

def fuse_pieces_adaptive(
    pieces: List[Any],
    eval_dataset: List[Tuple[str, str]],
    generate_with_piece: Callable[[str, Any], str],
    config: AdaptiveFusionConfig = None,
    metrics: List[str] = None
) -> np.ndarray:
    """
    ã‚¿ã‚¹ã‚¯æŒ‡æ¨™ã«åŸºã¥ã„ã¦ãƒ”ãƒ¼ã‚¹çµ±åˆé‡ã¿ã‚’æ±ºå®šã—ã€æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ã‚’èåˆã™ã‚‹ã€‚

    Args:
        pieces: çµ±åˆå¯¾è±¡ã® DirectionPiece ãƒªã‚¹ãƒˆ
        eval_dataset: List of (input_text, reference_text) ãƒšã‚¢
        generate_with_piece: (prompt, piece) -> ç”Ÿæˆæ–‡ ã®é–¢æ•°
        config: é©å¿œåŒ–çµ±åˆè¨­å®š
        metrics: ä½¿ç”¨ã™ã‚‹æ€§èƒ½æŒ‡æ¨™ãƒªã‚¹ãƒˆï¼ˆå¾Œæ–¹äº’æ›ç”¨ï¼‰

    Returns:
        np.ndarray: èåˆå¾Œã®æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«
    """
    logger.info(f"ğŸ¦ Adaptive piece fusion started")
    logger.info(f"   Pieces: {len(pieces)}")
    logger.info(f"   Eval samples: {len(eval_dataset)}")
    
    # è¨­å®šã®åˆæœŸåŒ–
    if config is None:
        config = AdaptiveFusionConfig()
        if metrics is not None:
            config.metrics = metrics
    
    if not pieces:
        logger.warning("No pieces provided for fusion")
        return np.array([])
    
    if len(pieces) == 1:
        logger.info("Single piece provided, returning its vector")
        piece = pieces[0]
        if hasattr(piece, 'u_component'):
            return np.array(piece.u_component)
        elif isinstance(piece, dict) and 'u_component' in piece:
            return np.array(piece['u_component'])
        else:
            logger.error("Invalid piece format")
            return np.array([])
    
    try:
        # 1. å„ãƒ”ãƒ¼ã‚¹ã®æ€§èƒ½è¨ˆæ¸¬
        logger.info("ğŸ“Š Step 1: Evaluating piece performances")
        performance_evaluator = PiecePerformanceEvaluator(config)
        performance_scores = performance_evaluator.evaluate_pieces_batch(
            pieces, eval_dataset, generate_with_piece
        )
        
        # 2. é‡ã¿è¨ˆç®—
        logger.info("âš–ï¸ Step 2: Computing adaptive weights")
        weight_calculator = AdaptiveWeightCalculator(config)
        weights = weight_calculator.compute_weights(performance_scores, pieces)
        
        logger.info(f"   Computed weights: {[f'{w:.4f}' for w in weights]}")
        
        # 3. é‡ã¿ä»˜ãçµ±åˆ
        logger.info("ğŸ”— Step 3: Fusing vectors with adaptive weights")
        
        # ãƒ™ã‚¯ãƒˆãƒ«æŠ½å‡º
        vectors = []
        for piece in pieces:
            if hasattr(piece, 'u_component'):
                vector = np.array(piece.u_component)
            elif isinstance(piece, dict) and 'u_component' in piece:
                vector = np.array(piece['u_component'])
            else:
                logger.warning(f"Invalid piece format, using zero vector")
                vector = np.zeros(384)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¬¡å…ƒ
            vectors.append(vector)
        
        vectors = np.array(vectors)
        
        # æ¬¡å…ƒçµ±ä¸€
        if len(vectors) > 1:
            target_dim = max(len(v) for v in vectors)
            aligned_vectors = []
            for vector in vectors:
                if len(vector) < target_dim:
                    # ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                    padded = np.zeros(target_dim)
                    padded[:len(vector)] = vector
                    aligned_vectors.append(padded)
                elif len(vector) > target_dim:
                    # åˆ‡ã‚Šè©°ã‚
                    aligned_vectors.append(vector[:target_dim])
                else:
                    aligned_vectors.append(vector)
            vectors = np.array(aligned_vectors)
        
        # é‡ã¿ä»˜ãå’Œ
        fused_vector = np.zeros(vectors.shape[1])
        for weight, vector in zip(weights, vectors):
            fused_vector += weight * vector
        
        # 4. æ­£è¦åŒ–
        if config.normalize_vectors:
            norm = np.linalg.norm(fused_vector)
            if norm > 0:
                fused_vector = fused_vector / norm
            else:
                logger.warning("Zero norm fused vector, skipping normalization")
        
        logger.info(f"âœ… Adaptive fusion completed")
        logger.info(f"   Fused vector norm: {np.linalg.norm(fused_vector):.4f}")
        
        return fused_vector
        
    except Exception as e:
        logger.error(f"âŒ Adaptive fusion error: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å‡ç­‰é‡ã¿çµ±åˆ
        logger.info("Falling back to uniform weight fusion")
        
        vectors = []
        for piece in pieces:
            if hasattr(piece, 'u_component'):
                vectors.append(np.array(piece.u_component))
            elif isinstance(piece, dict) and 'u_component' in piece:
                vectors.append(np.array(piece['u_component']))
        
        if vectors:
            fused_vector = np.mean(vectors, axis=0)
            if config and config.normalize_vectors:
                norm = np.linalg.norm(fused_vector)
                if norm > 0:
                    fused_vector = fused_vector / norm
            return fused_vector
        else:
            return np.array([])

class AdaptiveFusionChameleonEditor:
    """é©å¿œçš„çµ±åˆå¯¾å¿œCFS-Chameleonã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼"""
    
    def __init__(self, 
                 base_editor: Any = None,
                 fusion_config: AdaptiveFusionConfig = None):
        """
        åˆæœŸåŒ–
        
        Args:
            base_editor: ãƒ™ãƒ¼ã‚¹CFS-Chameleonã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼
            fusion_config: é©å¿œåŒ–çµ±åˆè¨­å®š
        """
        self.base_editor = base_editor
        self.fusion_config = fusion_config or AdaptiveFusionConfig()
        
        logger.info("âœ… AdaptiveFusionChameleonEditor initialized")
    
    def generate_with_adaptive_fusion(self,
                                    prompt: str,
                                    pieces: List[Any],
                                    eval_dataset: List[Tuple[str, str]],
                                    max_length: int = 100) -> str:
        """
        é©å¿œçš„çµ±åˆã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆ
        
        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            pieces: çµ±åˆå¯¾è±¡ã®DirectionPieceãƒªã‚¹ãƒˆ
            eval_dataset: è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            max_length: æœ€å¤§ç”Ÿæˆé•·
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        logger.info(f"ğŸ¦ Generating with adaptive fusion")
        
        # ãƒ¢ãƒƒã‚¯ç”Ÿæˆé–¢æ•°
        def mock_generate_with_piece(input_text: str, piece: Any) -> str:
            piece_id = getattr(piece, 'piece_id', 'unknown')
            return f"Generated response for '{input_text[:30]}...' using piece {piece_id}"
        
        try:
            # é©å¿œçš„çµ±åˆå®Ÿè¡Œ
            fused_vector = fuse_pieces_adaptive(
                pieces, eval_dataset, mock_generate_with_piece, self.fusion_config
            )
            
            if self.base_editor and hasattr(self.base_editor, 'generate_with_chameleon'):
                # ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ã§ã®ç”Ÿæˆ
                result = self.base_editor.generate_with_chameleon(
                    prompt=prompt,
                    max_length=max_length
                )
                return result
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆ
                return f"Adaptive fusion response to: {prompt[:50]}... (fusion vector norm: {np.linalg.norm(fused_vector):.4f})"
                
        except Exception as e:
            logger.error(f"âŒ Adaptive fusion generation error: {e}")
            return f"Generation error: {prompt[:30]}..."

def demonstrate_adaptive_fusion():
    """é©å¿œçš„ãƒ”ãƒ¼ã‚¹çµ±åˆã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("ğŸ¦ ã‚¿ã‚¹ã‚¯é©å¿œåŒ–ãƒ”ãƒ¼ã‚¹çµ±åˆãƒ‡ãƒ¢")
    print("=" * 60)
    
    # è¨­å®š
    config = AdaptiveFusionConfig(
        weight_method="softmax",
        temperature=1.0,
        eval_sample_size=5,
        metrics=["rouge", "bertscore"],
        parallel_evaluation=False  # ãƒ‡ãƒ¢ã§ã¯é€æ¬¡å®Ÿè¡Œ
    )
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ”ãƒ¼ã‚¹ä½œæˆ
    sample_pieces = [
        {
            "piece_id": "movie_piece",
            "u_component": np.random.randn(384),
            "importance": 0.8,
            "quality_score": 0.7,
            "semantic_tags": ["movies", "entertainment"]
        },
        {
            "piece_id": "cooking_piece", 
            "u_component": np.random.randn(384),
            "importance": 0.6,
            "quality_score": 0.8,
            "semantic_tags": ["cooking", "recipes"]
        },
        {
            "piece_id": "tech_piece",
            "u_component": np.random.randn(384),
            "importance": 0.9,
            "quality_score": 0.6,
            "semantic_tags": ["technology", "programming"]
        }
    ]
    
    # è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    eval_dataset = [
        ("æ˜ ç”»ã®æ¨è–¦ã‚’ã—ã¦ãã ã•ã„", "é¢ç™½ã„æ˜ ç”»ã‚’ãŠå‹§ã‚ã—ã¾ã™"),
        ("æ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’æ•™ãˆã¦", "ç¾å‘³ã—ã„æ–™ç†ã®ä½œã‚Šæ–¹ã‚’ã”ç´¹ä»‹ã—ã¾ã™"),
        ("ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã«ã¤ã„ã¦æ•™ãˆã¦", "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®åŸºç¤ã‚’èª¬æ˜ã—ã¾ã™"),
        ("ä»Šæ—¥ã®å¤©æ°—ã¯ã©ã†ã§ã™ã‹ï¼Ÿ", "ä»Šæ—¥ã¯æ™´ã‚Œã§ã™"),
        ("å¥½ããªæœ¬ã¯ä½•ã§ã™ã‹ï¼Ÿ", "æ§˜ã€…ãªæœ¬ã‚’ãŠå‹§ã‚ã§ãã¾ã™")
    ]
    
    # ãƒ¢ãƒƒã‚¯ç”Ÿæˆé–¢æ•°
    def mock_generate_with_piece(input_text: str, piece: Any) -> str:
        piece_id = piece.get('piece_id', 'unknown')
        tags = ', '.join(piece.get('semantic_tags', []))
        
        # ãƒ”ãƒ¼ã‚¹ã®æ„å‘³ã‚¿ã‚°ã«åŸºã¥ãç°¡å˜ãªç”Ÿæˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        if 'movies' in tags and 'æ˜ ç”»' in input_text:
            return "ãŠã™ã™ã‚ã®æ˜ ç”»ã‚’è©³ã—ãç´¹ä»‹ã„ãŸã—ã¾ã™ã€‚æœ€æ–°ä½œã‹ã‚‰åä½œã¾ã§å¹…åºƒãã”æ¡ˆå†…ã—ã¾ã™ã€‚"
        elif 'cooking' in tags and 'æ–™ç†' in input_text:
            return "ç¾å‘³ã—ã„æ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã§èª¬æ˜ã„ãŸã—ã¾ã™ã€‚"
        elif 'technology' in tags and 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°' in input_text:
            return "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®åŸºç¤ã‹ã‚‰å¿œç”¨ã¾ã§ä¸å¯§ã«è§£èª¬ã„ãŸã—ã¾ã™ã€‚"
        else:
            return f"å…¥åŠ›ã€Œ{input_text[:20]}...ã€ã«å¯¾ã™ã‚‹{piece_id}ã‹ã‚‰ã®ä¸€èˆ¬çš„ãªå›ç­”ã§ã™ã€‚"
    
    print(f"\nğŸ“Š Sample pieces:")
    for i, piece in enumerate(sample_pieces):
        print(f"   {i+1}. {piece['piece_id']} - importance: {piece['importance']}, quality: {piece['quality_score']}")
    
    print(f"\nğŸ“‹ Evaluation dataset: {len(eval_dataset)} samples")
    for i, (inp, ref) in enumerate(eval_dataset):
        print(f"   {i+1}. '{inp}' -> '{ref}'")
    
    # é©å¿œçš„çµ±åˆå®Ÿè¡Œ
    print(f"\nğŸš€ Running adaptive piece fusion...")
    start_time = time.time()
    
    fused_vector = fuse_pieces_adaptive(
        pieces=sample_pieces,
        eval_dataset=eval_dataset,
        generate_with_piece=mock_generate_with_piece,
        config=config
    )
    
    execution_time = time.time() - start_time
    
    print(f"\nâœ… Adaptive fusion completed!")
    print(f"   Execution time: {execution_time:.2f}s")
    print(f"   Fused vector shape: {fused_vector.shape}")
    print(f"   Fused vector norm: {np.linalg.norm(fused_vector):.4f}")
    
    # å¾“æ¥æ‰‹æ³•ã¨ã®æ¯”è¼ƒ
    print(f"\nğŸ”„ Comparison with traditional fusion:")
    print("-" * 40)
    
    # å‡ç­‰é‡ã¿çµ±åˆ
    uniform_vectors = [np.array(piece['u_component']) for piece in sample_pieces]
    uniform_fused = np.mean(uniform_vectors, axis=0)
    uniform_norm = np.linalg.norm(uniform_fused)
    if uniform_norm > 0:
        uniform_fused = uniform_fused / uniform_norm
    
    # é‡è¦åº¦ãƒ™ãƒ¼ã‚¹çµ±åˆ
    importance_weights = [piece['importance'] for piece in sample_pieces]
    importance_weights = np.array(importance_weights) / np.sum(importance_weights)
    importance_fused = np.zeros_like(uniform_vectors[0])
    for w, vec in zip(importance_weights, uniform_vectors):
        importance_fused += w * vec
    importance_norm = np.linalg.norm(importance_fused)
    if importance_norm > 0:
        importance_fused = importance_fused / importance_norm
    
    print(f"Uniform weights fusion:     norm = {np.linalg.norm(uniform_fused):.4f}")
    print(f"Importance-based fusion:    norm = {np.linalg.norm(importance_fused):.4f}")
    print(f"Adaptive task-based fusion: norm = {np.linalg.norm(fused_vector):.4f}")
    
    # ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦æ¯”è¼ƒ
    if fused_vector.shape == uniform_fused.shape:
        uniform_sim = np.dot(fused_vector, uniform_fused)
        importance_sim = np.dot(fused_vector, importance_fused)
        
        print(f"\nVector similarities to adaptive fusion:")
        print(f"   vs Uniform fusion:     {uniform_sim:.4f}")
        print(f"   vs Importance fusion:  {importance_sim:.4f}")
    
    # ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ã®ãƒ†ã‚¹ãƒˆ
    print(f"\nğŸ¯ Testing adaptive fusion editor:")
    print("-" * 40)
    
    editor = AdaptiveFusionChameleonEditor(fusion_config=config)
    
    test_prompts = [
        "æ˜ ç”»ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
        "æ–™ç†ã®ã‚³ãƒ„ã‚’æ•™ãˆã¦", 
        "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°å­¦ç¿’æ–¹æ³•"
    ]
    
    for prompt in test_prompts:
        result = editor.generate_with_adaptive_fusion(
            prompt=prompt,
            pieces=sample_pieces,
            eval_dataset=eval_dataset[:3],  # å°ã•ãªã‚µãƒ–ã‚»ãƒƒãƒˆ
            max_length=80
        )
        print(f"\nPrompt: '{prompt}'")
        print(f"Result: {result}")
    
    print("\nğŸ‰ ã‚¿ã‚¹ã‚¯é©å¿œåŒ–ãƒ”ãƒ¼ã‚¹çµ±åˆãƒ‡ãƒ¢å®Œäº†!")

if __name__ == "__main__":
    demonstrate_adaptive_fusion()