model:
  name: "meta-llama/Llama-3.2-3B-Instruct"
  device: "cuda"
  max_length: 512
  batch_size: 4
  torch_dtype: "float32"

chameleon:
  num_self_generated: 10
  target_layers: ["model.layers.16", "model.layers.20"]
  alpha_personal: 0.1
  alpha_general: -0.05

evaluation:
  max_users: 20
  metrics: ["exact_match", "bleu_score"]
  save_predictions: true
  
# LaMP_all データソース設定
data_sources:
  primary: "chameleon_prime_personalization/data/raw/LaMP-2/merged.json"
  backup: "data/raw/LaMP_all/LaMP_2/user-based/dev/dev_questions.json"
  answers_primary: "chameleon_prime_personalization/data/raw/LaMP-2/answers.json"
  answers_backup: "data/raw/LaMP_all/LaMP_2/user-based/dev/dev_outputs.json"
