chameleon:
  # alpha_personal: 0.3
  # alpha_general: -0.05
  alpha_personal: 9
  alpha_general: -5
  num_self_generated: 10
  # target_layers:
  # - model.layers.14.mlp
  target_layers: ["model.layers.20","model.layers.28","model.layers.30"]
  theta_p_path: chameleon_prime_personalization/processed/LaMP-2/theta_p.npy
  theta_n_path: chameleon_prime_personalization/processed/LaMP-2/theta_n.npy
  direction_p_path: chameleon_prime_personalization/processed/LaMP-2/theta_p.npy
  direction_n_path: chameleon_prime_personalization/processed/LaMP-2/theta_n.npy
data_sources:
  answers_backup: data/raw/LaMP_all/LaMP_2/user-based/dev/dev_outputs.json
  answers_primary: chameleon_prime_personalization/data/raw/LaMP-2/answers.json
  backup: data/raw/LaMP_all/LaMP_2/user-based/dev/dev_questions.json
  primary: chameleon_prime_personalization/data/raw/LaMP-2/merged.json
evaluation:
  max_users
: 20
  metrics:
  - exact_match
  - bleu_score
  save_predictions: true
model:
  batch_size: 4
  device: cuda
  max_length: 512
  name: meta-llama/Llama-3.2-3B-Instruct
  torch_dtype: float32
