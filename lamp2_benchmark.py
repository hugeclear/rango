#!/usr/bin/env python3
"""
LaMP-2 æ˜ ç”»ã‚¿ã‚°ä»˜ã‘ã‚¿ã‚¹ã‚¯ç”¨ã®è‡ªå‹•ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
Chameleonæ‰‹æ³•ã®æ€§èƒ½ã‚’è‡ªå‹•è©•ä¾¡ãƒ»æ¯”è¼ƒã™ã‚‹
"""

import json
import os
import time
from pathlib import Path
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support
import matplotlib.pyplot as plt
import seaborn as sns

# OpenAI APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆChameleonå®Ÿè£…æ™‚ã«ä½¿ç”¨ï¼‰
from openai import OpenAI

@dataclass
class EvaluationResult:
    """è©•ä¾¡çµæœã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    method_name: str
    accuracy: float
    f1_macro: float
    f1_micro: float
    precision: float
    recall: float
    inference_time: float
    total_samples: int
    correct_predictions: int

class LaMP2Evaluator:
    """LaMP-2æ˜ ç”»ã‚¿ã‚°ä»˜ã‘ã‚¿ã‚¹ã‚¯ã®è‡ªå‹•è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, data_path: str, output_dir: str = "./evaluation_results"):
        self.data_path = Path(data_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        self.test_data = self._load_test_data()
        self.ground_truth = self._load_ground_truth()
        
        # Chameleonã®thetaæ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ã‚’èª­ã¿è¾¼ã¿
        self.theta_p = self._load_theta_vectors("theta_p.json")
        self.theta_n = self._load_theta_vectors("theta_n.json")
        
        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        self.client = OpenAI()
        
        print(f"âœ… LaMP-2è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        print(f"   ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«æ•°: {len(self.test_data)}")
        print(f"   å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
    
    def _load_test_data(self) -> List[Dict]:
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        # å®Ÿéš›ã«è¦‹ã¤ã‹ã£ãŸãƒ‘ã‚¹ã‚’è¿½åŠ 
        possible_paths = [
            self.data_path / "processed" / "LaMP-2" / "merged.json",  # æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹
            Path("processed/LaMP-2/merged.json"),  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰
            Path("chameleon_prime_personalization/data/processed/LaMP-2/merged.json"),  # ãƒ•ãƒ«ãƒ‘ã‚¹
            Path("chameleon_prime_personalization/data/raw/LaMP-2/merged.json"),  # å®Ÿéš›ã®å ´æ‰€
            self.data_path / "raw" / "LaMP-2" / "merged.json",  # rawä»¥ä¸‹
            self.data_path / "merged.json"  # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ç›´ä¸‹
        ]
        
        for merged_path in possible_paths:
            if merged_path.exists():
                print(f"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {merged_path}")
                with open(merged_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
        
        # ã™ã¹ã¦ã®ãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
        print("âŒ merged.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä»¥ä¸‹ã®ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¾ã—ãŸ:")
        for path in possible_paths:
            print(f"   {path}")
        raise FileNotFoundError("merged.json not found in any expected location")
    
    def _load_ground_truth(self) -> Dict[str, str]:
        """æ­£è§£ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆLaMP-2ã®dev_outputs.jsonã‹ã‚‰ï¼‰"""
        # å®Ÿéš›ã«è¦‹ã¤ã‹ã£ãŸãƒ‘ã‚¹ã‚’è¿½åŠ 
        possible_paths = [
            self.data_path / "raw" / "LaMP-2" / "answers.json",
            Path("chameleon_prime_personalization/data/raw/LaMP-2/answers.json"),  # å®Ÿéš›ã®å ´æ‰€
            Path("processed/LaMP-2/answers.json"),
            self.data_path / "answers.json"
        ]
        
        for answers_path in possible_paths:
            if answers_path.exists():
                print(f"âœ… æ­£è§£ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {answers_path}")
                try:
                    with open(answers_path, 'r', encoding='utf-8') as f:
                        answers = json.load(f)
                    
                    print(f"   ğŸ“Š ç­”ãƒ‡ãƒ¼ã‚¿æ§‹é€ ç¢ºèª:")
                    print(f"   ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(answers)}")
                    if answers:
                        sample = answers[0]
                        print(f"   ã‚µãƒ³ãƒ—ãƒ«æ§‹é€ : {type(sample)}")
                        if isinstance(sample, dict):
                            print(f"   ã‚µãƒ³ãƒ—ãƒ«ã‚­ãƒ¼: {list(sample.keys())}")
                            print(f"   ã‚µãƒ³ãƒ—ãƒ«å†…å®¹: {sample}")
                        else:
                            print(f"   ã‚µãƒ³ãƒ—ãƒ«å†…å®¹: {sample}")
                    
                    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«åŸºã¥ã„ã¦å‡¦ç†ã‚’åˆ†å²
                    ground_truth = {}
                    
                    if not answers:
                        print("   âš ï¸  ç­”ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
                        return {}
                    
                    # æ§‹é€ ã‚’ç¢ºèªã—ã¦é©åˆ‡ã«å‡¦ç†
                    sample = answers[0]
                    
                    if isinstance(sample, dict):
                        # è¾æ›¸å½¢å¼ã®å ´åˆ
                        if "id" in sample and "output" in sample:
                            # æ¨™æº–å½¢å¼
                            ground_truth = {str(ans["id"]): str(ans["output"]).lower().strip() 
                                          for ans in answers if isinstance(ans, dict) and "id" in ans and "output" in ans}
                        elif "id" in sample and "answer" in sample:
                            # ä»£æ›¿å½¢å¼
                            ground_truth = {str(ans["id"]): str(ans["answer"]).lower().strip() 
                                          for ans in answers if isinstance(ans, dict) and "id" in ans and "answer" in ans}
                        else:
                            print(f"   âŒ äºˆæœŸã—ãªã„è¾æ›¸æ§‹é€ : {list(sample.keys())}")
                            continue
                    elif isinstance(sample, str):
                        # æ–‡å­—åˆ—å½¢å¼ã®å ´åˆï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’IDã¨ã—ã¦ä½¿ç”¨ï¼‰
                        ground_truth = {str(i): ans.lower().strip() for i, ans in enumerate(answers)}
                        print(f"   ğŸ“ æ–‡å­—åˆ—å½¢å¼ã¨ã—ã¦å‡¦ç†ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’IDã¨ã—ã¦ä½¿ç”¨ï¼‰")
                    else:
                        print(f"   âŒ äºˆæœŸã—ãªã„ãƒ‡ãƒ¼ã‚¿å½¢å¼: {type(sample)}")
                        continue
                    
                    print(f"   âœ… æ­£è§£ãƒ‡ãƒ¼ã‚¿å¤‰æ›å®Œäº†: {len(ground_truth)} ã‚µãƒ³ãƒ—ãƒ«")
                    if ground_truth:
                        sample_items = list(ground_truth.items())[:3]
                        print(f"   ğŸ“‹ å¤‰æ›ä¾‹: {sample_items}")
                    
                    return ground_truth
                    
                except Exception as e:
                    print(f"   âŒ æ­£è§£ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        print("âŒ answers.json ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹èª­ã¿è¾¼ã‚ã¾ã›ã‚“:")
        for path in possible_paths:
            print(f"   {path}")
        
        # è©•ä¾¡ã‚’ç¶šè¡Œã™ã‚‹ãŸã‚ã€ç©ºã®è¾æ›¸ã‚’è¿”ã™
        print("âš ï¸  æ­£è§£ãƒ‡ãƒ¼ã‚¿ãªã—ã§è©•ä¾¡ã‚’ç¶šè¡Œã—ã¾ã™ï¼ˆæ¯”è¼ƒè©•ä¾¡ã®ã¿ï¼‰")
        return {}
    
    def _load_theta_vectors(self, filename: str) -> np.ndarray:
        """Chameleonã®thetaæ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        # å®Ÿéš›ã«è¦‹ã¤ã‹ã£ãŸãƒ‘ã‚¹ã‚’è¿½åŠ 
        possible_paths = [
            self.data_path / "processed" / "LaMP-2" / filename,
            Path(f"processed/LaMP-2/{filename}"),  # å®Ÿéš›ã®å ´æ‰€
            Path(f"chameleon_prime_personalization/data/processed/LaMP-2/{filename}"),
            self.data_path / filename
        ]
        
        for theta_path in possible_paths:
            if theta_path.exists():
                print(f"âœ… Theta vectorèª­ã¿è¾¼ã¿: {theta_path}")
                with open(theta_path, 'r', encoding='utf-8') as f:
                    return np.array(json.load(f))
        
        print(f"âš ï¸  {filename} not found, Chameleonè©•ä¾¡ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™")
        print("ç¢ºèªã—ãŸãƒ‘ã‚¹:")
        for path in possible_paths:
            print(f"   {path}")
        return None
    
    def evaluate_baseline_llm(self, model: str = "gpt-3.5-turbo") -> EvaluationResult:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³LLMï¼ˆç·¨é›†ãªã—ï¼‰ã®è©•ä¾¡"""
        print(f"ğŸ”„ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³LLMè©•ä¾¡é–‹å§‹ (model: {model})")
        
        predictions = []
        start_time = time.time()
        
        for i, sample in enumerate(self.test_data):
            if i % 10 == 0:
                print(f"   é€²æ—: {i}/{len(self.test_data)}")
            
            prompt = f"""Given the following movie description, provide a single word tag that best describes the movie:

Movie: {sample['input']}

Tag:"""
            
            try:
                response = self.client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=10,
                    temperature=0
                )
                prediction = response.choices[0].message.content.strip().lower()
                predictions.append(prediction)
            except Exception as e:
                print(f"API Error for sample {sample['id']}: {e}")
                predictions.append("unknown")
        
        inference_time = time.time() - start_time
        return self._calculate_metrics("Baseline_LLM", predictions, inference_time)
    
    def evaluate_chameleon(self, alpha: float = 1.0, beta: float = 1.0) -> EvaluationResult:
        """Chameleonæ‰‹æ³•ã®è©•ä¾¡ï¼ˆåŸ‹ã‚è¾¼ã¿ç·¨é›†ç‰ˆï¼‰"""
        if self.theta_p is None or self.theta_n is None:
            print("âŒ Chameleonè©•ä¾¡: theta vectors not loaded")
            return None
            
        print(f"ğŸ”„ Chameleonè©•ä¾¡é–‹å§‹ (Î±={alpha}, Î²={beta})")
        
        predictions = []
        start_time = time.time()
        
        for i, sample in enumerate(self.test_data):
            if i % 10 == 0:
                print(f"   é€²æ—: {i}/{len(self.test_data)}")
            
            # Step 1: å…ƒã®æ˜ ç”»èª¬æ˜ã‚’åŸ‹ã‚è¾¼ã¿
            original_embedding = self._get_embedding(sample['input'])
            
            # Step 2: Chameleonç·¨é›†é©ç”¨
            # edited_emb = original + Î± * theta_p - Î² * theta_n
            
            # Ensure dimensions match before arithmetic
            from dimension_debug_helper import fit_to_hidden
            if self.theta_p is not None and self.theta_n is not None:
                # Fit theta vectors to embedding dimension
                target_dim = len(original_embedding) if hasattr(original_embedding, '__len__') else original_embedding.shape[-1]
                import torch
                device = torch.device('cpu')
                dtype = torch.float32
                
                fitted_theta_p = fit_to_hidden(self.theta_p, target_dim, device, dtype).cpu().numpy()
                fitted_theta_n = fit_to_hidden(self.theta_n, target_dim, device, dtype).cpu().numpy()
                
                
                edited_embedding = (
                    original_embedding + 
                    alpha * fitted_theta_p - 
                    beta * fitted_theta_n
                )
            else:
                edited_embedding = original_embedding
            
            # Step 3: ç·¨é›†ã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿ã§LLMæ¨è«–ï¼ˆç°¡ç•¥ç‰ˆï¼‰
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ç·¨é›†ã—ãŸåŸ‹ã‚è¾¼ã¿ã‚’LLMã®å†…éƒ¨è¡¨ç¾ã«æ³¨å…¥
            # ã“ã“ã§ã¯ç·¨é›†åŠ¹æœã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã—ãŸæ¨è«–ã‚’è¡Œã†
            prediction = self._chameleon_predict(sample, edited_embedding)
            predictions.append(prediction)
        
        inference_time = time.time() - start_time
        return self._calculate_metrics("Chameleon", predictions, inference_time)
    
    def _get_embedding(self, text: str) -> np.ndarray:
        """ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—"""
        try:
            response = self.client.embeddings.create(
                model="text-embedding-ada-002",
                input=text
            )
            return np.array(response.data[0].embedding)
        except Exception as e:
            print(f"Embedding error: {e}")
            return np.zeros(1536)  # ada-002ã®æ¬¡å…ƒæ•°
    
    def _chameleon_predict(self, sample: Dict, edited_embedding: np.ndarray) -> str:
        """Chameleonç·¨é›†å¾Œã®äºˆæ¸¬ï¼ˆç°¡ç•¥å®Ÿè£…ï¼‰"""
        # å®Ÿéš›ã«ã¯ã“ã“ã§ç·¨é›†ã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã—ãŸæ¨è«–ã‚’å®Ÿè¡Œ
        # ç¾åœ¨ã¯ç°¡æ˜“çš„ã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼å±¥æ­´ã¨edited embeddingã®é¡ä¼¼åº¦ã§äºˆæ¸¬
        
        user_profile = sample.get('profile', [])
        if not user_profile:
            return "drama"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å±¥æ­´ã‹ã‚‰å¤šæ•°æ±ºã§äºˆæ¸¬ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        profile_tags = []
        for item in user_profile[:3]:  # ä¸Šä½3ä»¶
            desc = item.get('description', '')
            if 'action' in desc.lower():
                profile_tags.append('action')
            elif 'comedy' in desc.lower():
                profile_tags.append('comedy')
            elif 'drama' in desc.lower():
                profile_tags.append('drama')
            elif 'horror' in desc.lower():
                profile_tags.append('horror')
            elif 'romance' in desc.lower():
                profile_tags.append('romance')
            else:
                profile_tags.append('drama')
        
        # å¤šæ•°æ±º
        if profile_tags:
            return max(set(profile_tags), key=profile_tags.count)
        return "drama"
    
    def _calculate_metrics(self, method_name: str, predictions: List[str], inference_time: float) -> EvaluationResult:
        """è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—"""
        # æ­£è§£ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
        true_labels = []
        pred_labels = []
        
        for i, sample in enumerate(self.test_data):
            sample_id = sample['id']
            if sample_id in self.ground_truth:
                true_labels.append(self.ground_truth[sample_id].lower().strip())
                pred_labels.append(predictions[i])
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        accuracy = accuracy_score(true_labels, pred_labels)
        f1_macro = f1_score(true_labels, pred_labels, average='macro', zero_division=0)
        f1_micro = f1_score(true_labels, pred_labels, average='micro', zero_division=0)
        precision, recall, _, _ = precision_recall_fscore_support(
            true_labels, pred_labels, average='macro', zero_division=0
        )
        
        correct_predictions = sum(1 for t, p in zip(true_labels, pred_labels) if t == p)
        
        return EvaluationResult(
            method_name=method_name,
            accuracy=accuracy,
            f1_macro=f1_macro,
            f1_micro=f1_micro,
            precision=precision,
            recall=recall,
            inference_time=inference_time,
            total_samples=len(true_labels),
            correct_predictions=correct_predictions
        )
    
    def run_full_evaluation(self) -> Dict[str, EvaluationResult]:
        """å…¨æ‰‹æ³•ã®è‡ªå‹•è©•ä¾¡ã‚’å®Ÿè¡Œ"""
        print("ğŸš€ LaMP-2 è‡ªå‹•ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡é–‹å§‹")
        print("=" * 50)
        
        results = {}
        
        # 1. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡
        try:
            results['baseline'] = self.evaluate_baseline_llm()
        except Exception as e:
            print(f"âŒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
        
        # 2. Chameleonè©•ä¾¡
        try:
            results['chameleon'] = self.evaluate_chameleon(alpha=1.0, beta=1.0)
        except Exception as e:
            print(f"âŒ Chameleonè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
        
        # 3. çµæœä¿å­˜ãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self._save_results(results)
        self._generate_report(results)
        
        return results
    
    def _save_results(self, results: Dict[str, EvaluationResult]):
        """è©•ä¾¡çµæœã‚’JSONã§ä¿å­˜"""
        results_dict = {}
        for method, result in results.items():
            if result:
                results_dict[method] = {
                    'accuracy': result.accuracy,
                    'f1_macro': result.f1_macro,
                    'f1_micro': result.f1_micro,
                    'precision': result.precision,
                    'recall': result.recall,
                    'inference_time': result.inference_time,
                    'total_samples': result.total_samples,
                    'correct_predictions': result.correct_predictions
                }
        
        with open(self.output_dir / "evaluation_results.json", 'w', encoding='utf-8') as f:
            json.dump(results_dict, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ è©•ä¾¡çµæœä¿å­˜: {self.output_dir / 'evaluation_results.json'}")
    
    def _generate_report(self, results: Dict[str, EvaluationResult]):
        """è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print("\n" + "=" * 50)
        print("ğŸ“Š LaMP-2 ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡çµæœ")
        print("=" * 50)
        
        for method, result in results.items():
            if result:
                print(f"\nğŸ”¹ {result.method_name}")
                print(f"   Accuracy:    {result.accuracy:.4f}")
                print(f"   F1 (macro):  {result.f1_macro:.4f}")
                print(f"   F1 (micro):  {result.f1_micro:.4f}")
                print(f"   Precision:   {result.precision:.4f}")
                print(f"   Recall:      {result.recall:.4f}")
                print(f"   æ¨è«–æ™‚é–“:     {result.inference_time:.2f}ç§’")
                print(f"   æ­£è§£æ•°:      {result.correct_predictions}/{result.total_samples}")
        
        # æ€§èƒ½æ¯”è¼ƒè¡¨ã‚’ç”Ÿæˆ
        self._plot_comparison(results)
    
    def _plot_comparison(self, results: Dict[str, EvaluationResult]):
        """æ€§èƒ½æ¯”è¼ƒã®ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ"""
        if len([r for r in results.values() if r]) < 2:
            return
        
        plt.figure(figsize=(12, 6))
        
        methods = []
        accuracies = []
        f1_scores = []
        
        for method, result in results.items():
            if result:
                methods.append(result.method_name)
                accuracies.append(result.accuracy)
                f1_scores.append(result.f1_macro)
        
        x = np.arange(len(methods))
        width = 0.35
        
        plt.subplot(1, 2, 1)
        plt.bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.8)
        plt.bar(x + width/2, f1_scores, width, label='F1 (macro)', alpha=0.8)
        plt.xlabel('Method')
        plt.ylabel('Score')
        plt.title('LaMP-2 Performance Comparison')
        plt.xticks(x, methods)
        plt.legend()
        plt.ylim(0, 1)
        
        # æ¨è«–æ™‚é–“ã®æ¯”è¼ƒ
        plt.subplot(1, 2, 2)
        inference_times = [results[m].inference_time for m in results if results[m]]
        plt.bar(methods, inference_times, alpha=0.8, color='orange')
        plt.xlabel('Method')
        plt.ylabel('Inference Time (seconds)')
        plt.title('Inference Time Comparison')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / "performance_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ æ€§èƒ½æ¯”è¼ƒã‚°ãƒ©ãƒ•ä¿å­˜: {self.output_dir / 'performance_comparison.png'}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="LaMP-2 è‡ªå‹•ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡")
    parser.add_argument("--data_path", default="./chameleon_prime_personalization/data", 
                       help="ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹")
    parser.add_argument("--output_dir", default="./evaluation_results", 
                       help="çµæœå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    args = parser.parse_args()
    
    # è©•ä¾¡å®Ÿè¡Œ
    evaluator = LaMP2Evaluator(args.data_path, args.output_dir)
    results = evaluator.run_full_evaluation()
    
    print("\nâœ… è‡ªå‹•ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡å®Œäº†!")
    print(f"ğŸ“ çµæœã¯ {args.output_dir} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

if __name__ == "__main__":
    main()