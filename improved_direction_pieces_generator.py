#!/usr/bin/env python3
"""
æ”¹å–„ç‰ˆæ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ãƒ”ãƒ¼ã‚¹ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
å±¥æ­´ãƒ™ãƒ¼ã‚¹ã®æ„å‘³çš„æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆã«ã‚ˆã‚ŠCFSç²¾åº¦ã‚’å‘ä¸Š

ğŸ¯ æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ:
- å¤–ç©ã«ã‚ˆã‚‹æ–¹å‘æƒ…å ±æ¶ˆå¤±ã‚’å›é¿
- å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¤šæ§˜ãªæ„å‘³çš„æ–¹å‘ã‚’æŠ½å‡º
- SVDåˆ†è§£ã§æ„å‘³çš„ã«è±Šã‹ãªãƒ”ãƒ¼ã‚¹ã‚’ç”Ÿæˆ
"""

import numpy as np
import torch
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import logging
from pathlib import Path
import json
import time

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from sentence_transformers import SentenceTransformer
    from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    print("âš ï¸ Transformers/SentenceTransformers not available. Using mock implementations.")
    TRANSFORMERS_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DirectionPiece:
    """æ”¹å–„ç‰ˆæ–¹å‘ãƒ”ãƒ¼ã‚¹æ§‹é€ """
    u_component: np.ndarray      # å·¦ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«æˆåˆ†
    singular_value: float        # ç‰¹ç•°å€¤
    v_component: np.ndarray      # å³ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«æˆåˆ†
    importance: float            # é‡è¦åº¦ï¼ˆç‰¹ç•°å€¤ã®å¯„ä¸ç‡ï¼‰
    quality_score: float         # å“è³ªã‚¹ã‚³ã‚¢
    semantic_context: str        # æ„å‘³çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
    source_history_indices: List[int]  # å…ƒå±¥æ­´ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    creation_timestamp: float    # ä½œæˆæ™‚åˆ»

class ImprovedDirectionPiecesGenerator:
    """æ”¹å–„ç‰ˆæ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ãƒ”ãƒ¼ã‚¹ç”Ÿæˆå™¨"""
    
    def __init__(self, 
                 llm_model_name: str = "meta-llama/Llama-3.2-3B-Instruct",
                 embedding_model_name: str = "all-MiniLM-L6-v2",
                 device: str = "cuda"):
        """
        åˆæœŸåŒ–
        
        Args:
            llm_model_name: è¨€ã„æ›ãˆç”Ÿæˆç”¨LLMãƒ¢ãƒ‡ãƒ«å
            embedding_model_name: åŸ‹ã‚è¾¼ã¿ç”¨ãƒ¢ãƒ‡ãƒ«å
            device: è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹
        """
        self.device = device
        self.llm_model_name = llm_model_name
        self.embedding_model_name = embedding_model_name
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        self._initialize_models()
        
        logger.info(f"âœ… ImprovedDirectionPiecesGenerator initialized")
        logger.info(f"   LLM: {llm_model_name}")
        logger.info(f"   Embedding: {embedding_model_name}")
        logger.info(f"   Device: {device}")
    
    def _initialize_models(self):
        """ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–"""
        if TRANSFORMERS_AVAILABLE:
            try:
                # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
                self.embedding_model = SentenceTransformer(self.embedding_model_name)
                logger.info(f"âœ… Embedding model loaded: {self.embedding_model_name}")
                
                # LLMãƒ¢ãƒ‡ãƒ«ï¼ˆè¨€ã„æ›ãˆç”¨ï¼‰
                self.tokenizer = AutoTokenizer.from_pretrained(self.llm_model_name)
                self.llm_model = AutoModelForCausalLM.from_pretrained(
                    self.llm_model_name,
                    torch_dtype=torch.float16,
                    device_map="auto" if self.device == "cuda" else "cpu"
                )
                
                # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                logger.info(f"âœ… LLM model loaded: {self.llm_model_name}")
                
            except Exception as e:
                logger.error(f"âŒ Model initialization error: {e}")
                self.embedding_model = None
                self.llm_model = None
                self.tokenizer = None
        else:
            # ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«
            self.embedding_model = None
            self.llm_model = None
            self.tokenizer = None
            logger.warning("âš ï¸ Using mock models for demonstration")
    
    def generate_paraphrases(self, 
                           original_text: str, 
                           neutral_reference: str,
                           num_variants: int = 3) -> Tuple[List[str], List[str]]:
        """
        LLMã‚’ä½¿ç”¨ã—ã¦ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ãƒ»ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ã®è¨€ã„æ›ãˆã‚’ç”Ÿæˆ
        
        Args:
            original_text: å…ƒãƒ†ã‚­ã‚¹ãƒˆ
            neutral_reference: ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ
            num_variants: ç”Ÿæˆã™ã‚‹è¨€ã„æ›ãˆæ•°
            
        Returns:
            (personal_paraphrases, neutral_paraphrases)
        """
        if self.llm_model is None or self.tokenizer is None:
            # ãƒ¢ãƒƒã‚¯å®Ÿè£…
            return self._mock_paraphrase_generation(original_text, neutral_reference, num_variants)
        
        try:
            personal_paraphrases = []
            neutral_paraphrases = []
            
            # ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«æ–¹å‘ã®è¨€ã„æ›ãˆç”Ÿæˆ
            personal_prompt = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã€ã‚ˆã‚Šå€‹äººçš„ã§æ„Ÿæƒ…çš„ãªè¡¨ç¾ã«è¨€ã„æ›ãˆã¦ãã ã•ã„ã€‚{num_variants}ã¤ã®ç•°ãªã‚‹è¨€ã„æ›ãˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

å…ƒãƒ†ã‚­ã‚¹ãƒˆ: {original_text}

è¨€ã„æ›ãˆ:"""
            
            personal_variants = self._generate_text_variants(personal_prompt, num_variants)
            personal_paraphrases.extend(personal_variants)
            
            # ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ã®è¨€ã„æ›ãˆç”Ÿæˆ
            neutral_prompt = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã€ã‚ˆã‚Šä¸­ç«‹çš„ã§å®¢è¦³çš„ãªè¡¨ç¾ã«è¨€ã„æ›ãˆã¦ãã ã•ã„ã€‚å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆã®ã‚ˆã†ãªä¸­ç«‹çš„ãªãƒˆãƒ¼ãƒ³ã«åˆã‚ã›ã¦ãã ã•ã„ã€‚{num_variants}ã¤ã®ç•°ãªã‚‹è¨€ã„æ›ãˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

å…ƒãƒ†ã‚­ã‚¹ãƒˆ: {original_text}
å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ: {neutral_reference}

è¨€ã„æ›ãˆ:"""
            
            neutral_variants = self._generate_text_variants(neutral_prompt, num_variants)
            neutral_paraphrases.extend(neutral_variants)
            
            logger.debug(f"Generated {len(personal_paraphrases)} personal and {len(neutral_paraphrases)} neutral paraphrases")
            
            return personal_paraphrases, neutral_paraphrases
            
        except Exception as e:
            logger.error(f"âŒ Paraphrase generation error: {e}")
            return self._mock_paraphrase_generation(original_text, neutral_reference, num_variants)
    
    def _generate_text_variants(self, prompt: str, num_variants: int) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆå¤‰ç¨®ç”Ÿæˆ"""
        variants = []
        
        for i in range(num_variants):
            try:
                inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
                inputs = {k: v.to(self.llm_model.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = self.llm_model.generate(
                        **inputs,
                        max_length=inputs['input_ids'].shape[1] + 50,
                        do_sample=True,
                        temperature=0.8,
                        top_p=0.9,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                
                generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                variant = generated[len(prompt):].strip()
                
                if variant and len(variant) > 10:  # æœ‰åŠ¹ãªç”Ÿæˆçµæœã®ã¿è¿½åŠ 
                    variants.append(variant)
                    
            except Exception as e:
                logger.warning(f"Generation attempt {i+1} failed: {e}")
                continue
        
        return variants
    
    def _mock_paraphrase_generation(self, original_text: str, neutral_reference: str, num_variants: int) -> Tuple[List[str], List[str]]:
        """ãƒ¢ãƒƒã‚¯è¨€ã„æ›ãˆç”Ÿæˆï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰"""
        personal_variants = [
            f"ç§ã¯æœ¬å½“ã«{original_text}ã¨æ„Ÿã˜ã¦ã„ã¾ã™",
            f"{original_text}ã¨ã„ã†ã“ã¨ã«æ·±ãå…±æ„Ÿã—ã¾ã™",
            f"å€‹äººçš„ã«ã¯{original_text}ã¨ã„ã†çµŒé¨“ã‚’ã—ã¾ã—ãŸ"
        ][:num_variants]
        
        neutral_variants = [
            f"{original_text}ã¨ã„ã†çŠ¶æ³ãŒè¦³å¯Ÿã•ã‚Œã¾ã™",
            f"ä¸€èˆ¬çš„ã«{original_text}ã¨ã„ã†ã“ã¨ãŒè¨€ãˆã¾ã™",
            f"å®¢è¦³çš„ã«è¦‹ã‚‹ã¨{original_text}ã§ã™"
        ][:num_variants]
        
        return personal_variants, neutral_variants
    
    def compute_embeddings(self, texts: List[str]) -> np.ndarray:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿è¨ˆç®—"""
        if self.embedding_model is None:
            # ãƒ¢ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿
            return np.random.randn(len(texts), 384)  # MiniLMã®æ¬¡å…ƒæ•°
        
        try:
            embeddings = self.embedding_model.encode(texts, convert_to_numpy=True)
            return embeddings
        except Exception as e:
            logger.error(f"âŒ Embedding computation error: {e}")
            return np.random.randn(len(texts), 384)
    
    def compute_direction_vectors(self, 
                                personal_paraphrases: List[str], 
                                neutral_paraphrases: List[str]) -> np.ndarray:
        """
        ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ãƒ»ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«è¨€ã„æ›ãˆã‹ã‚‰æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—
        
        Args:
            personal_paraphrases: ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«æ–¹å‘ã®è¨€ã„æ›ãˆãƒªã‚¹ãƒˆ
            neutral_paraphrases: ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ã®è¨€ã„æ›ãˆãƒªã‚¹ãƒˆ
            
        Returns:
            æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ã®è¡Œåˆ— (num_pairs, embedding_dim)
        """
        direction_vectors = []
        
        # å„ãƒšã‚¢ã‹ã‚‰æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—
        min_length = min(len(personal_paraphrases), len(neutral_paraphrases))
        
        for i in range(min_length):
            personal_emb = self.compute_embeddings([personal_paraphrases[i]])[0]
            neutral_emb = self.compute_embeddings([neutral_paraphrases[i]])[0]
            
            # æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ« = personal - neutral
            direction_vec = personal_emb - neutral_emb
            
            # æ­£è¦åŒ–
            direction_vec = direction_vec / (np.linalg.norm(direction_vec) + 1e-8)
            
            direction_vectors.append(direction_vec)
        
        return np.array(direction_vectors)
    
    def perform_svd_decomposition(self, 
                                direction_matrix: np.ndarray, 
                                rank_reduction: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«è¡Œåˆ—ã®SVDåˆ†è§£
        
        Args:
            direction_matrix: æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«è¡Œåˆ— (num_history, embedding_dim)
            rank_reduction: ãƒ©ãƒ³ã‚¯å‰Šæ¸›æ•°
            
        Returns:
            (U, S, Vt) - SVDåˆ†è§£çµæœ
        """
        try:
            # SVDåˆ†è§£å®Ÿè¡Œ
            U, S, Vt = np.linalg.svd(direction_matrix, full_matrices=False)
            
            # ãƒ©ãƒ³ã‚¯å‰Šæ¸›
            rank = min(rank_reduction, len(S))
            U_reduced = U[:, :rank]
            S_reduced = S[:rank]
            Vt_reduced = Vt[:rank, :]
            
            logger.info(f"âœ… SVD decomposition completed")
            logger.info(f"   Original shape: {direction_matrix.shape}")
            logger.info(f"   Reduced rank: {rank}")
            logger.info(f"   Variance explained: {np.sum(S_reduced**2) / np.sum(S**2):.4f}")
            
            return U_reduced, S_reduced, Vt_reduced
            
        except Exception as e:
            logger.error(f"âŒ SVD decomposition error: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ä½è¡Œåˆ—
            dim = direction_matrix.shape[1]
            rank = min(rank_reduction, dim)
            U_reduced = np.eye(direction_matrix.shape[0], rank)
            S_reduced = np.ones(rank)
            Vt_reduced = np.eye(rank, dim)
            
            return U_reduced, S_reduced, Vt_reduced
    
    def create_direction_pieces(self, 
                              U: np.ndarray, 
                              S: np.ndarray, 
                              Vt: np.ndarray,
                              history_texts: List[str],
                              semantic_contexts: List[str]) -> List[DirectionPiece]:
        """
        SVDçµæœã‹ã‚‰æ–¹å‘ãƒ”ãƒ¼ã‚¹ã‚’ä½œæˆ
        
        Args:
            U, S, Vt: SVDåˆ†è§£çµæœ
            history_texts: å…ƒå±¥æ­´ãƒ†ã‚­ã‚¹ãƒˆ
            semantic_contexts: æ„å‘³çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            æ–¹å‘ãƒ”ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆ
        """
        pieces = []
        total_variance = np.sum(S**2)
        
        for i in range(len(S)):
            # é‡è¦åº¦è¨ˆç®—ï¼ˆç‰¹ç•°å€¤ã®å¯„ä¸ç‡ï¼‰
            importance = (S[i]**2) / total_variance
            
            # å“è³ªã‚¹ã‚³ã‚¢ï¼ˆæš«å®šçš„ã«é‡è¦åº¦ã‚’ä½¿ç”¨ï¼‰
            quality_score = importance
            
            # æ„å‘³çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ±ºå®š
            if i < len(semantic_contexts):
                semantic_context = semantic_contexts[i]
            else:
                semantic_context = f"Direction component {i+1}"
            
            # å…ƒå±¥æ­´ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆã“ã®æˆåˆ†ã«æœ€ã‚‚å¯„ä¸ã™ã‚‹å±¥æ­´ã‚’ç‰¹å®šï¼‰
            source_indices = []
            if i < U.shape[1]:
                # Uæˆåˆ†ã§æœ€ã‚‚å¤§ããªå€¤ã‚’æŒã¤å±¥æ­´ã‚’ç‰¹å®š
                u_component = U[:, i]
                top_indices = np.argsort(np.abs(u_component))[-3:]  # ä¸Šä½3ã¤
                source_indices = top_indices.tolist()
            
            piece = DirectionPiece(
                u_component=U[:, i] if i < U.shape[1] else np.zeros(U.shape[0]),
                singular_value=S[i],
                v_component=Vt[i, :] if i < Vt.shape[0] else np.zeros(Vt.shape[1]),
                importance=importance,
                quality_score=quality_score,
                semantic_context=semantic_context,
                source_history_indices=source_indices,
                creation_timestamp=time.time()
            )
            
            pieces.append(piece)
        
        # é‡è¦åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        pieces.sort(key=lambda x: x.importance, reverse=True)
        
        logger.info(f"âœ… Created {len(pieces)} direction pieces")
        logger.info(f"   Top piece importance: {pieces[0].importance:.4f}")
        logger.info(f"   Total variance covered: {sum(p.importance for p in pieces):.4f}")
        
        return pieces

def generate_improved_direction_pieces(
    user_history_texts: List[str],
    neutral_reference_text: str,
    llm_model: Any = None,
    embedding_model: Any = None,
    rank_reduction: int = 16
) -> List[Dict[str, Any]]:
    """
    æ”¹å–„ç‰ˆã®æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ãƒ”ãƒ¼ã‚¹ç”Ÿæˆé–¢æ•°
    
    Args:
        user_history_texts: ãƒ¦ãƒ¼ã‚¶å±¥æ­´ã®ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
        neutral_reference_text: ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«åŸºæº–ãƒ†ã‚­ã‚¹ãƒˆ
        llm_model: è¨€ã„æ›ãˆç”Ÿæˆç”¨ã®LLMãƒ¢ãƒ‡ãƒ«ï¼ˆæœªä½¿ç”¨ã€äº’æ›æ€§ã®ãŸã‚ï¼‰
        embedding_model: ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆæœªä½¿ç”¨ã€äº’æ›æ€§ã®ãŸã‚ï¼‰
        rank_reduction: SVDåˆ†è§£æ™‚ã®ãƒ©ãƒ³ã‚¯å‰Šæ¸›æ•°
        
    Returns:
        ãƒ”ãƒ¼ã‚¹æƒ…å ±ã‚’å«ã‚€è¾æ›¸ã®ãƒªã‚¹ãƒˆ
    """
    logger.info(f"ğŸš€ Starting improved direction pieces generation")
    logger.info(f"   History texts: {len(user_history_texts)}")
    logger.info(f"   Rank reduction: {rank_reduction}")
    
    # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼åˆæœŸåŒ–
    generator = ImprovedDirectionPiecesGenerator()
    
    all_direction_vectors = []
    semantic_contexts = []
    
    # å„å±¥æ­´ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†
    for i, history_text in enumerate(user_history_texts):
        logger.info(f"ğŸ“ Processing history {i+1}/{len(user_history_texts)}: {history_text[:50]}...")
        
        try:
            # è¨€ã„æ›ãˆç”Ÿæˆ
            personal_paraphrases, neutral_paraphrases = generator.generate_paraphrases(
                history_text, neutral_reference_text, num_variants=3
            )
            
            # æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—
            direction_vectors = generator.compute_direction_vectors(
                personal_paraphrases, neutral_paraphrases
            )
            
            # çµæœã‚’é›†ç©
            all_direction_vectors.append(direction_vectors)
            semantic_contexts.extend([f"History {i+1} - variant {j+1}" 
                                   for j in range(len(direction_vectors))])
            
        except Exception as e:
            logger.error(f"âŒ Error processing history {i+1}: {e}")
            continue
    
    if not all_direction_vectors:
        logger.error("âŒ No direction vectors generated")
        return []
    
    # å…¨æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¡Œåˆ—ã«çµåˆ
    direction_matrix = np.vstack(all_direction_vectors)
    logger.info(f"ğŸ“Š Combined direction matrix shape: {direction_matrix.shape}")
    
    # SVDåˆ†è§£å®Ÿè¡Œ
    U, S, Vt = generator.perform_svd_decomposition(direction_matrix, rank_reduction)
    
    # æ–¹å‘ãƒ”ãƒ¼ã‚¹ä½œæˆ
    pieces = generator.create_direction_pieces(U, S, Vt, user_history_texts, semantic_contexts)
    
    # è¾æ›¸å½¢å¼ã«å¤‰æ›
    pieces_dict = []
    for piece in pieces:
        piece_dict = {
            "u_component": piece.u_component.tolist(),
            "singular_value": float(piece.singular_value),
            "v_component": piece.v_component.tolist(),
            "importance": float(piece.importance),
            "quality_score": float(piece.quality_score),
            "semantic_context": piece.semantic_context,
            "source_history_indices": piece.source_history_indices,
            "creation_timestamp": piece.creation_timestamp
        }
        pieces_dict.append(piece_dict)
    
    logger.info(f"ğŸ‰ Direction pieces generation completed!")
    logger.info(f"   Generated {len(pieces_dict)} pieces")
    
    return pieces_dict

# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ãƒ¡ã‚¤ãƒ³é–¢æ•°
def main():
    """ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""
    print("ğŸ¦ æ”¹å–„ç‰ˆæ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ãƒ”ãƒ¼ã‚¹ç”Ÿæˆãƒ‡ãƒ¢")
    print("="*60)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    user_history = [
        "ä»Šæ—¥ã¯æ˜ ç”»ã‚’è¦‹ã«è¡ŒããŸã„",
        "é¢ç™½ã„SFæ˜ ç”»ãŒå¥½ãã§ã™", 
        "æœ€è¿‘è¦‹ãŸæ˜ ç”»ã¯è‰¯ã‹ã£ãŸ",
        "æ˜ ç”»é¤¨ã§å‹é”ã¨æ¥½ã—ã„æ™‚é–“ã‚’éã”ã—ãŸ",
        "æ–°ã—ã„æ˜ ç”»ã®äºˆå‘Šç·¨ã‚’è¦‹ã¦ãƒ¯ã‚¯ãƒ¯ã‚¯ã—ãŸ"
    ]
    
    neutral_reference = "æ˜ ç”»ã‚’è¦‹ã‚‹ã“ã¨ã¯æ¥½ã—ã„ã§ã™"
    
    # æ”¹å–„ç‰ˆãƒ”ãƒ¼ã‚¹ç”Ÿæˆå®Ÿè¡Œ
    start_time = time.time()
    pieces = generate_improved_direction_pieces(
        user_history_texts=user_history,
        neutral_reference_text=neutral_reference,
        rank_reduction=8
    )
    execution_time = time.time() - start_time
    
    # çµæœè¡¨ç¤º
    print(f"\nğŸ“Š ç”Ÿæˆçµæœ:")
    print(f"   å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’")
    print(f"   ç”Ÿæˆãƒ”ãƒ¼ã‚¹æ•°: {len(pieces)}")
    
    for i, piece in enumerate(pieces[:3]):  # ä¸Šä½3ã¤ã®ã¿è¡¨ç¤º
        print(f"\nğŸ”¸ ãƒ”ãƒ¼ã‚¹ {i+1}:")
        print(f"   ç‰¹ç•°å€¤: {piece['singular_value']:.4f}")
        print(f"   é‡è¦åº¦: {piece['importance']:.4f}")
        print(f"   å“è³ªã‚¹ã‚³ã‚¢: {piece['quality_score']:.4f}")
        print(f"   æ„å‘³çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {piece['semantic_context']}")
        print(f"   Uæˆåˆ†æ¬¡å…ƒ: {len(piece['u_component'])}")
        print(f"   Væˆåˆ†æ¬¡å…ƒ: {len(piece['v_component'])}")
    
    print("\nğŸ‰ ãƒ‡ãƒ¢å®Œäº†!")

if __name__ == "__main__":
    main()