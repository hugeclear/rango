#!/usr/bin/env python3
"""
LaMP-QA CFS-Chameleonçµ±åˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
è³ªå•å¿œç­”ã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹å”èª¿çš„åŸ‹ã‚è¾¼ã¿ç·¨é›†ã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½è©•ä¾¡ãƒ»æ¯”è¼ƒãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

ç‰¹å¾´:
- å¾“æ¥ç‰ˆChameleon vs CFS-Chameleonæ€§èƒ½æ¯”è¼ƒï¼ˆQAã‚¿ã‚¹ã‚¯ç‰¹åŒ–ï¼‰
- QAè©•ä¾¡æŒ‡æ¨™: ROUGE-L, BLEU, BERTScore
- ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆæ€§èƒ½åˆ†æ
- å”èª¿å­¦ç¿’åŠ¹æœã®å®šé‡è©•ä¾¡
- çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®š
- å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´æ©Ÿèƒ½
"""

import json
import os
import time
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
import numpy as np
import torch
from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
import logging

# QAè©•ä¾¡æŒ‡æ¨™ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    from rouge_score import rouge_scorer
    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
    from bert_score import score as bert_score
    QA_METRICS_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ QA metrics libraries not available: {e}")
    print("Please install: pip install rouge-score nltk bert-score")
    QA_METRICS_AVAILABLE = False

# CFS-Chameleonçµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
try:
    from chameleon_cfs_integrator import CollaborativeChameleonEditor
    from cfs_chameleon_extension import CollaborativeDirectionPool, UserContext
    from chameleon_evaluator import ChameleonEvaluator
    CFS_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ CFS-Chameleon modules not available: {e}")
    CFS_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass 
class QAEvaluationResult:
    """QAè©•ä¾¡çµæœæ ¼ç´ã‚¯ãƒ©ã‚¹"""
    rouge_l: float
    bleu_score: float
    bert_score_f1: float
    inference_time: float
    cold_start_performance: float
    pool_utilization: float = 0.0
    user_coverage: int = 0
    user_scores: Dict[str, Dict[str, float]] = None

@dataclass
class QAComparisonResults:
    """QAæ¯”è¼ƒè©•ä¾¡çµæœ"""
    legacy_chameleon: QAEvaluationResult
    cfs_chameleon: QAEvaluationResult
    improvement_metrics: Dict[str, float]
    statistical_significance: Dict[str, float]

class LaMPQADataLoader:
    """LaMP-QA ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼"""
    
    def __init__(self, data_path: str = None):
        if data_path is None:
            data_path = "chameleon_prime_personalization/data/raw/LaMP-QA/merged.json"
        self.data_path = Path(data_path)
        self.merged_data = None
        self.ground_truth = None
        
    def load_merged_data(self) -> List[Dict]:
        """LaMP-QA merged.jsonã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        possible_paths = [
            self.data_path,
            Path("chameleon_prime_personalization/data/raw/LaMP-QA/merged.json"),
            Path("data/raw/LaMP-QA/merged.json"),
            Path("LaMP-QA/merged.json")
        ]
        
        for path in possible_paths:
            if path.exists():
                logger.info(f"Loading LaMP-QA data from: {path}")
                with open(path, 'r', encoding='utf-8') as f:
                    self.merged_data = json.load(f)
                return self.merged_data
                
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: LaMP-2ãƒ‡ãƒ¼ã‚¿ã§ä»£ç”¨
        logger.warning("LaMP-QA data not found, using LaMP-2 as fallback")
        fallback_paths = [
            "chameleon_prime_personalization/data/raw/LaMP-2/merged.json",
            "data/raw/LaMP-2/merged.json"
        ]
        
        for path in fallback_paths:
            if Path(path).exists():
                logger.info(f"Loading fallback data from: {path}")
                with open(path, 'r', encoding='utf-8') as f:
                    self.merged_data = json.load(f)
                return self.merged_data
        
        raise FileNotFoundError("Neither LaMP-QA nor LaMP-2 data found")
    
    def load_ground_truth(self) -> Dict[str, str]:
        """æ­£è§£ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        qa_answer_paths = [
            "chameleon_prime_personalization/data/raw/LaMP-QA/answers.json",
            "data/raw/LaMP-QA/answers.json"
        ]
        
        for path in qa_answer_paths:
            if Path(path).exists():
                logger.info(f"Loading QA answers from: {path}")
                with open(path, 'r', encoding='utf-8') as f:
                    answers = json.load(f)
                return {str(item.get('id', item.get('question_id'))): 
                       item.get('answer', item.get('ground_truth', '')) 
                       for item in answers}
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        logger.warning("LaMP-QA answers not found, using LaMP-2 as fallback")
        fallback_paths = [
            "chameleon_prime_personalization/data/raw/LaMP-2/answers.json",
            "data/raw/LaMP-2/answers.json"
        ]
        
        for path in fallback_paths:
            if Path(path).exists():
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, dict) and 'golds' in data:
                        # LaMP-2ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: {"task": "LaMP_2", "golds": [...]}
                        answers = data['golds']
                        return {str(item.get('id')): item.get('output', item.get('answer', '')) 
                                for item in answers}
                    elif isinstance(data, list):
                        # ç›´æ¥ãƒªã‚¹ãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                        return {str(item.get('id')): item.get('answer', item.get('output', '')) 
                                for item in data}
                    else:
                        return {}
        
        return {}

class QAMetricsCalculator:
    """QAè©•ä¾¡æŒ‡æ¨™è¨ˆç®—ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        if QA_METRICS_AVAILABLE:
            self.rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
            self.smoothing = SmoothingFunction().method1
        
    def calculate_rouge_l(self, prediction: str, reference: str) -> float:
        """ROUGE-Lè¨ˆç®—"""
        if not QA_METRICS_AVAILABLE:
            return 0.0
        
        try:
            scores = self.rouge_scorer.score(reference, prediction)
            return scores['rougeL'].fmeasure
        except:
            return 0.0
    
    def calculate_bleu(self, prediction: str, reference: str) -> float:
        """BLEUè¨ˆç®—"""
        if not QA_METRICS_AVAILABLE:
            return 0.0
        
        try:
            pred_tokens = prediction.lower().split()
            ref_tokens = [reference.lower().split()]
            return sentence_bleu(ref_tokens, pred_tokens, smoothing_function=self.smoothing)
        except:
            return 0.0
    
    def calculate_bert_score(self, predictions: List[str], references: List[str]) -> float:
        """BERTScoreè¨ˆç®—ï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰"""
        if not QA_METRICS_AVAILABLE or not predictions or not references:
            return 0.0
        
        try:
            P, R, F1 = bert_score(predictions, references, lang="en", verbose=False)
            return F1.mean().item()
        except:
            return 0.0

class LaMPQACFSBenchmark:
    """LaMP-QA CFS-Chameleonãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, use_collaboration: bool = True, config_path: str = None,
                 hook_layer: str = None, alpha_p: float = None, alpha_g: float = None,
                 rank_reduction: int = None):
        self.use_collaboration = use_collaboration
        self.config_path = config_path or "cfs_config.yaml"
        
        # å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        self.dynamic_params = {
            'hook_layer': hook_layer,
            'alpha_p': alpha_p, 
            'alpha_g': alpha_g,
            'rank_reduction': rank_reduction
        }
        
        # çµ±è¨ˆæƒ…å ±åˆæœŸåŒ–
        self.evaluation_stats = {
            'total_users': 0,
            'cold_start_users': 0,
            'warm_start_users': 0,
            'avg_user_history_length': 0.0
        }
        
        # ãƒ‡ãƒ¼ã‚¿ã¨ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼åˆæœŸåŒ–
        self.data_loader = LaMPQADataLoader()
        self.qa_calculator = QAMetricsCalculator()
        self.test_data = self.data_loader.load_merged_data()
        self.ground_truth = self.data_loader.load_ground_truth()
        
        # ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼åˆæœŸåŒ–
        self._initialize_editors()
        
        # çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.output_dir = Path("lampqa_evaluation_results")
        self.output_dir.mkdir(exist_ok=True)
        
        logger.info(f"âœ… LaMP-QA CFSè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        logger.info(f"   å”èª¿æ©Ÿèƒ½: {'æœ‰åŠ¹' if use_collaboration else 'ç„¡åŠ¹'}")
        logger.info(f"   ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«æ•°: {len(self.test_data)}")
        logger.info(f"   å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")

    def _initialize_editors(self):
        """ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼åˆæœŸåŒ–"""
        if not CFS_AVAILABLE:
            logger.error("CFS-Chameleonãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            self.cfs_editor = None
            self.legacy_editor = None
            return
        
        try:
            # å”èª¿ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼
            if self.use_collaboration:
                collab_config = self._load_collaboration_config()
                self.cfs_editor = CollaborativeChameleonEditor(
                    use_collaboration=True,
                    collaboration_config=collab_config,
                    config_path=self.config_path
                )
                # å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é©ç”¨
                self._apply_dynamic_params(self.cfs_editor)
                logger.info("âœ… CFS-Chameleonå”èª¿ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼åˆæœŸåŒ–å®Œäº†")
            else:
                self.cfs_editor = None
            
            # ãƒ¬ã‚¬ã‚·ãƒ¼ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ï¼ˆæ¯”è¼ƒç”¨ï¼‰
            self.legacy_editor = CollaborativeChameleonEditor(
                use_collaboration=False,
                config_path=self.config_path
            )
            # ãƒ¬ã‚¬ã‚·ãƒ¼ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ã«ã‚‚å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é©ç”¨
            self._apply_dynamic_params(self.legacy_editor)
            logger.info("âœ… ãƒ¬ã‚¬ã‚·ãƒ¼Chameleonã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼åˆæœŸåŒ–å®Œäº†")
            
            # Theta vectorsèª­ã¿è¾¼ã¿
            self._load_theta_vectors()
            
        except Exception as e:
            logger.error(f"ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            self.cfs_editor = None
            self.legacy_editor = None

    def _apply_dynamic_params(self, editor):
        """å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ã«é©ç”¨"""
        if self.dynamic_params['hook_layer']:
            editor._config_target_layers = [self.dynamic_params['hook_layer']]
        if self.dynamic_params['alpha_p'] is not None:
            editor._config_alpha_personal = self.dynamic_params['alpha_p']
        if self.dynamic_params['alpha_g'] is not None:
            editor._config_alpha_general = self.dynamic_params['alpha_g']

    def _load_collaboration_config(self) -> Dict[str, Any]:
        """å”èª¿è¨­å®šèª­ã¿è¾¼ã¿"""
        try:
            import yaml
            config_file = Path(self.config_path)
            if config_file.exists():
                with open(config_file, 'r') as f:
                    config = yaml.safe_load(f)
                    collaboration_config = config.get('collaboration', {})
                    
                    # å‹•çš„rank_reductioné©ç”¨
                    if self.dynamic_params['rank_reduction']:
                        collaboration_config['rank_reduction'] = self.dynamic_params['rank_reduction']
                    
                    return collaboration_config
        except Exception as e:
            logger.warning(f"å”èª¿è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        return {
            'pool_size': 1000,
            'rank_reduction': self.dynamic_params.get('rank_reduction', 32),
            'privacy_noise_std': 0.01,
            'enable_learning': False
        }

    def _load_theta_vectors(self):
        """Theta vectorsèª­ã¿è¾¼ã¿"""
        # CollaborativeChameleonEditorã§ã¯åˆæœŸåŒ–æ™‚ã«è‡ªå‹•èª­ã¿è¾¼ã¿ã•ã‚Œã‚‹ãŸã‚ã€
        # ã“ã“ã§ã¯èª­ã¿è¾¼ã¿çŠ¶æ³ã®ãƒã‚§ãƒƒã‚¯ã®ã¿å®Ÿè¡Œ
        logger.info("âœ… Theta vectors reading handled by CollaborativeChameleonEditor")
        
        # èª­ã¿è¾¼ã¿çŠ¶æ³ç¢ºèª
        if hasattr(self.legacy_editor, 'theta_personal') and self.legacy_editor.theta_personal is not None:
            logger.info(f"Legacy editor theta vectors: {self.legacy_editor.theta_personal.shape}")
        else:
            logger.warning("Legacy editor theta vectors not loaded")
            
        if self.cfs_editor and hasattr(self.cfs_editor, 'theta_personal') and self.cfs_editor.theta_personal is not None:
            logger.info(f"CFS editor theta vectors: {self.cfs_editor.theta_personal.shape}")
        else:
            logger.warning("CFS editor theta vectors not loaded")

    def evaluate_qa_performance(self, editor, samples: List[Dict], 
                               system_name: str) -> QAEvaluationResult:
        """QAæ€§èƒ½è©•ä¾¡"""
        predictions = []
        references = []
        user_scores = {}
        inference_times = []
        
        logger.info(f"ğŸ”„ {system_name}è©•ä¾¡é–‹å§‹")
        
        for i, sample in enumerate(samples):
            if i % 50 == 0:
                logger.info(f"   é€²æ—: {i}/{len(samples)}")
            
            try:
                user_id = sample.get('user_id', 'unknown')
                question = sample.get('input', sample.get('question', ''))
                
                start_time = time.time()
                
                # å”èª¿çš„ç”Ÿæˆã¾ãŸã¯ãƒ¬ã‚¬ã‚·ãƒ¼ç”Ÿæˆ
                if hasattr(editor, 'generate_with_chameleon'):
                    answer = editor.generate_with_chameleon(question, max_length=100)
                else:
                    answer = question  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                
                inference_time = time.time() - start_time
                inference_times.append(inference_time)
                
                # æ­£è§£å–å¾—
                sample_id = str(sample.get('id', i))
                reference = self.ground_truth.get(sample_id, '')
                
                predictions.append(answer)
                references.append(reference)
                
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥ã‚¹ã‚³ã‚¢è¨ˆç®—
                if user_id not in user_scores:
                    user_scores[user_id] = {'rouge_l': [], 'bleu': []}
                
                rouge_l = self.qa_calculator.calculate_rouge_l(answer, reference)
                bleu = self.qa_calculator.calculate_bleu(answer, reference)
                
                user_scores[user_id]['rouge_l'].append(rouge_l)
                user_scores[user_id]['bleu'].append(bleu)
                
            except Exception as e:
                logger.warning(f"ã‚µãƒ³ãƒ—ãƒ«{i}è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
                predictions.append("")
                references.append("")
                inference_times.append(0.0)
        
        # å…¨ä½“æŒ‡æ¨™è¨ˆç®—
        total_rouge_l = np.mean([self.qa_calculator.calculate_rouge_l(p, r) 
                                for p, r in zip(predictions, references)])
        total_bleu = np.mean([self.qa_calculator.calculate_bleu(p, r) 
                             for p, r in zip(predictions, references)])
        bert_f1 = self.qa_calculator.calculate_bert_score(predictions, references)
        
        # ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆæ€§èƒ½è¨ˆç®—
        cold_start_performance = self._calculate_cold_start_performance(user_scores)
        
        # ãƒ—ãƒ¼ãƒ«åˆ©ç”¨ç‡ï¼ˆCFS-Chameleonã®å ´åˆï¼‰
        pool_utilization = 0.0
        if hasattr(editor, 'direction_pool') and editor.direction_pool:
            pool_utilization = len(editor.direction_pool.pieces) / editor.direction_pool.pool_size
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥å¹³å‡ã‚¹ã‚³ã‚¢è¨ˆç®—
        user_avg_scores = {}
        for user_id, scores in user_scores.items():
            user_avg_scores[user_id] = {
                'rouge_l': np.mean(scores['rouge_l']) if scores['rouge_l'] else 0.0,
                'bleu': np.mean(scores['bleu']) if scores['bleu'] else 0.0
            }
        
        return QAEvaluationResult(
            rouge_l=total_rouge_l,
            bleu_score=total_bleu,
            bert_score_f1=bert_f1,
            inference_time=sum(inference_times),
            cold_start_performance=cold_start_performance,
            pool_utilization=pool_utilization,
            user_coverage=len(user_scores),
            user_scores=user_avg_scores
        )

    def _calculate_cold_start_performance(self, user_scores: Dict) -> float:
        """ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€§èƒ½è¨ˆç®—"""
        cold_start_scores = []
        for user_id, scores in user_scores.items():
            if len(scores['rouge_l']) <= 3:  # å±¥æ­´3ä»¶ä»¥ä¸‹ã‚’ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆã¨ã™ã‚‹
                cold_start_scores.extend(scores['rouge_l'])
        
        return np.mean(cold_start_scores) if cold_start_scores else 0.0

    def run_comparison_evaluation(self) -> QAComparisonResults:
        """æ¯”è¼ƒè©•ä¾¡å®Ÿè¡Œ"""
        logger.info("ğŸš€ LaMP-QA CFS-Chameleonæ¯”è¼ƒè©•ä¾¡é–‹å§‹")
        logger.info("=" * 60)
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆæœ€å¤§100ã‚µãƒ³ãƒ—ãƒ«ï¼‰
        test_samples = self.test_data[:100]
        
        # ãƒ¬ã‚¬ã‚·ãƒ¼Chameleonè©•ä¾¡
        legacy_result = self.evaluate_qa_performance(
            self.legacy_editor, test_samples, "å¾“æ¥ç‰ˆChameleon"
        )
        
        # CFS-Chameleonè©•ä¾¡ï¼ˆå”èª¿æ©Ÿèƒ½æœ‰åŠ¹æ™‚ï¼‰
        if self.use_collaboration and self.cfs_editor:
            cfs_result = self.evaluate_qa_performance(
                self.cfs_editor, test_samples, "CFS-Chameleon"
            )
        else:
            cfs_result = legacy_result  # å”èª¿æ©Ÿèƒ½ç„¡åŠ¹æ™‚ã¯åŒã˜çµæœ
        
        # æ”¹å–„æŒ‡æ¨™è¨ˆç®—
        improvement_metrics = {
            'rouge_l_improvement': ((cfs_result.rouge_l - legacy_result.rouge_l) / 
                                   legacy_result.rouge_l * 100) if legacy_result.rouge_l > 0 else 0.0,
            'bleu_improvement': ((cfs_result.bleu_score - legacy_result.bleu_score) / 
                                legacy_result.bleu_score * 100) if legacy_result.bleu_score > 0 else 0.0,
            'bert_improvement': ((cfs_result.bert_score_f1 - legacy_result.bert_score_f1) / 
                                legacy_result.bert_score_f1 * 100) if legacy_result.bert_score_f1 > 0 else 0.0,
            'speed_improvement': ((legacy_result.inference_time - cfs_result.inference_time) / 
                                 legacy_result.inference_time * 100) if legacy_result.inference_time > 0 else 0.0,
        }
        
        # çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®š
        statistical_significance = self._calculate_statistical_significance(
            legacy_result, cfs_result
        )
        
        # çµæœä¿å­˜
        results = QAComparisonResults(
            legacy_chameleon=legacy_result,
            cfs_chameleon=cfs_result,
            improvement_metrics=improvement_metrics,
            statistical_significance=statistical_significance
        )
        
        self._save_results(results)
        self._display_results(results)
        
        return results

    def _calculate_statistical_significance(self, legacy: QAEvaluationResult, 
                                          cfs: QAEvaluationResult) -> Dict[str, float]:
        """çµ±è¨ˆçš„æœ‰æ„æ€§è¨ˆç®—"""
        try:
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥ã‚¹ã‚³ã‚¢ã§æ¯”è¼ƒï¼ˆã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå°ã•ã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            legacy_scores = [scores['rouge_l'] for scores in legacy.user_scores.values()]
            cfs_scores = [scores['rouge_l'] for scores in cfs.user_scores.values()]
            
            if len(legacy_scores) > 1 and len(cfs_scores) > 1:
                t_stat, p_value = stats.ttest_rel(cfs_scores, legacy_scores)
            else:
                p_value = 1.0
                
            return {
                'p_value': p_value,
                'is_significant': p_value < 0.05
            }
        except:
            return {'p_value': 1.0, 'is_significant': False}

    def _save_results(self, results: QAComparisonResults):
        """çµæœä¿å­˜"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        # JSONä¿å­˜
        result_dict = {
            'legacy_chameleon': {
                'rouge_l': results.legacy_chameleon.rouge_l,
                'bleu_score': results.legacy_chameleon.bleu_score,
                'bert_score_f1': results.legacy_chameleon.bert_score_f1,
                'inference_time': results.legacy_chameleon.inference_time,
                'cold_start_performance': results.legacy_chameleon.cold_start_performance
            },
            'cfs_chameleon': {
                'rouge_l': results.cfs_chameleon.rouge_l,
                'bleu_score': results.cfs_chameleon.bleu_score,
                'bert_score_f1': results.cfs_chameleon.bert_score_f1,
                'inference_time': results.cfs_chameleon.inference_time,
                'cold_start_performance': results.cfs_chameleon.cold_start_performance,
                'pool_utilization': results.cfs_chameleon.pool_utilization,
                'user_coverage': results.cfs_chameleon.user_coverage
            },
            'improvement_metrics': results.improvement_metrics,
            'statistical_significance': results.statistical_significance,
            'dynamic_params': self.dynamic_params
        }
        
        output_file = self.output_dir / f"lampqa_comparison_{timestamp}.json"
        with open(output_file, 'w') as f:
            json.dump(result_dict, f, indent=2)
        
        logger.info(f"ğŸ“ çµæœä¿å­˜: {output_file}")

    def _display_results(self, results: QAComparisonResults):
        """çµæœè¡¨ç¤º"""
        print("\n" + "=" * 60)
        print("ğŸ“Š LaMP-QA CFS-Chameleonæ¯”è¼ƒè©•ä¾¡çµæœ")
        print("=" * 60)
        
        # ãƒ¬ã‚¬ã‚·ãƒ¼çµæœ
        print(f"\nğŸ”¸ å¾“æ¥ç‰ˆChameleon:")
        print(f"   ROUGE-L:      {results.legacy_chameleon.rouge_l:.4f}")
        print(f"   BLEU Score:   {results.legacy_chameleon.bleu_score:.4f}")
        print(f"   BERTScore:    {results.legacy_chameleon.bert_score_f1:.4f}")
        print(f"   æ¨è«–æ™‚é–“:     {results.legacy_chameleon.inference_time:.2f}ç§’")
        print(f"   ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆ: {results.legacy_chameleon.cold_start_performance:.4f}")
        
        # CFS-Chameleonçµæœ
        print(f"\nğŸ¦ CFS-Chameleon:")
        print(f"   ROUGE-L:      {results.cfs_chameleon.rouge_l:.4f}")
        print(f"   BLEU Score:   {results.cfs_chameleon.bleu_score:.4f}")
        print(f"   BERTScore:    {results.cfs_chameleon.bert_score_f1:.4f}")
        print(f"   æ¨è«–æ™‚é–“:     {results.cfs_chameleon.inference_time:.2f}ç§’")
        print(f"   ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆ: {results.cfs_chameleon.cold_start_performance:.4f}")
        print(f"   ãƒ—ãƒ¼ãƒ«åˆ©ç”¨ç‡: {results.cfs_chameleon.pool_utilization:.2%}")
        print(f"   ãƒ¦ãƒ¼ã‚¶ãƒ¼ç¯„å›²: {results.cfs_chameleon.user_coverage}äºº")
        
        # æ”¹å–„åŠ¹æœ
        print(f"\nğŸ“ˆ æ”¹å–„åŠ¹æœ:")
        print(f"   ROUGE-Læ”¹å–„:  {results.improvement_metrics['rouge_l_improvement']:+.1f}%")
        print(f"   BLEUæ”¹å–„:     {results.improvement_metrics['bleu_improvement']:+.1f}%")
        print(f"   BERTScoreæ”¹å–„: {results.improvement_metrics['bert_improvement']:+.1f}%")
        print(f"   é€Ÿåº¦æ”¹å–„:     {results.improvement_metrics['speed_improvement']:+.1f}%")
        print(f"   çµ±è¨ˆçš„æœ‰æ„æ€§: p = {results.statistical_significance['p_value']:.4f}")
        
        is_significant = results.statistical_significance['is_significant']
        print(f"   {'âœ… çµ±è¨ˆçš„ã«æœ‰æ„ãªæ”¹å–„ãŒç¢ºèªã•ã‚Œã¾ã—ãŸ' if is_significant else 'âš ï¸ çµ±è¨ˆçš„æœ‰æ„æ€§ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ'}")
        
        # å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¡¨ç¤º
        if any(v is not None for v in self.dynamic_params.values()):
            print(f"\nâš™ï¸ ä½¿ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
            for key, value in self.dynamic_params.items():
                if value is not None:
                    print(f"   {key}: {value}")
        
        print(f"\nâœ… LaMP-QA CFS-Chameleonæ¯”è¼ƒè©•ä¾¡å®Œäº†!")
        print(f"ğŸ“ çµæœä¿å­˜å…ˆ: {self.output_dir}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="LaMP-QA CFS-Chameleon Benchmark")
    parser.add_argument("--compare_modes", action="store_true", 
                       help="Compare legacy vs CFS-Chameleon")
    parser.add_argument("--use_collaboration", action="store_true",
                       help="Enable collaborative features")
    parser.add_argument("--config", default="cfs_config.yaml",
                       help="Configuration file path")
    parser.add_argument("--hook_layer", type=str,
                       help="Target hook layer (e.g., model.layers.16.mlp)")
    parser.add_argument("--alpha_p", type=float,
                       help="Alpha personal parameter")
    parser.add_argument("--alpha_g", type=float,
                       help="Alpha general parameter")
    parser.add_argument("--rank_reduction", type=int,
                       help="Rank reduction for collaboration pool")
    
    args = parser.parse_args()
    
    if not QA_METRICS_AVAILABLE:
        logger.error("QA evaluation metrics not available. Please install required libraries.")
        return
    
    if not CFS_AVAILABLE:
        logger.error("CFS-Chameleon modules not available.")
        return
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    benchmark = LaMPQACFSBenchmark(
        use_collaboration=args.use_collaboration,
        config_path=args.config,
        hook_layer=args.hook_layer,
        alpha_p=args.alpha_p,
        alpha_g=args.alpha_g,
        rank_reduction=args.rank_reduction
    )
    
    if args.compare_modes:
        results = benchmark.run_comparison_evaluation()
        
        # ç°¡å˜ãªå½±éŸ¿åˆ†æã‚³ãƒ¡ãƒ³ãƒˆ
        print(f"\nğŸ’¡ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å½±éŸ¿åˆ†æ:")
        if args.hook_layer:
            print(f"   Hookå±¤ {args.hook_layer}: {'æ€§èƒ½å‘ä¸Š' if any(v > 0 for v in results.improvement_metrics.values()) else 'æ€§èƒ½å½±éŸ¿é™å®šçš„'}")
        if args.alpha_p:
            print(f"   Î±_personal {args.alpha_p}: {'å€‹æ€§å¼·åŒ–åŠ¹æœ' if results.improvement_metrics['rouge_l_improvement'] > 0 else 'éå‰°ç·¨é›†ã®å¯èƒ½æ€§'}")
        if args.rank_reduction:
            print(f"   Rank reduction {args.rank_reduction}: {'å”èª¿ç²¾åº¦å‘ä¸Š' if results.cfs_chameleon.pool_utilization > 0.3 else 'å”èª¿åŠ¹æœé™å®šçš„'}")
        
        print(f"   ç·åˆè©•ä¾¡: {'ğŸ† ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–æˆåŠŸ' if results.statistical_significance['is_significant'] else 'ğŸ”§ æ›´ãªã‚‹èª¿æ•´ãŒå¿…è¦'}")
    
    logger.info("ğŸ‰ ã™ã¹ã¦ã®è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸ!")

def test_theta_vector_loading():
    """Theta vectorèª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆé–¢æ•°"""
    print("\nğŸ§ª Theta Vectorèª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 50)
    
    try:
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
        config_path = "cfs_config.yaml"
        editor = CollaborativeChameleonEditor(
            use_collaboration=False,
            config_path=config_path
        )
        
        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        print(f"ğŸ“ Config path: {config_path}")
        print(f"ğŸ”— Theta P path: {getattr(editor, 'theta_p_path', 'Not set')}")
        print(f"ğŸ”— Theta N path: {getattr(editor, 'theta_n_path', 'Not set')}")
        
        # Assertion checks
        assert hasattr(editor, "theta_personal"), "theta_personal attribute not found"
        assert hasattr(editor, "theta_neutral"), "theta_neutral attribute not found"
        
        if editor.theta_personal is not None and editor.theta_neutral is not None:
            print(f"âœ… Theta vectors loaded successfully:")
            print(f"   Personal shape: {editor.theta_personal.shape}")
            print(f"   Neutral shape:  {editor.theta_neutral.shape}")
            return True
        else:
            print("âŒ Theta vectors are None")
            return False
            
    except Exception as e:
        print(f"âŒ Theta vector loading test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_direction_vector_loading():
    """Direction vectorèª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆé–¢æ•°"""
    print("\nğŸ§ª Direction Vectorèª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 50)
    
    try:
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
        config_path = "cfs_config.yaml"
        editor = CollaborativeChameleonEditor(
            use_collaboration=False,
            config_path=config_path
        )
        
        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        print(f"ğŸ“ Config path: {config_path}")
        print(f"ğŸ”— Direction P path: {getattr(editor, 'direction_p_path', 'Not set')}")
        print(f"ğŸ”— Direction N path: {getattr(editor, 'direction_n_path', 'Not set')}")
        
        # Assertion checks
        assert hasattr(editor, "direction_personal"), "direction_personal attribute not found"
        assert hasattr(editor, "direction_neutral"), "direction_neutral attribute not found"
        
        if editor.direction_personal is not None and editor.direction_neutral is not None:
            print(f"âœ… Direction vectors loaded successfully:")
            print(f"   Personal shape: {editor.direction_personal.shape}")
            print(f"   Neutral shape:  {editor.direction_neutral.shape}")
            print(f"   Direction vectors ready for Chameleon editing!")
            return True
        else:
            print("âŒ Direction vectors are None")
            return False
            
    except Exception as e:
        print(f"âŒ Direction vector loading test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    # Theta vectorèª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    theta_test_success = test_theta_vector_loading()
    
    # Direction vectorèª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    direction_test_success = test_direction_vector_loading()
    
    if theta_test_success and direction_test_success:
        print("\nğŸš€ All vector loading tests passed, proceeding with main evaluation...")
        main()
    else:
        print("\nâš ï¸ Vector loading tests failed, please check configuration")
        if not theta_test_success:
            print("   âŒ Theta vector test failed")
        if not direction_test_success:
            print("   âŒ Direction vector test failed")
        exit(1)