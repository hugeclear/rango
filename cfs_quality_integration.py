#!/usr/bin/env python3
"""
CFS-Chameleonå“è³ªã‚¹ã‚³ã‚¢çµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
ã‚¿ã‚¹ã‚¯ãƒ™ãƒ¼ã‚¹å“è³ªè©•ä¾¡ã‚’CFS-Chameleonã‚·ã‚¹ãƒ†ãƒ ã«çµ±åˆ
"""

import numpy as np
import torch
from typing import List, Dict, Any, Tuple, Optional
import logging
import time
import json
from pathlib import Path

# ã‚¿ã‚¹ã‚¯ãƒ™ãƒ¼ã‚¹å“è³ªè©•ä¾¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from task_based_quality_evaluator import (
    calculate_improved_quality_score,
    QualityEvaluationConfig,
    create_sample_evaluation_datasets
)

# CFS-Chameleoné–¢é€£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
try:
    from cfs_chameleon_extension import DirectionPiece, CollaborativeDirectionPool
    from chameleon_cfs_integrator import CollaborativeChameleonEditor
    from cfs_improved_integration import ImprovedCFSChameleonEditor
    CFS_AVAILABLE = True
except ImportError:
    print("âš ï¸ CFS modules not available")
    CFS_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class QualityAwareCFSEditor:
    """å“è³ªè©•ä¾¡æ©Ÿèƒ½ä»˜ãCFS-Chameleonã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼"""
    
    def __init__(self, 
                 base_config_path: str = "cfs_config.yaml",
                 quality_config: QualityEvaluationConfig = None,
                 enable_quality_evaluation: bool = True):
        """
        åˆæœŸåŒ–
        
        Args:
            base_config_path: ãƒ™ãƒ¼ã‚¹CFSè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
            quality_config: å“è³ªè©•ä¾¡è¨­å®š
            enable_quality_evaluation: å“è³ªè©•ä¾¡ã®æœ‰åŠ¹åŒ–
        """
        self.base_config_path = base_config_path
        self.quality_config = quality_config or QualityEvaluationConfig(
            metrics=["rouge", "bleu", "bertscore"],
            metric_weights={"rouge": 0.4, "bleu": 0.3, "bertscore": 0.3},
            max_eval_samples=20
        )
        self.enable_quality_evaluation = enable_quality_evaluation
        
        # CFS-Chameleonã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ã®åˆæœŸåŒ–
        self._initialize_cfs_editor()
        
        # è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        self.evaluation_datasets = self._setup_evaluation_data()
        
        logger.info("âœ… QualityAwareCFSEditor initialized")
        logger.info(f"   Quality evaluation: {'enabled' if enable_quality_evaluation else 'disabled'}")
        logger.info(f"   Available datasets: {list(self.evaluation_datasets.keys())}")
    
    def _initialize_cfs_editor(self):
        """CFS-Chameleonã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ã®åˆæœŸåŒ–"""
        if CFS_AVAILABLE:
            try:
                self.cfs_editor = ImprovedCFSChameleonEditor(
                    use_collaboration=True,
                    config_path=self.base_config_path,
                    enable_improved_pieces=True
                )
                logger.info("âœ… ImprovedCFSChameleonEditor loaded")
            except Exception as e:
                logger.warning(f"ImprovedCFSChameleonEditor failed, using basic editor: {e}")
                self.cfs_editor = CollaborativeChameleonEditor(
                    use_collaboration=True,
                    config_path=self.base_config_path
                )
        else:
            self.cfs_editor = None
            logger.warning("âš ï¸ CFS-Chameleon not available, using mock editor")
    
    def _setup_evaluation_data(self) -> Dict[str, List[Tuple[str, str]]]:
        """è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        datasets = create_sample_evaluation_datasets()
        
        # ã‚«ã‚¹ã‚¿ãƒ CFSç‰¹åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¿½åŠ 
        datasets["cfs_personalization"] = [
            (
                "ç§ã®å¥½ã¿ã«åˆã£ãŸæ˜ ç”»ã‚’æŽ¨è–¦ã—ã¦ãã ã•ã„",
                "ã‚ãªãŸã®å¥½ã¿ã‚’è€ƒæ…®ã—ãŸæ˜ ç”»ã‚’ãŠã™ã™ã‚ã—ã¾ã™"
            ),
            (
                "å€‹äººçš„ãªè³ªå•ã«ç­”ãˆã¦ãã ã•ã„ï¼šä»Šæ—¥ã®æ°—åˆ†ã¯ã©ã†ã§ã™ã‹ï¼Ÿ", 
                "å€‹äººçš„ãªçŠ¶æ³ã‚’è¸ã¾ãˆã¦ãŠç­”ãˆã—ã¾ã™"
            ),
            (
                "ç§ã®å±¥æ­´ã«åŸºã¥ã„ã¦ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ãã ã•ã„",
                "ã‚ãªãŸã®éŽåŽ»ã®çµŒé¨“ã‚’è€ƒæ…®ã—ãŸã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¾ã™"
            ),
            (
                "ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸå­¦ç¿’è¨ˆç”»ã‚’ä½œæˆã—ã¦ãã ã•ã„",
                "ã‚ãªãŸã«æœ€é©åŒ–ã•ã‚ŒãŸå­¦ç¿’ãƒ—ãƒ©ãƒ³ã‚’ææ¡ˆã—ã¾ã™"
            ),
            (
                "ç§ã®èˆˆå‘³ã«åˆã£ãŸè©±é¡Œã§ä¼šè©±ã—ã¾ã—ã‚‡ã†",
                "ã‚ãªãŸã®é–¢å¿ƒäº‹ã«ã¤ã„ã¦æ¥½ã—ãè©±ã—ã¾ã—ã‚‡ã†"
            )
        ]
        
        return datasets
    
    def generate_with_quality_piece(self, 
                                  input_text: str, 
                                  piece: Any,
                                  alpha_personal: float = 0.1,
                                  alpha_neutral: float = -0.05) -> str:
        """
        å“è³ªè©•ä¾¡å¯¾å¿œã®ç”Ÿæˆé–¢æ•°
        
        Args:
            input_text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            piece: æ–¹å‘ãƒ”ãƒ¼ã‚¹
            alpha_personal: ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«æ–¹å‘å¼·åº¦
            alpha_neutral: ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘å¼·åº¦
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        if self.cfs_editor is None:
            # ãƒ¢ãƒƒã‚¯ç”Ÿæˆ
            return f"Mock quality-aware generation: {input_text[:50]}..."
        
        try:
            # CFS-Chameleonã§ã®ç”Ÿæˆ
            if hasattr(self.cfs_editor, 'generate_with_improved_collaboration'):
                result = self.cfs_editor.generate_with_improved_collaboration(
                    prompt=input_text,
                    user_id="quality_eval_user",
                    alpha_personal=alpha_personal,
                    alpha_neutral=alpha_neutral,
                    max_length=self.quality_config.generation_max_length
                )
            else:
                result = self.cfs_editor.generate_with_chameleon(
                    prompt=input_text,
                    alpha_personal=alpha_personal,
                    alpha_neutral=alpha_neutral,
                    max_length=self.quality_config.generation_max_length
                )
            
            return result
            
        except Exception as e:
            logger.error(f"Generation error: {e}")
            return f"Generation failed: {input_text[:30]}..."
    
    def evaluate_piece_quality(self, 
                             piece: Any,
                             dataset_name: str = "cfs_personalization") -> float:
        """
        æ–¹å‘ãƒ”ãƒ¼ã‚¹ã®å“è³ªè©•ä¾¡
        
        Args:
            piece: è©•ä¾¡å¯¾è±¡ã®æ–¹å‘ãƒ”ãƒ¼ã‚¹
            dataset_name: ä½¿ç”¨ã™ã‚‹è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
            
        Returns:
            å“è³ªã‚¹ã‚³ã‚¢
        """
        if not self.enable_quality_evaluation:
            logger.info("Quality evaluation disabled, returning default score")
            return 0.5
        
        if dataset_name not in self.evaluation_datasets:
            logger.warning(f"Dataset {dataset_name} not found, using default")
            dataset_name = "cfs_personalization"
        
        eval_dataset = self.evaluation_datasets[dataset_name]
        
        # å“è³ªè©•ä¾¡å®Ÿè¡Œ
        logger.info(f"ðŸŽ¯ Evaluating piece quality on {dataset_name} dataset")
        
        def generate_func(input_text: str, piece: Any) -> str:
            return self.generate_with_quality_piece(input_text, piece)
        
        try:
            quality_score = calculate_improved_quality_score(
                piece=piece,
                eval_dataset=eval_dataset,
                generate_with_piece=generate_func,
                config=self.quality_config
            )
            
            logger.info(f"âœ… Piece quality evaluation completed: {quality_score:.4f}")
            return quality_score
            
        except Exception as e:
            logger.error(f"âŒ Quality evaluation failed: {e}")
            return 0.0
    
    def batch_evaluate_pieces_quality(self, 
                                    pieces: List[Any],
                                    dataset_name: str = "cfs_personalization") -> List[float]:
        """
        è¤‡æ•°æ–¹å‘ãƒ”ãƒ¼ã‚¹ã®ä¸€æ‹¬å“è³ªè©•ä¾¡
        
        Args:
            pieces: è©•ä¾¡å¯¾è±¡ã®æ–¹å‘ãƒ”ãƒ¼ã‚¹ãƒªã‚¹ãƒˆ
            dataset_name: ä½¿ç”¨ã™ã‚‹è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
            
        Returns:
            å“è³ªã‚¹ã‚³ã‚¢ã®ãƒªã‚¹ãƒˆ
        """
        logger.info(f"ðŸ”„ Batch evaluating {len(pieces)} pieces")
        
        quality_scores = []
        for i, piece in enumerate(pieces):
            logger.info(f"   Evaluating piece {i+1}/{len(pieces)}")
            
            score = self.evaluate_piece_quality(piece, dataset_name)
            quality_scores.append(score)
        
        logger.info(f"âœ… Batch evaluation completed")
        logger.info(f"   Average quality: {np.mean(quality_scores):.4f}")
        logger.info(f"   Score range: {np.min(quality_scores):.4f} - {np.max(quality_scores):.4f}")
        
        return quality_scores
    
    def update_pieces_with_quality_scores(self, 
                                        pieces: List[Any],
                                        dataset_name: str = "cfs_personalization") -> List[Any]:
        """
        æ–¹å‘ãƒ”ãƒ¼ã‚¹ã®å“è³ªã‚¹ã‚³ã‚¢ã‚’æ›´æ–°
        
        Args:
            pieces: æ–¹å‘ãƒ”ãƒ¼ã‚¹ãƒªã‚¹ãƒˆ
            dataset_name: è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
            
        Returns:
            å“è³ªã‚¹ã‚³ã‚¢æ›´æ–°æ¸ˆã¿ãƒ”ãƒ¼ã‚¹ãƒ†ãƒªã‚¹ãƒˆ
        """
        quality_scores = self.batch_evaluate_pieces_quality(pieces, dataset_name)
        
        updated_pieces = []
        for piece, quality_score in zip(pieces, quality_scores):
            # ãƒ”ãƒ¼ã‚¹ã®å“è³ªã‚¹ã‚³ã‚¢ã‚’æ›´æ–°
            if hasattr(piece, 'quality_score'):
                piece.quality_score = quality_score
            elif isinstance(piece, dict):
                piece['quality_score'] = quality_score
            else:
                logger.warning(f"Cannot update quality score for piece type: {type(piece)}")
            
            updated_pieces.append(piece)
        
        return updated_pieces
    
    def generate_quality_report(self, 
                              pieces: List[Any],
                              dataset_name: str = "cfs_personalization") -> Dict[str, Any]:
        """
        å“è³ªè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        
        Args:
            pieces: æ–¹å‘ãƒ”ãƒ¼ã‚¹ãƒªã‚¹ãƒˆ
            dataset_name: è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
            
        Returns:
            å“è³ªãƒ¬ãƒãƒ¼ãƒˆ
        """
        logger.info("ðŸ“Š Generating quality evaluation report")
        
        quality_scores = self.batch_evaluate_pieces_quality(pieces, dataset_name)
        
        report = {
            "evaluation_timestamp": time.time(),
            "dataset_name": dataset_name,
            "total_pieces": len(pieces),
            "quality_scores": quality_scores,
            "statistics": {
                "mean": float(np.mean(quality_scores)),
                "std": float(np.std(quality_scores)),
                "min": float(np.min(quality_scores)),
                "max": float(np.max(quality_scores)),
                "median": float(np.median(quality_scores))
            },
            "quality_distribution": {
                "high_quality_count": sum(1 for s in quality_scores if s > 0.7),
                "medium_quality_count": sum(1 for s in quality_scores if 0.3 < s <= 0.7),
                "low_quality_count": sum(1 for s in quality_scores if s <= 0.3)
            },
            "evaluation_config": {
                "metrics": self.quality_config.metrics,
                "metric_weights": self.quality_config.metric_weights,
                "max_eval_samples": self.quality_config.max_eval_samples
            }
        }
        
        return report

def demonstrate_quality_aware_cfs():
    """å“è³ªè©•ä¾¡æ©Ÿèƒ½ä»˜ãCFS-Chameleonã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("ðŸ¦Ž å“è³ªè©•ä¾¡æ©Ÿèƒ½ä»˜ãCFS-Chameleonãƒ‡ãƒ¢")
    print("=" * 60)
    
    # å“è³ªè©•ä¾¡è¨­å®š
    quality_config = QualityEvaluationConfig(
        metrics=["rouge", "bleu", "bertscore"],
        metric_weights={"rouge": 0.4, "bleu": 0.3, "bertscore": 0.3},
        max_eval_samples=5,
        generation_max_length=80
    )
    
    # å“è³ªè©•ä¾¡æ©Ÿèƒ½ä»˜ãã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ã®åˆæœŸåŒ–
    editor = QualityAwareCFSEditor(
        base_config_path="cfs_config.yaml",
        quality_config=quality_config,
        enable_quality_evaluation=True
    )
    
    # ã‚µãƒ³ãƒ—ãƒ«æ–¹å‘ãƒ”ãƒ¼ã‚¹ã®ä½œæˆï¼ˆå®Ÿéš›ã«ã¯DirectionPieceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰
    sample_pieces = [
        {"id": "piece_1", "vector": np.random.randn(3072), "quality_score": 0.0},
        {"id": "piece_2", "vector": np.random.randn(3072), "quality_score": 0.0},
        {"id": "piece_3", "vector": np.random.randn(3072), "quality_score": 0.0}
    ]
    
    print(f"\nðŸ“ Sample pieces created: {len(sample_pieces)}")
    
    # å€‹åˆ¥ãƒ”ãƒ¼ã‚¹å“è³ªè©•ä¾¡ã®ãƒ†ã‚¹ãƒˆ
    print("\nðŸŽ¯ Individual piece quality evaluation:")
    for i, piece in enumerate(sample_pieces):
        print(f"\nðŸ”¸ Evaluating piece {i+1}:")
        
        score = editor.evaluate_piece_quality(piece, "cfs_personalization")
        print(f"   Quality Score: {score:.4f}")
    
    # ä¸€æ‹¬å“è³ªè©•ä¾¡ã®ãƒ†ã‚¹ãƒˆ
    print("\nðŸ”„ Batch quality evaluation:")
    updated_pieces = editor.update_pieces_with_quality_scores(
        sample_pieces, "cfs_personalization"
    )
    
    for i, piece in enumerate(updated_pieces):
        score = piece.get('quality_score', 'N/A')
        print(f"   Piece {i+1}: {score:.4f}")
    
    # å“è³ªãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
    print("\nðŸ“Š Quality evaluation report:")
    report = editor.generate_quality_report(sample_pieces, "cfs_personalization")
    
    print(f"   Total pieces: {report['total_pieces']}")
    print(f"   Average quality: {report['statistics']['mean']:.4f}")
    print(f"   Quality range: {report['statistics']['min']:.4f} - {report['statistics']['max']:.4f}")
    print(f"   High quality pieces: {report['quality_distribution']['high_quality_count']}")
    print(f"   Medium quality pieces: {report['quality_distribution']['medium_quality_count']}")
    print(f"   Low quality pieces: {report['quality_distribution']['low_quality_count']}")
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    report_file = "quality_evaluation_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\nðŸ’¾ Quality report saved: {report_file}")
    print("\nðŸŽ‰ å“è³ªè©•ä¾¡æ©Ÿèƒ½ä»˜ãCFS-Chameleonãƒ‡ãƒ¢å®Œäº†!")

if __name__ == "__main__":
    demonstrate_quality_aware_cfs()