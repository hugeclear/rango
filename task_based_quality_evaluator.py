#!/usr/bin/env python3
"""
CFS-Chameleonå‘ã‘ã‚¿ã‚¹ã‚¯ãƒ™ãƒ¼ã‚¹å“è³ªã‚¹ã‚³ã‚¢è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
å®Ÿéš›ã®ç”Ÿæˆã‚¿ã‚¹ã‚¯æ€§èƒ½ï¼ˆROUGE, BLEU, BERTScoreï¼‰ã«åŸºã¥ãå“è³ªã‚¹ã‚³ã‚¢ç®—å‡º
"""

import numpy as np
import torch
from typing import List, Tuple, Callable, Dict, Any, Optional
from dataclasses import dataclass
import logging
import time
import json
from pathlib import Path

# è©•ä¾¡ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
try:
    from rouge_score import rouge_scorer
    from sacrebleu import BLEU
    from bert_score import score as bert_score
    EVALUATION_LIBS_AVAILABLE = True
except ImportError:
    print("âš ï¸ Evaluation libraries not available. Using mock implementations.")
    EVALUATION_LIBS_AVAILABLE = False

# CFS-Chameleoné–¢é€£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
try:
    from cfs_chameleon_extension import DirectionPiece
    from chameleon_cfs_integrator import CollaborativeChameleonEditor
    CFS_AVAILABLE = True
except ImportError:
    print("âš ï¸ CFS modules not available. Using mock implementations.")
    CFS_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class TaskPerformanceMetrics:
    """ã‚¿ã‚¹ã‚¯æ€§èƒ½æŒ‡æ¨™ã®çµæœ"""
    rouge_l: float
    bleu_score: float
    bert_score: float
    weighted_average: float
    sample_count: int

@dataclass
class QualityEvaluationConfig:
    """å“è³ªè©•ä¾¡è¨­å®š"""
    metrics: List[str] = None
    metric_weights: Dict[str, float] = None
    max_eval_samples: int = 50
    generation_max_length: int = 100
    normalize_scores: bool = True
    
    def __post_init__(self):
        if self.metrics is None:
            self.metrics = ["rouge", "bleu", "bertscore"]
        
        if self.metric_weights is None:
            self.metric_weights = {
                "rouge": 0.4,
                "bleu": 0.3, 
                "bertscore": 0.3
            }

class TaskBasedQualityEvaluator:
    """ã‚¿ã‚¹ã‚¯ãƒ™ãƒ¼ã‚¹å“è³ªè©•ä¾¡å™¨"""
    
    def __init__(self, config: QualityEvaluationConfig = None):
        """
        åˆæœŸåŒ–
        
        Args:
            config: è©•ä¾¡è¨­å®š
        """
        self.config = config or QualityEvaluationConfig()
        self._initialize_evaluators()
        
        logger.info("âœ… TaskBasedQualityEvaluator initialized")
        logger.info(f"   Metrics: {self.config.metrics}")
        logger.info(f"   Weights: {self.config.metric_weights}")
    
    def _initialize_evaluators(self):
        """è©•ä¾¡å™¨ã®åˆæœŸåŒ–"""
        if EVALUATION_LIBS_AVAILABLE:
            # ROUGEè©•ä¾¡å™¨
            self.rouge_scorer = rouge_scorer.RougeScorer(
                ['rouge1', 'rouge2', 'rougeL'], use_stemmer=True
            )
            
            # BLEUè©•ä¾¡å™¨
            self.bleu_scorer = BLEU()
            
            logger.info("âœ… Real evaluation libraries loaded")
        else:
            # ãƒ¢ãƒƒã‚¯è©•ä¾¡å™¨
            self.rouge_scorer = None
            self.bleu_scorer = None
            logger.warning("âš ï¸ Using mock evaluation implementations")
    
    def compute_rouge_score(self, prediction: str, reference: str) -> float:
        """ROUGE-L ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        if self.rouge_scorer:
            try:
                scores = self.rouge_scorer.score(reference, prediction)
                return scores['rougeL'].fmeasure
            except Exception as e:
                logger.warning(f"ROUGE computation error: {e}")
                return 0.0
        else:
            # ãƒ¢ãƒƒã‚¯å®Ÿè£…ï¼šç°¡å˜ãªé‡è¤‡ç‡
            pred_tokens = set(prediction.lower().split())
            ref_tokens = set(reference.lower().split())
            if len(ref_tokens) == 0:
                return 0.0
            overlap = len(pred_tokens & ref_tokens)
            return overlap / len(ref_tokens)
    
    def compute_bleu_score(self, prediction: str, reference: str) -> float:
        """BLEU ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        if self.bleu_scorer:
            try:
                # SacreBLEUã¯è¤‡æ•°å‚ç…§ã‚’ã‚µãƒãƒ¼ãƒˆ
                score = self.bleu_scorer.sentence_score(prediction, [reference])
                return score.score / 100.0  # 0-1ã«æ­£è¦åŒ–
            except Exception as e:
                logger.warning(f"BLEU computation error: {e}")
                return 0.0
        else:
            # ãƒ¢ãƒƒã‚¯å®Ÿè£…ï¼šn-gramé‡è¤‡
            pred_words = prediction.lower().split()
            ref_words = reference.lower().split()
            
            if len(pred_words) == 0 or len(ref_words) == 0:
                return 0.0
            
            # å˜ç´”ãª1-gram BLEUè¿‘ä¼¼
            matches = 0
            for word in pred_words:
                if word in ref_words:
                    matches += 1
            
            precision = matches / len(pred_words)
            recall = matches / len(ref_words)
            
            if precision + recall == 0:
                return 0.0
            
            return 2 * (precision * recall) / (precision + recall)
    
    def compute_bert_score(self, prediction: str, reference: str) -> float:
        """BERTScore è¨ˆç®—"""
        if EVALUATION_LIBS_AVAILABLE:
            try:
                # BERTScoreã¯æ–‡ã®ãƒªã‚¹ãƒˆã‚’æœŸå¾…
                P, R, F1 = bert_score([prediction], [reference], lang='en', verbose=False)
                return F1[0].item()
            except Exception as e:
                logger.warning(f"BERTScore computation error: {e}")
                return 0.0
        else:
            # ãƒ¢ãƒƒã‚¯å®Ÿè£…ï¼šæ–‡å­—ãƒ¬ãƒ™ãƒ«é¡ä¼¼åº¦
            pred_chars = set(prediction.lower())
            ref_chars = set(reference.lower())
            
            if len(ref_chars) == 0:
                return 0.0
            
            overlap = len(pred_chars & ref_chars)
            return overlap / max(len(pred_chars), len(ref_chars))
    
    def evaluate_single_sample(self, 
                             prediction: str, 
                             reference: str) -> Dict[str, float]:
        """å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«ã®è©•ä¾¡"""
        metrics = {}
        
        if "rouge" in self.config.metrics:
            metrics["rouge"] = self.compute_rouge_score(prediction, reference)
        
        if "bleu" in self.config.metrics:
            metrics["bleu"] = self.compute_bleu_score(prediction, reference)
        
        if "bertscore" in self.config.metrics:
            metrics["bertscore"] = self.compute_bert_score(prediction, reference)
        
        return metrics
    
    def calculate_weighted_score(self, metrics: Dict[str, float]) -> float:
        """åŠ é‡å¹³å‡ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        total_weight = 0.0
        weighted_sum = 0.0
        
        for metric, score in metrics.items():
            if metric in self.config.metric_weights:
                weight = self.config.metric_weights[metric]
                weighted_sum += score * weight
                total_weight += weight
        
        if total_weight == 0:
            return 0.0
        
        return weighted_sum / total_weight

def calculate_improved_quality_score(
    piece: Any,  # DirectionPiece (CFS_AVAILABLEã§ãªã„å ´åˆã¯Any)
    eval_dataset: List[Tuple[str, str]],
    generate_with_piece: Callable[[str, Any], str],
    metrics: List[str] = None,
    config: QualityEvaluationConfig = None
) -> float:
    """
    ã‚¿ã‚¹ã‚¯æ€§èƒ½æŒ‡æ¨™ã«åŸºã¥ãå“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    
    Args:
        piece: è©•ä¾¡å¯¾è±¡ã®æ–¹å‘ãƒ”ãƒ¼ã‚¹
        eval_dataset: List of (input_text, reference_text) ãƒšã‚¢
        generate_with_piece: (prompt, piece) â†’ LLMç”Ÿæˆçµæœ ã‚’è¿”ã™é–¢æ•°
        metrics: ä½¿ç”¨ã™ã‚‹æŒ‡æ¨™ãƒªã‚¹ãƒˆï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
        config: è©•ä¾¡è¨­å®š
        
    Returns:
        float: è¤‡æ•°æŒ‡æ¨™ã®åŠ é‡å¹³å‡ã«ã‚ˆã‚‹å“è³ªã‚¹ã‚³ã‚¢
    """
    # è¨­å®šã®åˆæœŸåŒ–
    if config is None:
        config = QualityEvaluationConfig()
        if metrics:
            config.metrics = metrics
    
    # è©•ä¾¡å™¨ã®åˆæœŸåŒ–
    evaluator = TaskBasedQualityEvaluator(config)
    
    logger.info(f"ğŸ¯ Starting task-based quality evaluation for piece")
    logger.info(f"   Evaluation samples: {min(len(eval_dataset), config.max_eval_samples)}")
    logger.info(f"   Metrics: {config.metrics}")
    
    # è©•ä¾¡å®Ÿè¡Œ
    all_metrics = []
    successful_evaluations = 0
    
    # ã‚µãƒ³ãƒ—ãƒ«æ•°åˆ¶é™
    eval_samples = eval_dataset[:config.max_eval_samples]
    
    for i, (input_text, reference_text) in enumerate(eval_samples):
        try:
            # ãƒ”ãƒ¼ã‚¹ã‚’é©ç”¨ã—ãŸç”Ÿæˆ
            logger.debug(f"Evaluating sample {i+1}/{len(eval_samples)}")
            
            generated_text = generate_with_piece(input_text, piece)
            
            if not generated_text or len(generated_text.strip()) == 0:
                logger.warning(f"Empty generation for sample {i+1}")
                continue
            
            # å„æŒ‡æ¨™ã®è¨ˆç®—
            sample_metrics = evaluator.evaluate_single_sample(
                generated_text, reference_text
            )
            
            all_metrics.append(sample_metrics)
            successful_evaluations += 1
            
        except Exception as e:
            logger.warning(f"Evaluation error for sample {i+1}: {e}")
            continue
    
    if successful_evaluations == 0:
        logger.error("âŒ No successful evaluations")
        return 0.0
    
    # é›†ç´„çµ±è¨ˆã®è¨ˆç®—
    aggregated_metrics = {}
    for metric in config.metrics:
        if metric in all_metrics[0]:  # æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã«ã‚ã‚‹æŒ‡æ¨™ã®ã¿
            scores = [m[metric] for m in all_metrics if metric in m]
            aggregated_metrics[metric] = np.mean(scores)
    
    # åŠ é‡å¹³å‡ã«ã‚ˆã‚‹æœ€çµ‚å“è³ªã‚¹ã‚³ã‚¢
    final_quality_score = evaluator.calculate_weighted_score(aggregated_metrics)
    
    # æ­£è¦åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if config.normalize_scores:
        # 0-1ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—
        final_quality_score = max(0.0, min(1.0, final_quality_score))
    
    logger.info(f"âœ… Quality evaluation completed")
    logger.info(f"   Successful evaluations: {successful_evaluations}/{len(eval_samples)}")
    logger.info(f"   Individual metrics: {aggregated_metrics}")
    logger.info(f"   Final quality score: {final_quality_score:.4f}")
    
    return final_quality_score

def create_mock_generation_function():
    """ãƒ¢ãƒƒã‚¯ç”Ÿæˆé–¢æ•°ã®ä½œæˆï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
    def mock_generate_with_piece(input_text: str, piece: Any) -> str:
        """
        ãƒ¢ãƒƒã‚¯ç”Ÿæˆé–¢æ•°
        å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€CFS-Chameleonã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã—ã¦pieceé©ç”¨ç”Ÿæˆã‚’è¡Œã†
        """
        # åŸºæœ¬çš„ãªå¤‰æ›ï¼ˆå®Ÿéš›ã«ã¯LLMã‚’ä½¿ç”¨ï¼‰
        if "è¦ç´„" in input_text:
            return f"è¦ç´„çµæœï¼š{input_text[:50]}ã®è¦ç‚¹"
        elif "ç¿»è¨³" in input_text:
            return f"Translation: {input_text[:30]} translated"
        else:
            return f"ç”Ÿæˆçµæœï¼š{input_text[:40]}ã«å¯¾ã™ã‚‹å¿œç­”"
    
    return mock_generate_with_piece

def create_sample_evaluation_datasets() -> Dict[str, List[Tuple[str, str]]]:
    """ã‚µãƒ³ãƒ—ãƒ«è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ"""
    datasets = {
        "summarization": [
            (
                "ä»Šæ—¥ã¯å¤©æ°—ãŒè‰¯ãã€å…¬åœ’ã§å‹äººã¨æ•£æ­©ã‚’æ¥½ã—ã¿ã¾ã—ãŸã€‚æ¡œã®èŠ±ãŒå’²ã„ã¦ã„ã¦ã€ã¨ã¦ã‚‚ç¾ã—ã„æ™¯è‰²ã§ã—ãŸã€‚å¤šãã®äººãŒèŠ±è¦‹ã‚’ã—ã¦ãŠã‚Šã€è³‘ã‚„ã‹ãªé›°å›²æ°—ã§ã—ãŸã€‚", 
                "å‹äººã¨å…¬åœ’ã§æ¡œã‚’è¦‹ãªãŒã‚‰æ•£æ­©ã‚’æ¥½ã—ã‚“ã ã€‚"
            ),
            (
                "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒå§‹ã¾ã‚Šã€ãƒãƒ¼ãƒ ãƒ¡ãƒ³ãƒãƒ¼ã¨ã®ä¼šè­°ã‚’è¡Œã„ã¾ã—ãŸã€‚ã‚¿ã‚¹ã‚¯ã®åˆ†æ‹…ã‚’æ±ºã‚ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚æ¥é€±ã¾ã§ã«å„è‡ªã®æ‹…å½“éƒ¨åˆ†ã‚’å®Œæˆã•ã›ã‚‹äºˆå®šã§ã™ã€‚",
                "æ–°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ãƒãƒ¼ãƒ ä¼šè­°ã‚’è¡Œã„ã€ã‚¿ã‚¹ã‚¯åˆ†æ‹…ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æ±ºã‚ãŸã€‚"
            ),
            (
                "æœ€è¿‘èª­ã‚“ã æœ¬ãŒã¨ã¦ã‚‚é¢ç™½ãã€ä¸»äººå…¬ã®æˆé•·ç‰©èªã«æ„Ÿå‹•ã—ã¾ã—ãŸã€‚å›°é›£ã‚’ä¹—ã‚Šè¶Šãˆã¦ç›®æ¨™ã‚’é”æˆã™ã‚‹å§¿ã«å‹‡æ°—ã‚’ã‚‚ã‚‰ã„ã¾ã—ãŸã€‚å‹äººã«ã‚‚å‹§ã‚ãŸã„ã¨æ€ã„ã¾ã™ã€‚",
                "é¢ç™½ã„æœ¬ã‚’èª­ã¿ã€ä¸»äººå…¬ã®æˆé•·ç‰©èªã«æ„Ÿå‹•ã—å‹äººã«å‹§ã‚ãŸã„ã€‚"
            )
        ],
        
        "qa": [
            (
                "æ—¥æœ¬ã®é¦–éƒ½ã¯ã©ã“ã§ã™ã‹ï¼Ÿ",
                "æ±äº¬"
            ),
            (
                "æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
                "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ãŒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è‡ªå‹•çš„ã«å­¦ç¿’ã—ã¦äºˆæ¸¬ã‚„åˆ¤æ–­ã‚’è¡Œã†æŠ€è¡“"
            ),
            (
                "å¥åº·çš„ãªé£Ÿäº‹ã®ã‚³ãƒ„ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
                "ãƒãƒ©ãƒ³ã‚¹ã®è‰¯ã„æ „é¤Šæ‘‚å–ã€é©é‡ã®é£Ÿäº‹ã€é‡èœã‚„æœç‰©ã‚’å¤šãå–ã‚‹ã“ã¨ãŒé‡è¦"
            )
        ],
        
        "dialogue": [
            (
                "ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­ã€‚",
                "ãã†ã§ã™ã­ï¼æ•£æ­©ã«ã¯ã´ã£ãŸã‚Šã®æ—¥ã§ã™ã­ã€‚"
            ),
            (
                "é€±æœ«ã¯ä½•ã‚’ã—ã¾ã™ã‹ï¼Ÿ",
                "å‹äººã¨æ˜ ç”»ã‚’è¦‹ã«è¡Œãäºˆå®šã§ã™ã€‚ã‚ãªãŸã¯ã„ã‹ãŒã§ã™ã‹ï¼Ÿ"
            ),
            (
                "æœ€è¿‘å¿™ã—ãã¦ç–²ã‚Œã¦ã„ã¾ã™ã€‚",
                "ãŠç–²ã‚Œæ§˜ã§ã™ã€‚ã—ã£ã‹ã‚Šä¼‘æ¯ã‚’å–ã‚‹ã“ã¨ã‚‚å¤§åˆ‡ã§ã™ã‚ˆã€‚"
            )
        ]
    }
    
    return datasets

def demonstrate_task_based_quality_evaluation():
    """ã‚¿ã‚¹ã‚¯ãƒ™ãƒ¼ã‚¹å“è³ªè©•ä¾¡ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("ğŸ¯ ã‚¿ã‚¹ã‚¯ãƒ™ãƒ¼ã‚¹å“è³ªã‚¹ã‚³ã‚¢è©•ä¾¡ãƒ‡ãƒ¢")
    print("=" * 60)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ
    datasets = create_sample_evaluation_datasets()
    
    # ãƒ¢ãƒƒã‚¯ç”Ÿæˆé–¢æ•°ã®ä½œæˆ
    generate_func = create_mock_generation_function()
    
    # ç•°ãªã‚‹è¨­å®šã§ã®è©•ä¾¡
    configs = [
        QualityEvaluationConfig(
            metrics=["rouge", "bleu"],
            metric_weights={"rouge": 0.6, "bleu": 0.4},
            max_eval_samples=3
        ),
        QualityEvaluationConfig(
            metrics=["rouge", "bleu", "bertscore"],
            metric_weights={"rouge": 0.4, "bleu": 0.3, "bertscore": 0.3},
            max_eval_samples=3
        ),
        QualityEvaluationConfig(
            metrics=["bertscore"],
            metric_weights={"bertscore": 1.0},
            max_eval_samples=3
        )
    ]
    
    # å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã‚³ãƒ³ãƒ•ã‚£ã‚°ã§è©•ä¾¡
    for dataset_name, dataset in datasets.items():
        print(f"\nğŸ“Š Dataset: {dataset_name}")
        print("-" * 40)
        
        for i, config in enumerate(configs):
            print(f"\nğŸ”¸ Configuration {i+1}: {config.metrics}")
            
            # ãƒ€ãƒŸãƒ¼ãƒ”ãƒ¼ã‚¹ï¼ˆå®Ÿéš›ã«ã¯DirectionPieceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰
            mock_piece = {"id": f"piece_{i}", "vector": np.random.randn(768)}
            
            try:
                quality_score = calculate_improved_quality_score(
                    piece=mock_piece,
                    eval_dataset=dataset,
                    generate_with_piece=generate_func,
                    config=config
                )
                
                print(f"   Quality Score: {quality_score:.4f}")
                
            except Exception as e:
                print(f"   Error: {e}")
    
    print("\nğŸ‰ ã‚¿ã‚¹ã‚¯ãƒ™ãƒ¼ã‚¹å“è³ªè©•ä¾¡ãƒ‡ãƒ¢å®Œäº†!")

def create_real_cfs_quality_evaluator():
    """å®Ÿéš›ã®CFS-Chameleonã¨çµ±åˆã—ãŸå“è³ªè©•ä¾¡é–¢æ•°"""
    
    def cfs_generate_with_piece(input_text: str, piece: Any) -> str:
        """
        CFS-Chameleonã‚’ä½¿ç”¨ã—ãŸå®Ÿéš›ã®ç”Ÿæˆé–¢æ•°
        
        Args:
            input_text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            piece: DirectionPiece
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not CFS_AVAILABLE:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ¢ãƒƒã‚¯ç”Ÿæˆ
            return f"Mock generation for: {input_text[:50]}..."
        
        try:
            # CFS-Chameleonã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ã®åˆæœŸåŒ–
            editor = CollaborativeChameleonEditor(
                use_collaboration=True,
                config_path="cfs_config.yaml"
            )
            
            # æ–¹å‘ãƒ”ãƒ¼ã‚¹ã‚’ä¸€æ™‚çš„ã«ãƒ—ãƒ¼ãƒ«ã«è¿½åŠ 
            if hasattr(editor, 'direction_pool') and hasattr(editor.direction_pool, 'pieces'):
                editor.direction_pool.pieces.append(piece)
            
            # ç”Ÿæˆå®Ÿè¡Œ
            result = editor.generate_with_chameleon(
                prompt=input_text,
                alpha_personal=0.1,
                alpha_neutral=-0.05,
                max_length=100
            )
            
            return result
            
        except Exception as e:
            logger.error(f"CFS generation error: {e}")
            return f"Generation error: {input_text[:30]}..."
    
    return cfs_generate_with_piece

if __name__ == "__main__":
    # ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
    demonstrate_task_based_quality_evaluation()
    
    # å®Ÿéš›ã®CFSçµ±åˆä¾‹
    print("\n" + "=" * 60)
    print("ğŸ¦ CFS-Chameleonçµ±åˆä¾‹")
    
    cfs_generate_func = create_real_cfs_quality_evaluator()
    datasets = create_sample_evaluation_datasets()
    
    config = QualityEvaluationConfig(
        metrics=["rouge", "bleu", "bertscore"],
        max_eval_samples=2
    )
    
    # è¦ç´„ã‚¿ã‚¹ã‚¯ã§ã®è©•ä¾¡ä¾‹
    mock_piece = {"vector": np.random.randn(3072), "quality": 0.5}
    
    try:
        quality_score = calculate_improved_quality_score(
            piece=mock_piece,
            eval_dataset=datasets["summarization"],
            generate_with_piece=cfs_generate_func,
            config=config
        )
        
        print(f"âœ… CFS-Chameleon Quality Score: {quality_score:.4f}")
        
    except Exception as e:
        print(f"âŒ CFS evaluation error: {e}")