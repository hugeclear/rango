#!/usr/bin/env python3
"""
GraphRAGå‹•çš„CFS-Chameleonå‘ã‘FAISSç´¢å¼•æ§‹ç¯‰ã‚·ã‚¹ãƒ†ãƒ  (Step-2 å®Œå…¨ç‰ˆ)
Step-1ã®å…±é€šåŸ‹ã‚è¾¼ã¿ã‹ã‚‰FAISSç´¢å¼•ã‚’æ§‹ç¯‰ã—ã€Top-kè¿‘å‚æ¤œç´¢ã§ç–ã‚°ãƒ©ãƒ•ã‚¨ãƒƒã‚¸ã‚’ç”Ÿæˆ

## æ©Ÿèƒ½:
- å…¥åŠ›: NPY+JSON / Parquetå½¢å¼å¯¾å¿œ
- ç´¢å¼•: IVF+PQ (å¤§è¦æ¨¡å‘ã‘) / HNSW (é«˜ç²¾åº¦å‘ã‘)
- ãƒ¡ãƒˆãƒªãƒƒã‚¯: cosine (L2æ­£è¦åŒ–è‡ªå‹•) / l2è·é›¢
- GPU/CPUè‡ªå‹•é¸æŠã€OOMå›é¿ã€ãƒãƒƒãƒå‡¦ç†
- ç–ã‚°ãƒ©ãƒ•ã‚¨ãƒƒã‚¸ãƒªã‚¹ãƒˆå‡ºåŠ›ï¼ˆPPRç”¨ï¼‰
- çµ±è¨ˆæƒ…å ±ãƒ­ã‚°ï¼ˆã‚²ãƒ¼ãƒˆè¨­è¨ˆç”¨ï¼‰
- å˜ä½“ãƒ†ã‚¹ãƒˆå†…è”µ

## å¯¾è±¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯:
- LaMP-2ï¼ˆç”Ÿæˆã‚¿ã‚¹ã‚¯ï¼‰
- Tenrecï¼ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ï¼‰
"""

import os
import sys
import json
import argparse
import random
import warnings
import time
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Union
import numpy as np
import pandas as pd
import logging
from tqdm import tqdm
from scipy import stats

# ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯
try:
    import faiss
    import pyarrow.parquet as pq
    DEPENDENCIES_OK = True
except ImportError as e:
    print(f"âŒ ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒä¸è¶³: {e}")
    print("pip install faiss-gpu pandas pyarrow tqdm numpy scipy ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    print("CPUç’°å¢ƒã®å ´åˆã¯ faiss-gpu â†’ faiss-cpu ã«å¤‰æ›´")
    sys.exit(1)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# è­¦å‘ŠæŠ‘åˆ¶
warnings.filterwarnings("ignore", category=FutureWarning)

class FAISSIndexBuilder:
    """FAISSç´¢å¼•æ§‹ç¯‰ãƒ»è¿‘å‚æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self,
                 index_type: str = "ivf_pq",
                 metric: str = "cosine",
                 normalize: bool = None,
                 use_gpu: Union[str, bool] = "auto",
                 nlist: int = None,
                 seed: int = 42):
        """
        åˆæœŸåŒ–
        
        Args:
            index_type: ç´¢å¼•ã‚¿ã‚¤ãƒ— ("ivf_pq", "hnsw_flat")
            metric: è·é›¢ãƒ¡ãƒˆãƒªãƒƒã‚¯ ("cosine", "l2")  
            normalize: L2æ­£è¦åŒ–ãƒ•ãƒ©ã‚°ï¼ˆNone=è‡ªå‹•ã€cosineãªã‚‰ Trueï¼‰
            use_gpu: GPUä½¿ç”¨ ("auto", True, False)
            nlist: IVFã‚¯ãƒ©ã‚¹ã‚¿æ•° (None=è‡ªå‹•æœ€é©åŒ–)
            seed: ä¹±æ•°ã‚·ãƒ¼ãƒ‰
        """
        self.index_type = index_type
        self.metric = metric
        self.nlist = nlist  # å¾Œã§è‡ªå‹•è£œæ­£ã®å¯èƒ½æ€§ã‚ã‚Š
        
        # æ­£è¦åŒ–è¨­å®š: cosineãªã‚‰è‡ªå‹•ã§Trueã€ãã‚Œä»¥å¤–ã¯Falseã¾ãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®š
        if normalize is None:
            self.normalize = (metric == "cosine")
        else:
            self.normalize = normalize
            
        self.seed = seed
        
        # å†ç¾æ€§ç¢ºä¿
        self._set_seed(seed)
        
        # GPUè¨­å®š
        if use_gpu == "auto":
            self.use_gpu = faiss.get_num_gpus() > 0
        else:
            self.use_gpu = bool(use_gpu)
            
        # GPUåˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
        if self.use_gpu and faiss.get_num_gpus() == 0:
            logger.warning("GPUæŒ‡å®šã•ã‚Œã¾ã—ãŸãŒFAISS GPUãŒåˆ©ç”¨ä¸å¯ã€CPUã§å®Ÿè¡Œ")
            self.use_gpu = False
            
        logger.info(f"ğŸš€ FAISSIndexBuilderåˆæœŸåŒ–")
        logger.info(f"   ç´¢å¼•ã‚¿ã‚¤ãƒ—: {index_type}")
        logger.info(f"   è·é›¢ãƒ¡ãƒˆãƒªãƒƒã‚¯: {metric}")
        logger.info(f"   L2æ­£è¦åŒ–: {self.normalize}")
        logger.info(f"   GPUä½¿ç”¨: {self.use_gpu} (åˆ©ç”¨å¯èƒ½GPUæ•°: {faiss.get_num_gpus()})")
        
        # å†…éƒ¨çŠ¶æ…‹
        self.index = None
        self.ids = None
        self.embeddings = None
        self.dim = None
        
    def _set_seed(self, seed: int):
        """ä¹±æ•°ã‚·ãƒ¼ãƒ‰å›ºå®š"""
        random.seed(seed)
        np.random.seed(seed)
    
    @staticmethod
    def _round_to_multiple(x, base=8):
        """æ•°å€¤ã‚’æŒ‡å®šã®å€æ•°ã«ä¸¸ã‚ã‚‹"""
        return int(base * max(1, round(x / base)))
    
    def _choose_nlist(self, n_samples: int) -> int:
        """
        IVF nlist ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è‡ªå‹•æœ€é©åŒ–
        
        Args:
            n_samples: ã‚µãƒ³ãƒ—ãƒ«æ•°
            
        Returns:
            æœ€é©åŒ–ã•ã‚ŒãŸnlistå€¤
        """
        # ãƒ¦ãƒ¼ã‚¶æŒ‡å®šãŒå¦¥å½“ãªã‚‰ãã®ã¾ã¾ä½¿ã†
        if isinstance(self.nlist, int) and 64 <= self.nlist <= 16384:
            return self.nlist
        
        # è‡ªå‹•æ¨å¥¨: 4*sqrt(N) ã‚’ 8ã®å€æ•°ã¸ä¸¸ã‚ã€å¢ƒç•Œã‚’ã‚¯ãƒªãƒƒãƒ—
        import math
        suggested = self._round_to_multiple(4.0 * math.sqrt(max(1, n_samples)), 8)
        chosen = int(min(16384, max(64, suggested)))
        
        if self.nlist is None:
            logger.info(f"IVF nlist auto-selected: N={n_samples:,}, suggested={suggested}, used={chosen}")
        else:
            logger.info(f"IVF nlist auto-corrected: user nlist={self.nlist} is out of range, used={chosen}")
        
        return chosen
        
    def load_embeddings(self, 
                       emb_npy: Optional[str] = None,
                       ids_json: Optional[str] = None,
                       emb_parquet: Optional[str] = None) -> Tuple[np.ndarray, List[str]]:
        """
        åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        
        Args:
            emb_npy: NPYãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            ids_json: IDãƒªã‚¹ãƒˆJSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            emb_parquet: Parquetãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆåˆ—: id, embeddingï¼‰
            
        Returns:
            (åŸ‹ã‚è¾¼ã¿é…åˆ—, IDãƒªã‚¹ãƒˆ)
        """
        logger.info("ğŸ“‚ åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹")
        
        if emb_parquet:
            # Parquetå½¢å¼èª­ã¿è¾¼ã¿
            logger.info(f"   Parquetå½¢å¼: {emb_parquet}")
            try:
                if not Path(emb_parquet).exists():
                    raise FileNotFoundError(f"Parquetãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {emb_parquet}")
                    
                df = pd.read_parquet(emb_parquet)
                
                # å¿…è¦åˆ—ãƒã‚§ãƒƒã‚¯
                required_cols = ['id', 'embedding']
                missing_cols = [col for col in required_cols if col not in df.columns]
                if missing_cols:
                    raise ValueError(f"å¿…è¦ãªåˆ—ãŒä¸è¶³: {missing_cols}, åˆ©ç”¨å¯èƒ½: {list(df.columns)}")
                
                # IDãƒªã‚¹ãƒˆæŠ½å‡º
                ids = [str(id_val) for id_val in df['id'].tolist()]
                
                # åŸ‹ã‚è¾¼ã¿é…åˆ—å¤‰æ›
                embeddings_list = df['embedding'].tolist()
                
                # list[float] â†’ numpyé…åˆ—å¤‰æ›
                try:
                    embeddings = np.array(embeddings_list, dtype=np.float32)
                except (ValueError, TypeError) as e:
                    # å…¥ã‚Œå­ãƒªã‚¹ãƒˆå‡¦ç†
                    embeddings = np.vstack([np.array(emb, dtype=np.float32) for emb in embeddings_list])
                
                logger.info(f"âœ… Parquetèª­ã¿è¾¼ã¿å®Œäº†: {len(ids):,} åŸ‹ã‚è¾¼ã¿")
                
            except Exception as e:
                raise RuntimeError(f"âŒ CRITICAL: Parquetèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
                
        elif emb_npy and ids_json:
            # NPY + JSONå½¢å¼èª­ã¿è¾¼ã¿
            logger.info(f"   NPY + JSONå½¢å¼: {emb_npy}, {ids_json}")
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
                if not Path(emb_npy).exists():
                    raise FileNotFoundError(f"NPYãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {emb_npy}")
                if not Path(ids_json).exists():
                    raise FileNotFoundError(f"JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {ids_json}")
                
                # åŸ‹ã‚è¾¼ã¿èª­ã¿è¾¼ã¿
                embeddings = np.load(emb_npy).astype(np.float32)
                
                # IDèª­ã¿è¾¼ã¿
                with open(ids_json, 'r', encoding='utf-8') as f:
                    ids_raw = json.load(f)
                ids = [str(id_val) for id_val in ids_raw]
                
                logger.info(f"âœ… NPY+JSONèª­ã¿è¾¼ã¿å®Œäº†: {len(ids):,} åŸ‹ã‚è¾¼ã¿")
                
            except Exception as e:
                raise RuntimeError(f"âŒ CRITICAL: NPY+JSONèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
                
        else:
            raise ValueError(
                "âŒ CRITICAL: --emb_parquet ã¾ãŸã¯ (--emb_npy + --ids_json) ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„"
            )
            
        # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        if len(ids) != len(embeddings):
            raise RuntimeError(
                f"âŒ CRITICAL: IDæ•°({len(ids)})ã¨åŸ‹ã‚è¾¼ã¿æ•°({len(embeddings)})ãŒä¸ä¸€è‡´"
            )
            
        # æ¬¡å…ƒãƒã‚§ãƒƒã‚¯
        if len(embeddings.shape) != 2:
            raise RuntimeError(
                f"âŒ CRITICAL: åŸ‹ã‚è¾¼ã¿å½¢çŠ¶ãŒä¸æ­£: {embeddings.shape} (æœŸå¾…: (N, dim))"
            )
            
        # æ¬¡å…ƒä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
        self.dim = embeddings.shape[1]
        if self.dim not in [384, 768, 1024, 1536]:  # ä¸€èˆ¬çš„ãªåŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
            logger.warning(f"âš ï¸ éæ¨™æº–çš„ãªåŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {self.dim}")
            
        self.embeddings = embeddings
        self.ids = ids
        
        # çµ±è¨ˆæƒ…å ±ãƒ­ã‚°å‡ºåŠ›ï¼ˆã‚²ãƒ¼ãƒˆè¨­è¨ˆç”¨ï¼‰
        self._log_embedding_statistics()
        
        # L2æ­£è¦åŒ–å‡¦ç†
        if self.normalize:
            logger.info("ğŸ”„ L2æ­£è¦åŒ–ãƒã‚§ãƒƒã‚¯ãƒ»å®Ÿè¡Œä¸­...")
            self._apply_l2_normalization()
        
        logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {len(ids):,} samples Ã— {self.dim} dims")
        return embeddings, ids
        
    def _log_embedding_statistics(self):
        """åŸ‹ã‚è¾¼ã¿çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆã‚²ãƒ¼ãƒˆè¨­è¨ˆç”¨ï¼‰"""
        if self.embeddings is None:
            return
            
        # åŸºæœ¬çµ±è¨ˆ
        emb_flat = self.embeddings.flatten()
        mean_val = np.mean(emb_flat)
        std_val = np.std(emb_flat)
        
        # ãƒãƒ«ãƒ çµ±è¨ˆ
        norms = np.linalg.norm(self.embeddings, axis=1)
        norm_mean = np.mean(norms)
        norm_std = np.std(norms)
        norm_min = np.min(norms)
        norm_max = np.max(norms)
        
        # é«˜æ¬¡çµ±è¨ˆï¼ˆã‚²ãƒ¼ãƒˆè¨­è¨ˆç”¨ï¼‰
        try:
            skewness = stats.skew(emb_flat)
            kurt = stats.kurtosis(emb_flat)
        except:
            skewness, kurt = 0.0, 0.0
            
        logger.info(f"ğŸ“Š åŸ‹ã‚è¾¼ã¿çµ±è¨ˆæƒ…å ±ï¼ˆã‚²ãƒ¼ãƒˆè¨­è¨ˆç”¨ï¼‰:")
        logger.info(f"   å€¤çµ±è¨ˆ: mean={mean_val:.6f}, std={std_val:.6f}")
        logger.info(f"   ãƒãƒ«ãƒ : mean={norm_mean:.6f}, std={norm_std:.6f}, min={norm_min:.6f}, max={norm_max:.6f}")
        logger.info(f"   åˆ†å¸ƒå½¢çŠ¶: skewness={skewness:.6f}, kurtosis={kurt:.6f}")
        
    def _apply_l2_normalization(self):
        """L2æ­£è¦åŒ–å®Ÿè¡Œï¼ˆé‡è¤‡æ­£è¦åŒ–ã‚’å›é¿ï¼‰"""
        # æ—¢ã«æ­£è¦åŒ–æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
        norms = np.linalg.norm(self.embeddings, axis=1)
        is_already_normalized = np.allclose(norms, 1.0, rtol=1e-3, atol=1e-4)
        
        if is_already_normalized:
            logger.info("âœ… åŸ‹ã‚è¾¼ã¿ã¯æ—¢ã«L2æ­£è¦åŒ–æ¸ˆã¿ï¼ˆé‡è¤‡æ­£è¦åŒ–ã‚’å›é¿ï¼‰")
            return
            
        # L2æ­£è¦åŒ–å®Ÿè¡Œ
        logger.info("   L2æ­£è¦åŒ–ã‚’å®Ÿè¡Œä¸­...")
        faiss.normalize_L2(self.embeddings)
        
        # æ­£è¦åŒ–å¾Œç¢ºèª
        norms_after = np.linalg.norm(self.embeddings, axis=1)
        logger.info(f"âœ… L2æ­£è¦åŒ–å®Œäº†: ãƒãƒ«ãƒ ç¯„å›² [{norms_after.min():.6f}, {norms_after.max():.6f}]")
        
    def build_index(self,
                   pq_m: int = 16,
                   pq_bits: int = 8,
                   hnsw_m: int = 32,
                   efc: int = 200,
                   train_size: int = 200000,
                   add_batch_size: int = 10000) -> faiss.Index:
        """
        FAISSç´¢å¼•æ§‹ç¯‰
        
        Args:
            pq_m: PQåˆ†å‰²æ•°
            pq_bits: PQãƒ“ãƒƒãƒˆæ•°
            hnsw_m: HNSWæ¥ç¶šæ•°
            efc: HNSWæ§‹ç¯‰efConstruction
            train_size: è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸Šé™
            add_batch_size: è¿½åŠ ãƒãƒƒãƒã‚µã‚¤ã‚º
            
        Returns:
            æ§‹ç¯‰æ¸ˆã¿FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        """
        if self.embeddings is None:
            raise RuntimeError("âŒ CRITICAL: åŸ‹ã‚è¾¼ã¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            
        logger.info(f"ğŸ”§ FAISSç´¢å¼•æ§‹ç¯‰é–‹å§‹: {self.index_type}")
        start_time = time.time()
        
        # ç´¢å¼•ä½œæˆ
        if self.index_type == "ivf_pq":
            # IVF+PQç´¢å¼•
            nlist = self._choose_nlist(len(self.embeddings))
            logger.info(f"   IVFãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: nlist={nlist}, pq_m={pq_m}, pq_bits={pq_bits}")
            
            # æ¬¡å…ƒãƒã‚§ãƒƒã‚¯ï¼ˆPQåˆ†å‰²å¯èƒ½æ€§ï¼‰
            if self.dim % pq_m != 0:
                logger.warning(f"âš ï¸ æ¬¡å…ƒ({self.dim})ãŒPQåˆ†å‰²æ•°({pq_m})ã§å‰²ã‚Šåˆ‡ã‚Œã¾ã›ã‚“ã€èª¿æ•´ã‚’æ¨å¥¨")
                
            # quantizerä½œæˆï¼ˆcosine/L2å…±ã«L2è·é›¢ã§è¿‘ä¼¼å¯èƒ½ï¼‰
            quantizer = faiss.IndexFlatL2(self.dim)
            index = faiss.IndexIVFPQ(quantizer, self.dim, nlist, pq_m, pq_bits)
            
        elif self.index_type == "hnsw_flat":
            # HNSWç´¢å¼•
            logger.info(f"   HNSWãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: M={hnsw_m}, efConstruction={efc}")
            
            # HNSWã¯L2è·é›¢ãƒ™ãƒ¼ã‚¹
            index = faiss.IndexHNSWFlat(self.dim, hnsw_m)
            index.hnsw.efConstruction = efc
            
        else:
            raise ValueError(f"âŒ æœªå¯¾å¿œç´¢å¼•ã‚¿ã‚¤ãƒ—: {self.index_type}")
            
        # GPUåŒ–
        if self.use_gpu:
            try:
                logger.info("ğŸš€ GPUç´¢å¼•ã«å¤‰æ›ä¸­...")
                # GPUåŒ–å‰ã®ç´¢å¼•ã‚¿ã‚¤ãƒ—ã‚’ä¿å­˜
                cpu_index = index
                index = faiss.index_cpu_to_all_gpus(index)
                logger.info("âœ… GPUç´¢å¼•åŒ–å®Œäº†")
            except Exception as e:
                logger.warning(f"âš ï¸ GPUåŒ–å¤±æ•—ã€CPUã§ç¶šè¡Œ: {e}")
                self.use_gpu = False
                index = cpu_index
                
        # è¨“ç·´ï¼ˆIVF+PQã®ã¿ï¼‰
        if self.index_type == "ivf_pq":
            logger.info("ğŸ“ IVF+PQç´¢å¼•è¨“ç·´é–‹å§‹")
            
            # è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æº–å‚™
            n_samples = len(self.embeddings)
            actual_train_size = min(train_size, n_samples)
            
            if actual_train_size < n_samples:
                # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                train_indices = np.random.choice(n_samples, actual_train_size, replace=False)
                train_data = self.embeddings[train_indices]
                logger.info(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {actual_train_size:,} / {n_samples:,} samples (ãƒ©ãƒ³ãƒ€ãƒ æŠ½å‡º)")
            else:
                train_data = self.embeddings
                logger.info(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {actual_train_size:,} samples (å…¨ãƒ‡ãƒ¼ã‚¿)")
                
            # è¨“ç·´å®Ÿè¡Œ
            logger.info("   è¨“ç·´å®Ÿè¡Œä¸­...")
            try:
                index.train(train_data)
                logger.info("âœ… IVF+PQè¨“ç·´å®Œäº†")
            except Exception as e:
                raise RuntimeError(f"âŒ CRITICAL: IVF+PQè¨“ç·´å¤±æ•—: {e}")
                
        # ãƒ‡ãƒ¼ã‚¿è¿½åŠ ï¼ˆãƒãƒƒãƒå‡¦ç†ã§OOMå›é¿ï¼‰
        logger.info(f"ğŸ“¥ ç´¢å¼•ã¸ã®ãƒ‡ãƒ¼ã‚¿è¿½åŠ é–‹å§‹ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚º: {add_batch_size:,}ï¼‰")
        
        with tqdm(total=len(self.embeddings), desc="ãƒ‡ãƒ¼ã‚¿è¿½åŠ ", unit="vectors") as pbar:
            for i in range(0, len(self.embeddings), add_batch_size):
                end_i = min(i + add_batch_size, len(self.embeddings))
                batch_data = self.embeddings[i:end_i]
                
                try:
                    index.add(batch_data)
                    pbar.update(len(batch_data))
                    
                    # GPUãƒ¡ãƒ¢ãƒªç®¡ç†
                    if self.use_gpu and hasattr(faiss, 'gpu'):
                        faiss.gpu.synchronize_all_devices()
                        
                except Exception as e:
                    raise RuntimeError(f"âŒ CRITICAL: ãƒãƒƒãƒ {i//add_batch_size + 1} è¿½åŠ å¤±æ•—: {e}")
                    
        elapsed_time = time.time() - start_time
        vectors_per_sec = len(self.embeddings) / elapsed_time
        
        logger.info(f"âœ… ç´¢å¼•æ§‹ç¯‰å®Œäº†:")
        logger.info(f"   å‡¦ç†æ™‚é–“: {elapsed_time:.2f}s")
        logger.info(f"   å‡¦ç†é€Ÿåº¦: {vectors_per_sec:.1f} vectors/sec")
        logger.info(f"   ç´¢å¼•ã‚µã‚¤ã‚º: {index.ntotal:,} vectors")
        
        self.index = index
        return index
        
    def search_neighbors(self,
                        topk: int = 10,
                        batch_size: int = 8192,
                        efs: int = 128) -> Tuple[np.ndarray, np.ndarray]:
        """
        Top-kè¿‘å‚æ¤œç´¢ï¼ˆå…¨ãƒ™ã‚¯ãƒˆãƒ«å¯¾è±¡ï¼‰
        
        Args:
            topk: è¿‘å‚æ•°
            batch_size: æ¤œç´¢ãƒãƒƒãƒã‚µã‚¤ã‚º
            efs: HNSWæ¤œç´¢efSearch
            
        Returns:
            (è·é›¢é…åˆ—, ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é…åˆ—) - shape: (n_queries, topk)
        """
        if self.index is None or self.embeddings is None:
            raise RuntimeError("âŒ CRITICAL: ç´¢å¼•ã¾ãŸã¯åŸ‹ã‚è¾¼ã¿ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
        logger.info(f"ğŸ” Top-{topk} è¿‘å‚æ¤œç´¢é–‹å§‹")
        logger.info(f"   ã‚¯ã‚¨ãƒªæ•°: {len(self.embeddings):,}")
        logger.info(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size:,}")
        
        # HNSWæ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        if self.index_type == "hnsw_flat":
            try:
                if hasattr(self.index, 'hnsw'):
                    self.index.hnsw.efSearch = efs
                elif hasattr(self.index, 'index') and hasattr(self.index.index, 'hnsw'):
                    # GPUç‰ˆã®å ´åˆ
                    self.index.index.hnsw.efSearch = efs
                logger.info(f"   HNSW efSearch: {efs}")
            except Exception as e:
                logger.warning(f"âš ï¸ HNSW efSearchè¨­å®šå¤±æ•—: {e}")
                
        start_time = time.time()
        
        # ãƒãƒƒãƒæ¤œç´¢ã§OOMå›é¿
        all_distances = []
        all_indices = []
        
        with tqdm(total=len(self.embeddings), desc="è¿‘å‚æ¤œç´¢", unit="queries") as pbar:
            for i in range(0, len(self.embeddings), batch_size):
                end_i = min(i + batch_size, len(self.embeddings))
                batch_queries = self.embeddings[i:end_i]
                
                try:
                    # ãƒãƒƒãƒæ¤œç´¢å®Ÿè¡Œ
                    batch_distances, batch_indices = self.index.search(batch_queries, topk)
                    
                    all_distances.append(batch_distances)
                    all_indices.append(batch_indices)
                    
                    pbar.update(len(batch_queries))
                    
                    # GPU ãƒ¡ãƒ¢ãƒªåŒæœŸ
                    if self.use_gpu and hasattr(faiss, 'gpu'):
                        faiss.gpu.synchronize_all_devices()
                        
                except Exception as e:
                    raise RuntimeError(f"âŒ CRITICAL: ãƒãƒƒãƒ {i//batch_size + 1} æ¤œç´¢å¤±æ•—: {e}")
                    
        # çµæœçµåˆ
        distances = np.vstack(all_distances)
        indices = np.vstack(all_indices)
        
        elapsed_time = time.time() - start_time
        queries_per_sec = len(self.embeddings) / elapsed_time
        
        logger.info(f"âœ… è¿‘å‚æ¤œç´¢å®Œäº†:")
        logger.info(f"   å‡¦ç†æ™‚é–“: {elapsed_time:.2f}s")
        logger.info(f"   æ¤œç´¢é€Ÿåº¦: {queries_per_sec:.1f} queries/sec")
        logger.info(f"   çµæœå½¢çŠ¶: {distances.shape}")
        
        return distances, indices
        
    def convert_to_similarities(self, distances: np.ndarray) -> np.ndarray:
        """
        è·é›¢ã‚’é¡ä¼¼åº¦ã«å¤‰æ›ï¼ˆcosineãƒ¡ãƒˆãƒªãƒƒã‚¯ç”¨ï¼‰
        
        Args:
            distances: FAISS L2è·é›¢é…åˆ—
            
        Returns:
            é¡ä¼¼åº¦é…åˆ— (cosineæ™‚ã¯å¤‰æ›ã€l2æ™‚ã¯ãã®ã¾ã¾)
        """
        if self.metric != "cosine":
            # L2è·é›¢ã¯ãã®ã¾ã¾è¿”å´ï¼ˆè·é›¢ã¨ã—ã¦ä½¿ç”¨ï¼‰
            return distances
            
        # L2è·é›¢ â†’ cosineé¡ä¼¼åº¦å¤‰æ›ï¼ˆå®‰å…¨ã‚¯ãƒ©ãƒ³ãƒ—é©ç”¨ï¼‰
        # æ­£è¦åŒ–æ¸ˆã¿ãƒ™ã‚¯ãƒˆãƒ«ãªã‚‰ L2è·é›¢ dâˆˆ[0,2]ã€å¤‰æ›: sim = 1 - 0.5 * d^2
        with np.errstate(invalid="ignore", over="ignore", under="ignore"):
            distances = np.clip(distances, 0.0, 2.0, out=distances)
            similarities = 1.0 - 0.5 * (distances * distances)
        
        similarities = np.clip(similarities, -1.0, 1.0)  # [-1, 1]ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—
        
        return similarities
        
    def create_graph_edges(self,
                          distances: np.ndarray,
                          indices: np.ndarray,
                          undirected: bool = False,
                          remove_self_loops: bool = True) -> pd.DataFrame:
        """
        è¿‘å‚æ¤œç´¢çµæœã‹ã‚‰ç–ã‚°ãƒ©ãƒ•ã‚¨ãƒƒã‚¸ãƒªã‚¹ãƒˆç”Ÿæˆ
        
        Args:
            distances: è·é›¢é…åˆ— (n_queries, topk)
            indices: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é…åˆ— (n_queries, topk)
            undirected: ç„¡å‘ã‚°ãƒ©ãƒ•ã«ã™ã‚‹ã‹
            remove_self_loops: è‡ªå·±ãƒ«ãƒ¼ãƒ—é™¤å»ã™ã‚‹ã‹
            
        Returns:
            ã‚¨ãƒƒã‚¸ãƒªã‚¹ãƒˆDataFrame (src_id, dst_id, score)
        """
        logger.info("ğŸ•¸ï¸ ç–ã‚°ãƒ©ãƒ•ã‚¨ãƒƒã‚¸ç”Ÿæˆé–‹å§‹")
        logger.info(f"   å…¥åŠ›å½¢çŠ¶: {distances.shape}")
        logger.info(f"   ç„¡å‘ã‚°ãƒ©ãƒ•: {undirected}")
        logger.info(f"   è‡ªå·±ãƒ«ãƒ¼ãƒ—é™¤å»: {remove_self_loops}")
        
        # è·é›¢â†’é¡ä¼¼åº¦å¤‰æ›ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
        scores = self.convert_to_similarities(distances)
        
        # ã‚¨ãƒƒã‚¸ãƒªã‚¹ãƒˆæ§‹ç¯‰
        edges = []
        n_queries, topk = indices.shape
        
        logger.info("   ã‚¨ãƒƒã‚¸ãƒªã‚¹ãƒˆæ§‹ç¯‰ä¸­...")
        for i in tqdm(range(n_queries), desc="ã‚¨ãƒƒã‚¸ç”Ÿæˆ", unit="nodes", leave=False):
            src_id = self.ids[i]
            
            for j in range(topk):
                dst_idx = indices[i, j]
                
                # ç„¡åŠ¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆFAISS ãŒ -1 ã‚’è¿”ã™ã“ã¨ãŒã‚ã‚‹ï¼‰
                if dst_idx < 0 or dst_idx >= len(self.ids):
                    continue
                    
                dst_id = self.ids[dst_idx]
                score = float(scores[i, j])
                
                # è‡ªå·±ãƒ«ãƒ¼ãƒ—é™¤å»
                if remove_self_loops and src_id == dst_id:
                    continue
                    
                edges.append({
                    'src_id': src_id,
                    'dst_id': dst_id,
                    'score': score
                })
                
        # DataFrameä½œæˆ
        if not edges:
            logger.warning("âš ï¸ ã‚¨ãƒƒã‚¸ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            return pd.DataFrame(columns=['src_id', 'dst_id', 'score'])
            
        df_edges = pd.DataFrame(edges)
        logger.info(f"   åˆæœŸã‚¨ãƒƒã‚¸æ•°: {len(df_edges):,}")
        
        # ç„¡å‘ã‚°ãƒ©ãƒ•åŒ–
        if undirected:
            logger.info("   ç„¡å‘ã‚°ãƒ©ãƒ•åŒ–å®Ÿè¡Œä¸­...")
            
            # é€†å‘ãã‚¨ãƒƒã‚¸ç”Ÿæˆ
            reverse_edges = df_edges.copy()
            reverse_edges = reverse_edges.rename(columns={'src_id': 'dst_id', 'dst_id': 'src_id'})
            
            # çµåˆãƒ»é‡è¤‡é™¤å»
            df_combined = pd.concat([df_edges, reverse_edges], ignore_index=True)
            df_edges = df_combined.drop_duplicates(subset=['src_id', 'dst_id'], keep='first')
            
            logger.info(f"   ç„¡å‘ã‚°ãƒ©ãƒ•åŒ–å¾Œã‚¨ãƒƒã‚¸æ•°: {len(df_edges):,}")
            
        # ã‚°ãƒ©ãƒ•çµ±è¨ˆæƒ…å ±
        unique_nodes = len(set(df_edges['src_id'].tolist() + df_edges['dst_id'].tolist()))
        avg_degree = len(df_edges) / unique_nodes if unique_nodes > 0 else 0
        score_stats = df_edges['score'].describe()
        
        logger.info(f"âœ… ã‚°ãƒ©ãƒ•ã‚¨ãƒƒã‚¸ç”Ÿæˆå®Œäº†:")
        logger.info(f"   ã‚¨ãƒƒã‚¸æ•°: {len(df_edges):,}")
        logger.info(f"   ãƒãƒ¼ãƒ‰æ•°: {unique_nodes:,}")
        logger.info(f"   å¹³å‡æ¬¡æ•°: {avg_degree:.2f}")
        logger.info(f"   ã‚¹ã‚³ã‚¢çµ±è¨ˆ: min={score_stats['min']:.6f}, max={score_stats['max']:.6f}, mean={score_stats['mean']:.6f}")
        
        return df_edges
        
    def save_results(self,
                    distances: np.ndarray,
                    indices: np.ndarray,
                    df_edges: pd.DataFrame,
                    output_dir: str,
                    **index_params) -> Dict[str, str]:
        """
        çµæœä¿å­˜
        
        Args:
            distances: è·é›¢é…åˆ—
            indices: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é…åˆ—
            df_edges: ã‚¨ãƒƒã‚¸DataFrame
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            **index_params: ç´¢å¼•ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰
            
        Returns:
            ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è¾æ›¸
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ’¾ çµæœä¿å­˜é–‹å§‹: {output_dir}")
        saved_files = {}
        
        # 1. FAISSç´¢å¼•ä¿å­˜
        try:
            index_path = output_path / "index.faiss"
            
            # GPUç´¢å¼•ã‚’CPUã«æˆ»ã—ã¦ã‹ã‚‰ä¿å­˜
            if self.use_gpu:
                cpu_index = faiss.index_gpu_to_cpu(self.index)
                faiss.write_index(cpu_index, str(index_path))
            else:
                faiss.write_index(self.index, str(index_path))
                
            saved_files['index'] = str(index_path)
            logger.info(f"âœ… FAISSç´¢å¼•ä¿å­˜å®Œäº†: {index_path}")
            
        except Exception as e:
            logger.error(f"âŒ FAISSç´¢å¼•ä¿å­˜å¤±æ•—: {e}")
            
        # 2. è¿‘å‚çµæœParquetä¿å­˜ï¼ˆé•·ã„å½¢å¼ï¼‰
        try:
            neighbors_data = []
            scores = self.convert_to_similarities(distances)
            
            for i in range(len(indices)):
                src_id = self.ids[i]
                for j, (dst_idx, score, distance) in enumerate(zip(indices[i], scores[i], distances[i])):
                    if dst_idx >= 0 and dst_idx < len(self.ids):
                        neighbors_data.append({
                            'src_id': src_id,
                            'dst_id': self.ids[dst_idx],
                            'rank': j + 1,
                            'score': float(score),
                            'distance': float(distance)
                        })
                        
            df_neighbors = pd.DataFrame(neighbors_data)
            neighbors_path = output_path / "neighbors.parquet"
            df_neighbors.to_parquet(neighbors_path, index=False)
            saved_files['neighbors'] = str(neighbors_path)
            logger.info(f"âœ… è¿‘å‚çµæœä¿å­˜å®Œäº†: {neighbors_path}")
            
        except Exception as e:
            logger.error(f"âŒ è¿‘å‚çµæœä¿å­˜å¤±æ•—: {e}")
            
        # 3. ã‚°ãƒ©ãƒ•ã‚¨ãƒƒã‚¸ä¿å­˜ï¼ˆPPRç”¨ï¼‰
        try:
            edges_path = output_path / "graph_edges.parquet"
            df_edges.to_parquet(edges_path, index=False)
            saved_files['edges'] = str(edges_path)
            logger.info(f"âœ… ã‚°ãƒ©ãƒ•ã‚¨ãƒƒã‚¸ä¿å­˜å®Œäº†: {edges_path}")
            
        except Exception as e:
            logger.error(f"âŒ ã‚°ãƒ©ãƒ•ã‚¨ãƒƒã‚¸ä¿å­˜å¤±æ•—: {e}")
            
        # 4. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        try:
            metadata = {
                # ç´¢å¼•è¨­å®š
                'index_type': self.index_type,
                'metric': self.metric,
                'normalize': self.normalize,
                'use_gpu': self.use_gpu,
                
                # ãƒ‡ãƒ¼ã‚¿æƒ…å ±
                'embedding_dim': self.dim,
                'total_vectors': len(self.embeddings),
                'topk': distances.shape[1] if distances.size > 0 else 0,
                'total_edges': len(df_edges),
                
                # å®Ÿè¡Œè¨­å®š
                'seed': self.seed,
                'created_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                
                # ç´¢å¼•ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                **index_params
            }
            
            metadata_path = output_path / "meta.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            saved_files['metadata'] = str(metadata_path)
            logger.info(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {metadata_path}")
            
        except Exception as e:
            logger.error(f"âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å¤±æ•—: {e}")
            
        logger.info(f"ğŸ¯ ä¿å­˜å®Œäº†: {len(saved_files)} ãƒ•ã‚¡ã‚¤ãƒ«")
        return saved_files


def run_self_test():
    """å˜ä½“ãƒ†ã‚¹ãƒˆï¼ˆç–‘ä¼¼ãƒ‡ãƒ¼ã‚¿ã§å‹•ä½œæ¤œè¨¼ï¼‰"""
    logger.info("ğŸ§ª å˜ä½“ãƒ†ã‚¹ãƒˆå®Ÿè¡Œé–‹å§‹")
    
    try:
        # ç–‘ä¼¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        np.random.seed(42)
        n_samples = 1000
        dim = 128
        
        embeddings = np.random.randn(n_samples, dim).astype(np.float32)
        ids = [f"test_id_{i:04d}" for i in range(n_samples)]
        
        logger.info(f"   ç–‘ä¼¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: {n_samples} samples Ã— {dim} dims")
        
        # IVF+PQ ãƒ†ã‚¹ãƒˆ
        logger.info("   IVF+PQç´¢å¼•ãƒ†ã‚¹ãƒˆ...")
        builder_ivf = FAISSIndexBuilder(
            index_type="ivf_pq",
            metric="cosine",
            use_gpu=False,  # ãƒ†ã‚¹ãƒˆã¯CPUã§
            nlist=16,  # å°ã•ãªå€¤ã§ãƒ†ã‚¹ãƒˆ
            seed=42
        )
        
        builder_ivf.embeddings = embeddings
        builder_ivf.ids = ids
        builder_ivf.dim = dim
        
        if builder_ivf.normalize:
            faiss.normalize_L2(builder_ivf.embeddings)
            
        # å°è¦æ¨¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ç´¢å¼•æ§‹ç¯‰
        index_ivf = builder_ivf.build_index(
            pq_m=8,
            pq_bits=8,
            train_size=500,
            add_batch_size=200
        )
        
        # è¿‘å‚æ¤œç´¢
        distances_ivf, indices_ivf = builder_ivf.search_neighbors(topk=5, batch_size=100)
        
        # ã‚¨ãƒƒã‚¸ç”Ÿæˆ
        df_edges_ivf = builder_ivf.create_graph_edges(
            distances_ivf, indices_ivf, undirected=True, remove_self_loops=True
        )
        
        # HNSW ãƒ†ã‚¹ãƒˆ
        logger.info("   HNSWç´¢å¼•ãƒ†ã‚¹ãƒˆ...")
        builder_hnsw = FAISSIndexBuilder(
            index_type="hnsw_flat",
            metric="l2",
            use_gpu=False,
            nlist=None,  # HNSWã§ã¯ä½¿ã‚ãªã„
            seed=42
        )
        
        builder_hnsw.embeddings = embeddings.copy()
        builder_hnsw.ids = ids.copy()
        builder_hnsw.dim = dim
        
        index_hnsw = builder_hnsw.build_index(hnsw_m=16, efc=100, add_batch_size=200)
        distances_hnsw, indices_hnsw = builder_hnsw.search_neighbors(topk=5, batch_size=100)
        df_edges_hnsw = builder_hnsw.create_graph_edges(distances_hnsw, indices_hnsw)
        
        # çµæœæ¤œè¨¼
        assert distances_ivf.shape == (n_samples, 5), f"IVFè·é›¢å½¢çŠ¶ã‚¨ãƒ©ãƒ¼: {distances_ivf.shape}"
        assert indices_ivf.shape == (n_samples, 5), f"IVFã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å½¢çŠ¶ã‚¨ãƒ©ãƒ¼: {indices_ivf.shape}"
        assert len(df_edges_ivf) > 0, "IVFã‚¨ãƒƒã‚¸ç”Ÿæˆå¤±æ•—"
        
        assert distances_hnsw.shape == (n_samples, 5), f"HNSWè·é›¢å½¢çŠ¶ã‚¨ãƒ©ãƒ¼: {distances_hnsw.shape}"
        assert indices_hnsw.shape == (n_samples, 5), f"HNSWã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å½¢çŠ¶ã‚¨ãƒ©ãƒ¼: {indices_hnsw.shape}"
        assert len(df_edges_hnsw) > 0, "HNSWã‚¨ãƒƒã‚¸ç”Ÿæˆå¤±æ•—"
        
        logger.info("âœ… å˜ä½“ãƒ†ã‚¹ãƒˆæˆåŠŸ")
        logger.info(f"   IVFçµæœ: è·é›¢ç¯„å›² [{distances_ivf.min():.6f}, {distances_ivf.max():.6f}], ã‚¨ãƒƒã‚¸æ•° {len(df_edges_ivf):,}")
        logger.info(f"   HNSWçµæœ: è·é›¢ç¯„å›² [{distances_hnsw.min():.6f}, {distances_hnsw.max():.6f}], ã‚¨ãƒƒã‚¸æ•° {len(df_edges_hnsw):,}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ å˜ä½“ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    parser = argparse.ArgumentParser(
        description="GraphRAGå‹•çš„CFS-Chameleonå‘ã‘FAISSç´¢å¼•æ§‹ç¯‰ (Step-2)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
å®Ÿè¡Œä¾‹:
  # LaMP-2ãƒ¦ãƒ¼ã‚¶ãƒ¼ (cosine, IVF+PQ, GPU)
  python build_faiss_index.py \\
    --emb_npy ./embeddings/lamp2_user_embeddings.npy \\
    --ids_json ./embeddings/lamp2_user_embeddings.index.json \\
    --metric cosine --normalize \\
    --index_type ivf_pq --nlist 4096 --pq_m 16 --pq_bits 8 \\
    --train_size 200000 --topk 10 \\
    --out_dir ./faiss/lamp2_users --use_gpu \\
    --batch_size 8192 --seed 42

  # Tenrecãƒ¦ãƒ¼ã‚¶ãƒ¼ (l2, HNSW, CPU)
  python build_faiss_index.py \\
    --emb_parquet ./embeddings/tenrec_user_embeddings.parquet \\
    --metric l2 \\
    --index_type hnsw_flat --hnsw_m 32 --efc 200 --efs 128 \\
    --topk 20 \\
    --out_dir ./faiss/tenrec_users --batch_size 4096 --seed 42

  # Tenrecã‚¢ã‚¤ãƒ†ãƒ  (cosine, IVF+PQ, ç„¡å‘ã‚°ãƒ©ãƒ•)
  python build_faiss_index.py \\
    --emb_npy ./embeddings/tenrec_item_embeddings.npy \\
    --ids_json ./embeddings/tenrec_item_embeddings.index.json \\
    --metric cosine --normalize \\
    --index_type ivf_pq --nlist 8192 --pq_m 32 --pq_bits 8 \\
    --train_size 400000 --topk 15 --undirected \\
    --out_dir ./faiss/tenrec_items --use_gpu \\
    --batch_size 4096 --seed 42

  # å˜ä½“ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
  python build_faiss_index.py --self_test
        """
    )
    
    # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆæ’ä»–çš„ï¼‰
    input_group = parser.add_mutually_exclusive_group()
    input_group.add_argument('--emb_parquet', type=str,
                            help='ParquetåŸ‹ã‚è¾¼ã¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆåˆ—: id, embeddingï¼‰')
    
    # NPY+JSONå…¥åŠ›ã‚°ãƒ«ãƒ¼ãƒ—
    npy_group = parser.add_argument_group('NPY+JSONå…¥åŠ›')
    npy_group.add_argument('--emb_npy', type=str, help='NPYåŸ‹ã‚è¾¼ã¿ãƒ•ã‚¡ã‚¤ãƒ«')
    npy_group.add_argument('--ids_json', type=str, help='JSON IDãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«')
    
    # å¿…é ˆå¼•æ•°ï¼ˆãƒ†ã‚¹ãƒˆæ™‚ã¯ä¸è¦ï¼‰
    parser.add_argument('--out_dir', type=str, required='--self_test' not in sys.argv,
                        help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    # ç´¢å¼•è¨­å®š
    parser.add_argument('--metric', type=str, default='cosine', choices=['cosine', 'l2'],
                        help='è·é›¢ãƒ¡ãƒˆãƒªãƒƒã‚¯ (default: cosine)')
    parser.add_argument('--normalize', action='store_true',
                        help='æ˜ç¤ºçš„L2æ­£è¦åŒ–ãƒ•ãƒ©ã‚°ï¼ˆcosineã¯è‡ªå‹•ã§Trueï¼‰')
    parser.add_argument('--index_type', type=str, default='ivf_pq',
                        choices=['ivf_pq', 'hnsw_flat'],
                        help='ç´¢å¼•ã‚¿ã‚¤ãƒ— (default: ivf_pq)')
    
    # è¿‘å‚æ¤œç´¢
    parser.add_argument('--topk', type=int, default=10,
                        help='è¿‘å‚æ•° (default: 10)')
    parser.add_argument('--undirected', action='store_true',
                        help='ç„¡å‘ã‚°ãƒ©ãƒ•ã‚¨ãƒƒã‚¸ç”Ÿæˆ')
    
    # IVF+PQãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    ivf_group = parser.add_argument_group('IVF+PQè¨­å®š')
    ivf_group.add_argument('--nlist', type=int, default=None,
                          help='IVFã®ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã€‚æœªæŒ‡å®šãªã‚‰è‡ªå‹•æœ€é©åŒ–(â‰ˆ4*sqrt(N), 64ã€œ16384)')
    ivf_group.add_argument('--pq_m', type=int, default=16,
                          help='PQåˆ†å‰²æ•° (default: 16)')
    ivf_group.add_argument('--pq_bits', type=int, default=8,
                          help='PQãƒ“ãƒƒãƒˆæ•° (default: 8)')
    ivf_group.add_argument('--train_size', type=int, default=200000,
                          help='è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸Šé™ (default: 200000)')
    
    # HNSWãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    hnsw_group = parser.add_argument_group('HNSWè¨­å®š')
    hnsw_group.add_argument('--hnsw_m', type=int, default=32,
                           help='HNSWæ¥ç¶šæ•° (default: 32)')
    hnsw_group.add_argument('--efc', type=int, default=200,
                           help='HNSWæ§‹ç¯‰efConstruction (default: 200)')
    hnsw_group.add_argument('--efs', type=int, default=128,
                           help='HNSWæ¤œç´¢efSearch (default: 128)')
    
    # å®Ÿè¡Œç’°å¢ƒ
    parser.add_argument('--use_gpu', type=str, default='auto',
                        help='GPUä½¿ç”¨ (auto/true/false, default: auto)')
    parser.add_argument('--batch_size', type=int, default=8192,
                        help='æ¤œç´¢ãƒãƒƒãƒã‚µã‚¤ã‚º (default: 8192)')
    parser.add_argument('--add_batch_size', type=int, default=10000,
                        help='ç´¢å¼•è¿½åŠ ãƒãƒƒãƒã‚µã‚¤ã‚º (default: 10000)')
    parser.add_argument('--seed', type=int, default=42,
                        help='ä¹±æ•°ã‚·ãƒ¼ãƒ‰ (default: 42)')
    
    # ãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒãƒƒã‚°
    parser.add_argument('--self_test', action='store_true',
                        help='å˜ä½“ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ')
    parser.add_argument('--verbose', action='store_true',
                        help='è©³ç´°ãƒ­ã‚°è¡¨ç¤º')
    
    args = parser.parse_args()
    
    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«è¨­å®š
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        
    # å˜ä½“ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    if args.self_test:
        success = run_self_test()
        sys.exit(0 if success else 1)
        
    # å¼•æ•°æ¤œè¨¼
    if not args.emb_parquet and not (args.emb_npy and args.ids_json):
        parser.error("--emb_parquet ã¾ãŸã¯ (--emb_npy + --ids_json) ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        
    if not args.out_dir:
        parser.error("--out_dir ã¯å¿…é ˆã§ã™")
        
    try:
        # GPUè¨­å®šå¤‰æ›
        if args.use_gpu.lower() == 'auto':
            use_gpu = 'auto'
        elif args.use_gpu.lower() in ['true', '1', 'yes']:
            use_gpu = True
        else:
            use_gpu = False
            
        # FAISSç´¢å¼•ãƒ“ãƒ«ãƒ€ãƒ¼åˆæœŸåŒ–
        builder = FAISSIndexBuilder(
            index_type=args.index_type,
            metric=args.metric,
            normalize=args.normalize,  # Noneã®å ´åˆã€cosineãªã‚‰è‡ªå‹•ã§True
            use_gpu=use_gpu,
            nlist=args.nlist,
            seed=args.seed
        )
        
        # åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        embeddings, ids = builder.load_embeddings(
            emb_npy=args.emb_npy,
            ids_json=args.ids_json,
            emb_parquet=args.emb_parquet
        )
        
        # ç´¢å¼•æ§‹ç¯‰
        index = builder.build_index(
            pq_m=args.pq_m,
            pq_bits=args.pq_bits,
            hnsw_m=args.hnsw_m,
            efc=args.efc,
            train_size=args.train_size,
            add_batch_size=args.add_batch_size
        )
        
        # è¿‘å‚æ¤œç´¢
        distances, indices = builder.search_neighbors(
            topk=args.topk,
            batch_size=args.batch_size,
            efs=args.efs
        )
        
        # ã‚°ãƒ©ãƒ•ã‚¨ãƒƒã‚¸ç”Ÿæˆ
        df_edges = builder.create_graph_edges(
            distances=distances,
            indices=indices,
            undirected=args.undirected,
            remove_self_loops=True
        )
        
        # çµæœä¿å­˜
        saved_files = builder.save_results(
            distances=distances,
            indices=indices,
            df_edges=df_edges,
            output_dir=args.out_dir,
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            nlist=args.nlist,
            pq_m=args.pq_m,
            pq_bits=args.pq_bits,
            hnsw_m=args.hnsw_m,
            efc=args.efc,
            efs=args.efs,
            train_size=args.train_size,
            add_batch_size=args.add_batch_size,
            topk=args.topk,
            batch_size=args.batch_size,
            undirected=args.undirected
        )
        
        # å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ
        print(f"\nğŸ‰ FAISSç´¢å¼•æ§‹ç¯‰å®Œäº†!")
        print(f"ğŸ“Š å‡¦ç†çµ±è¨ˆ:")
        print(f"   ç´¢å¼•ã‚¿ã‚¤ãƒ—: {args.index_type}")
        print(f"   ãƒ¡ãƒˆãƒªãƒƒã‚¯: {args.metric}")
        print(f"   L2æ­£è¦åŒ–: {builder.normalize}")
        print(f"   ãƒ™ã‚¯ãƒˆãƒ«æ•°: {len(embeddings):,}")
        print(f"   æ¬¡å…ƒæ•°: {builder.dim}")
        print(f"   Top-k: {args.topk}")
        print(f"   ã‚¨ãƒƒã‚¸æ•°: {len(df_edges):,}")
        print(f"   GPUä½¿ç”¨: {builder.use_gpu}")
        print(f"   å‡ºåŠ›å…ˆ: {args.out_dir}")
        
        print(f"\nğŸ“ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«:")
        for file_type, file_path in saved_files.items():
            print(f"   {file_type}: {file_path}")
            
    except Exception as e:
        logger.error(f"âŒ å®Ÿè¡Œå¤±æ•—: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()