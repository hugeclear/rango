#!/usr/bin/env python3
"""
CFS-Chameleonæ„å‘³çš„é¡ä¼¼åº¦çµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
CollaborativeDirectionPoolã®æ”¹ä¿®ã¨æ„å‘³çš„é¡ä¼¼åº¦è¨ˆç®—ã®çµ±åˆ
"""

import numpy as np
import torch
from typing import List, Dict, Any, Optional, Union, Tuple
import logging
import time
import json
from pathlib import Path

# æ„å‘³çš„é¡ä¼¼åº¦ã‚¨ãƒ³ã‚¸ãƒ³
from semantic_similarity_engine import (
    SemanticSimilarityEngine,
    SemanticSimilarityConfig,
    compute_semantic_similarity_rich,
    compute_batch_semantic_similarity,
    HybridSimilarityCalculator
)

# CFS-Chameleoné–¢é€£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
try:
    from cfs_chameleon_extension import DirectionPiece, CollaborativeDirectionPool
    from chameleon_cfs_integrator import CollaborativeChameleonEditor
    CFS_AVAILABLE = True
except ImportError:
    print("âš ï¸ CFS modules not available. Using mock implementations.")
    CFS_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SemanticAwareDirectionPool:
    """æ„å‘³çš„é¡ä¼¼åº¦å¯¾å¿œã®æ–¹å‘ãƒ”ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«"""
    
    def __init__(self,
                 capacity: int = 1000,
                 rank: int = 32,
                 semantic_config: SemanticSimilarityConfig = None,
                 use_hybrid_similarity: bool = True,
                 similarity_threshold: float = 0.1):
        """
        åˆæœŸåŒ–
        
        Args:
            capacity: ãƒ—ãƒ¼ãƒ«å®¹é‡
            rank: SVDãƒ©ãƒ³ã‚¯
            semantic_config: æ„å‘³çš„é¡ä¼¼åº¦è¨­å®š
            use_hybrid_similarity: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰é¡ä¼¼åº¦ä½¿ç”¨ãƒ•ãƒ©ã‚°
            similarity_threshold: é¡ä¼¼åº¦é–¾å€¤
        """
        self.capacity = capacity
        self.rank = rank
        self.similarity_threshold = similarity_threshold
        self.use_hybrid_similarity = use_hybrid_similarity
        
        # ãƒ”ãƒ¼ã‚¹æ ¼ç´
        self.pieces: List[Any] = []
        self.user_mapping: Dict[str, List[int]] = {}
        
        # æ„å‘³çš„é¡ä¼¼åº¦ã‚¨ãƒ³ã‚¸ãƒ³
        self.semantic_config = semantic_config or SemanticSimilarityConfig()
        self.semantic_engine = SemanticSimilarityEngine(self.semantic_config)
        
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è¨ˆç®—å™¨
        if self.use_hybrid_similarity:
            self.hybrid_calculator = HybridSimilarityCalculator(
                self.semantic_engine,
                semantic_weight=0.8,
                vector_weight=0.2
            )
        else:
            self.hybrid_calculator = None
        
        logger.info("âœ… SemanticAwareDirectionPool initialized")
        logger.info(f"   Capacity: {capacity}, Rank: {rank}")
        logger.info(f"   Hybrid similarity: {use_hybrid_similarity}")
        logger.info(f"   Similarity threshold: {similarity_threshold}")
    
    def add_piece(self, piece: Any, user_id: str = None):
        """ãƒ”ãƒ¼ã‚¹ã‚’ãƒ—ãƒ¼ãƒ«ã«è¿½åŠ """
        if len(self.pieces) >= self.capacity:
            self._evict_least_used_piece()
        
        self.pieces.append(piece)
        piece_index = len(self.pieces) - 1
        
        if user_id:
            if user_id not in self.user_mapping:
                self.user_mapping[user_id] = []
            self.user_mapping[user_id].append(piece_index)
        
        logger.debug(f"Added piece to pool: index {piece_index}, user {user_id}")
    
    def _evict_least_used_piece(self):
        """æœ€ã‚‚ä½¿ç”¨é »åº¦ã®ä½ã„ãƒ”ãƒ¼ã‚¹ã‚’å‰Šé™¤"""
        if not self.pieces:
            return
        
        # ç°¡å˜ãªå®Ÿè£…: æœ€åˆã®ãƒ”ãƒ¼ã‚¹ã‚’å‰Šé™¤
        removed_piece = self.pieces.pop(0)
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ›´æ–°
        for user_id, indices in self.user_mapping.items():
            self.user_mapping[user_id] = [i-1 for i in indices if i > 0]
        
        logger.debug("Evicted least used piece from pool")
    
    def compute_context_similarity_semantic(self, 
                                          user_context: Union[str, np.ndarray],
                                          candidate_pieces: List[Any] = None) -> np.ndarray:
        """
        æ„å‘³çš„é¡ä¼¼åº¦ã«åŸºã¥ãã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦è¨ˆç®—
        
        Args:
            user_context: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¾ãŸã¯ãƒ™ã‚¯ãƒˆãƒ«ï¼‰
            candidate_pieces: å€™è£œãƒ”ãƒ¼ã‚¹ãƒªã‚¹ãƒˆï¼ˆNoneã®å ´åˆã¯å…¨ãƒ”ãƒ¼ã‚¹ï¼‰
            
        Returns:
            é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢é…åˆ—
        """
        if candidate_pieces is None:
            candidate_pieces = self.pieces
        
        if not candidate_pieces:
            return np.array([])
        
        logger.debug(f"Computing semantic context similarity for {len(candidate_pieces)} pieces")
        
        try:
            if self.use_hybrid_similarity and self.hybrid_calculator:
                # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰é¡ä¼¼åº¦è¨ˆç®—
                similarities = []
                for piece in candidate_pieces:
                    sim = self.hybrid_calculator.compute_hybrid_similarity(user_context, piece)
                    similarities.append(sim)
                
                similarity_scores = np.array(similarities)
            else:
                # ç´”ç²‹ãªæ„å‘³çš„é¡ä¼¼åº¦è¨ˆç®—
                similarity_scores = compute_batch_semantic_similarity(
                    [user_context], candidate_pieces, self.semantic_engine
                )[0]  # æœ€åˆã®è¡Œã®ã¿å–å¾—
            
            logger.debug(f"Similarity scores: min={similarity_scores.min():.4f}, max={similarity_scores.max():.4f}, avg={similarity_scores.mean():.4f}")
            
            return similarity_scores
            
        except Exception as e:
            logger.error(f"âŒ Semantic context similarity error: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ©ãƒ³ãƒ€ãƒ é¡ä¼¼åº¦
            return np.random.uniform(0.1, 0.5, len(candidate_pieces))
    
    def compute_semantic_similarity_legacy(self,
                                         user_context: Union[str, np.ndarray],
                                         candidate_pieces: List[Any] = None) -> np.ndarray:
        """
        å¾“æ¥æ‰‹æ³•ã¨ã®äº’æ›æ€§ã®ãŸã‚ã®æ„å‘³çš„é¡ä¼¼åº¦è¨ˆç®—
        
        Args:
            user_context: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            candidate_pieces: å€™è£œãƒ”ãƒ¼ã‚¹ãƒªã‚¹ãƒˆ
            
        Returns:
            é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢é…åˆ—
        """
        if candidate_pieces is None:
            candidate_pieces = self.pieces
        
        similarities = []
        for piece in candidate_pieces:
            try:
                sim = compute_semantic_similarity_rich(
                    user_context, piece, self.semantic_engine
                )
                similarities.append(sim)
            except Exception as e:
                logger.warning(f"Piece similarity error: {e}")
                similarities.append(0.1)
        
        return np.array(similarities)
    
    def select_collaborative_pieces_semantic(self,
                                           user_context: Union[str, np.ndarray],
                                           user_id: str = None,
                                           top_k: int = 5,
                                           diversity_weight: float = 0.2) -> List[Tuple[Any, float]]:
        """
        æ„å‘³çš„é¡ä¼¼åº¦ã«åŸºã¥ãå”èª¿ãƒ”ãƒ¼ã‚¹é¸æŠ
        
        Args:
            user_context: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            top_k: é¸æŠã™ã‚‹ãƒ”ãƒ¼ã‚¹æ•°
            diversity_weight: å¤šæ§˜æ€§é‡ã¿
            
        Returns:
            é¸æŠã•ã‚ŒãŸãƒ”ãƒ¼ã‚¹ã¨é¡ä¼¼åº¦ã®ãƒªã‚¹ãƒˆ
        """
        if not self.pieces:
            logger.warning("No pieces in pool for selection")
            return []
        
        logger.info(f"ğŸ” Selecting collaborative pieces with semantic similarity")
        logger.info(f"   Pool size: {len(self.pieces)}, Top-k: {top_k}")
        
        try:
            # é¡ä¼¼åº¦è¨ˆç®—
            similarity_scores = self.compute_context_similarity_semantic(user_context)
            
            # é–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            valid_indices = np.where(similarity_scores >= self.similarity_threshold)[0]
            
            if len(valid_indices) == 0:
                logger.warning("No pieces meet similarity threshold, using top candidates")
                valid_indices = np.argsort(similarity_scores)[-min(top_k*2, len(similarity_scores)):]
            
            # å¤šæ§˜æ€§ã‚’è€ƒæ…®ã—ãŸé¸æŠ
            selected_pieces = self._select_diverse_pieces(
                valid_indices, similarity_scores, top_k, diversity_weight
            )
            
            # çµæœæ§‹ç¯‰
            results = []
            for idx in selected_pieces:
                piece = self.pieces[idx]
                score = similarity_scores[idx]
                results.append((piece, float(score)))
            
            logger.info(f"âœ… Selected {len(results)} pieces with scores: {[f'{s:.3f}' for _, s in results]}")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ Semantic piece selection error: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ©ãƒ³ãƒ€ãƒ é¸æŠ
            selected_indices = np.random.choice(len(self.pieces), min(top_k, len(self.pieces)), replace=False)
            return [(self.pieces[i], 0.5) for i in selected_indices]
    
    def _select_diverse_pieces(self,
                             candidate_indices: np.ndarray,
                             similarity_scores: np.ndarray,
                             top_k: int,
                             diversity_weight: float) -> List[int]:
        """
        å¤šæ§˜æ€§ã‚’è€ƒæ…®ã—ãŸãƒ”ãƒ¼ã‚¹é¸æŠ
        
        Args:
            candidate_indices: å€™è£œãƒ”ãƒ¼ã‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            similarity_scores: é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢
            top_k: é¸æŠæ•°
            diversity_weight: å¤šæ§˜æ€§é‡ã¿
            
        Returns:
            é¸æŠã•ã‚ŒãŸãƒ”ãƒ¼ã‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        """
        if len(candidate_indices) <= top_k:
            return candidate_indices.tolist()
        
        selected_indices = []
        remaining_indices = candidate_indices.tolist()
        
        # æœ€é«˜é¡ä¼¼åº¦ã®ãƒ”ãƒ¼ã‚¹ã‚’æœ€åˆã«é¸æŠ
        best_idx = candidate_indices[np.argmax(similarity_scores[candidate_indices])]
        selected_indices.append(best_idx)
        remaining_indices.remove(best_idx)
        
        # æ®‹ã‚Šã‚’å¤šæ§˜æ€§ã‚’è€ƒæ…®ã—ã¦é¸æŠ
        for _ in range(min(top_k - 1, len(remaining_indices))):
            if not remaining_indices:
                break
            
            best_candidate = None
            best_score = -1
            
            for candidate_idx in remaining_indices:
                # é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢
                similarity_score = similarity_scores[candidate_idx]
                
                # å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢ï¼ˆæ—¢é¸æŠãƒ”ãƒ¼ã‚¹ã¨ã®éé¡ä¼¼åº¦ï¼‰
                diversity_score = self._compute_diversity_score(
                    candidate_idx, selected_indices
                )
                
                # ç·åˆã‚¹ã‚³ã‚¢
                combined_score = (1 - diversity_weight) * similarity_score + diversity_weight * diversity_score
                
                if combined_score > best_score:
                    best_score = combined_score
                    best_candidate = candidate_idx
            
            if best_candidate is not None:
                selected_indices.append(best_candidate)
                remaining_indices.remove(best_candidate)
        
        return selected_indices
    
    def _compute_diversity_score(self, candidate_idx: int, selected_indices: List[int]) -> float:
        """
        å€™è£œãƒ”ãƒ¼ã‚¹ã¨æ—¢é¸æŠãƒ”ãƒ¼ã‚¹ã¨ã®å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
        
        Args:
            candidate_idx: å€™è£œãƒ”ãƒ¼ã‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            selected_indices: æ—¢é¸æŠãƒ”ãƒ¼ã‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚¹ãƒˆ
            
        Returns:
            å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢ï¼ˆé«˜ã„ã»ã©å¤šæ§˜ï¼‰
        """
        if not selected_indices:
            return 1.0
        
        try:
            candidate_piece = self.pieces[candidate_idx]
            selected_pieces = [self.pieces[i] for i in selected_indices]
            
            # é¸æŠæ¸ˆã¿ãƒ”ãƒ¼ã‚¹ã¨ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
            similarities = []
            for selected_piece in selected_pieces:
                # ç°¡å˜ãªå®Ÿè£…: semantic_tagsã®é‡è¤‡åº¦ã§è©•ä¾¡
                candidate_tags = set()
                selected_tags = set()
                
                if hasattr(candidate_piece, 'semantic_tags'):
                    candidate_tags = set(candidate_piece.semantic_tags or [])
                elif isinstance(candidate_piece, dict) and 'semantic_tags' in candidate_piece:
                    candidate_tags = set(candidate_piece['semantic_tags'] or [])
                
                if hasattr(selected_piece, 'semantic_tags'):
                    selected_tags = set(selected_piece.semantic_tags or [])
                elif isinstance(selected_piece, dict) and 'semantic_tags' in selected_piece:
                    selected_tags = set(selected_piece['semantic_tags'] or [])
                
                if candidate_tags and selected_tags:
                    overlap = len(candidate_tags & selected_tags)
                    union = len(candidate_tags | selected_tags)
                    similarity = overlap / union if union > 0 else 0
                else:
                    similarity = 0.1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½é¡ä¼¼åº¦
                
                similarities.append(similarity)
            
            # å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢ = 1 - æœ€å¤§é¡ä¼¼åº¦
            diversity_score = 1.0 - max(similarities)
            return max(0.0, diversity_score)
            
        except Exception as e:
            logger.debug(f"Diversity score computation error: {e}")
            return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def get_pool_statistics(self) -> Dict[str, Any]:
        """ãƒ—ãƒ¼ãƒ«çµ±è¨ˆæƒ…å ±å–å¾—"""
        if not self.pieces:
            return {"total_pieces": 0, "users": 0}
        
        # semantic_tagsã®åˆ†æ
        all_tags = set()
        tag_counts = {}
        
        for piece in self.pieces:
            tags = []
            if hasattr(piece, 'semantic_tags') and piece.semantic_tags:
                tags = piece.semantic_tags
            elif isinstance(piece, dict) and 'semantic_tags' in piece:
                if isinstance(piece['semantic_tags'], list):
                    tags = piece['semantic_tags']
                else:
                    tags = [piece['semantic_tags']]
            
            for tag in tags:
                all_tags.add(tag)
                tag_counts[tag] = tag_counts.get(tag, 0) + 1
        
        return {
            "total_pieces": len(self.pieces),
            "users": len(self.user_mapping),
            "unique_semantic_tags": len(all_tags),
            "most_common_tags": sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:5],
            "cache_size": len(self.semantic_engine.embedding_cache.text_to_embedding),
            "similarity_threshold": self.similarity_threshold
        }

class SemanticCollaborativeChameleonEditor:
    """æ„å‘³çš„é¡ä¼¼åº¦å¯¾å¿œCFS-Chameleonã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼"""
    
    def __init__(self,
                 base_editor: Any = None,
                 semantic_config: SemanticSimilarityConfig = None,
                 pool_capacity: int = 1000):
        """
        åˆæœŸåŒ–
        
        Args:
            base_editor: ãƒ™ãƒ¼ã‚¹CFS-Chameleonã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼
            semantic_config: æ„å‘³çš„é¡ä¼¼åº¦è¨­å®š
            pool_capacity: ãƒ—ãƒ¼ãƒ«å®¹é‡
        """
        self.base_editor = base_editor
        
        # æ„å‘³çš„æ–¹å‘ãƒ—ãƒ¼ãƒ«ã‚’åˆæœŸåŒ–
        self.semantic_pool = SemanticAwareDirectionPool(
            capacity=pool_capacity,
            semantic_config=semantic_config,
            use_hybrid_similarity=True
        )
        
        logger.info("âœ… SemanticCollaborativeChameleonEditor initialized")
        logger.info(f"   Pool capacity: {pool_capacity}")
    
    def generate_with_semantic_collaboration(self,
                                           prompt: str,
                                           user_context: str = None,
                                           user_id: str = None,
                                           alpha_personal: float = 0.1,
                                           alpha_neutral: float = -0.05,
                                           max_length: int = 100) -> str:
        """
        æ„å‘³çš„å”èª¿æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆ
        
        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            user_context: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            alpha_personal: ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«æ–¹å‘å¼·åº¦
            alpha_neutral: ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘å¼·åº¦
            max_length: æœ€å¤§ç”Ÿæˆé•·
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        context_for_similarity = user_context or prompt
        
        # æ„å‘³çš„é¡ä¼¼åº¦ã«åŸºã¥ããƒ”ãƒ¼ã‚¹é¸æŠ
        selected_pieces = self.semantic_pool.select_collaborative_pieces_semantic(
            user_context=context_for_similarity,
            user_id=user_id,
            top_k=3
        )
        
        logger.info(f"ğŸ¦ Generating with semantic collaboration")
        logger.info(f"   Selected pieces: {len(selected_pieces)}")
        logger.info(f"   Similarity scores: {[f'{s:.3f}' for _, s in selected_pieces]}")
        
        if self.base_editor and hasattr(self.base_editor, 'generate_with_chameleon'):
            try:
                # ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ã§ã®ç”Ÿæˆ
                result = self.base_editor.generate_with_chameleon(
                    prompt=prompt,
                    alpha_personal=alpha_personal,
                    alpha_neutral=alpha_neutral,
                    max_length=max_length
                )
                return result
            except Exception as e:
                logger.error(f"Base editor generation error: {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆ
        return f"Semantic collaborative response to: {prompt[:50]}..."

def demonstrate_semantic_integration():
    """æ„å‘³çš„é¡ä¼¼åº¦çµ±åˆã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("ğŸ¦ CFS-Chameleonæ„å‘³çš„é¡ä¼¼åº¦çµ±åˆãƒ‡ãƒ¢")
    print("=" * 60)
    
    # æ„å‘³çš„ãƒ—ãƒ¼ãƒ«ã®åˆæœŸåŒ–
    semantic_config = SemanticSimilarityConfig(
        primary_model="sentence-transformers",
        cache_embeddings=True
    )
    
    pool = SemanticAwareDirectionPool(
        capacity=100,
        semantic_config=semantic_config,
        use_hybrid_similarity=True
    )
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ”ãƒ¼ã‚¹ã‚’è¿½åŠ 
    sample_pieces = [
        {"id": "sci_fi", "semantic_tags": ["science_fiction", "literature"], "u_component": np.random.randn(768)},
        {"id": "cooking", "semantic_tags": ["cooking", "recipes", "food"], "u_component": np.random.randn(768)},
        {"id": "movies", "semantic_tags": ["entertainment", "films", "cinema"], "u_component": np.random.randn(768)},
        {"id": "tech", "semantic_tags": ["programming", "technology"], "u_component": np.random.randn(768)},
        {"id": "travel", "semantic_tags": ["travel", "adventure", "exploration"], "u_component": np.random.randn(768)}
    ]
    
    for piece in sample_pieces:
        pool.add_piece(piece, f"user_{piece['id']}")
    
    print(f"ğŸ“¦ Added {len(sample_pieces)} pieces to semantic pool")
    
    # ãƒ—ãƒ¼ãƒ«çµ±è¨ˆè¡¨ç¤º
    stats = pool.get_pool_statistics()
    print(f"\nğŸ“Š Pool Statistics:")
    print(f"   Total pieces: {stats['total_pieces']}")
    print(f"   Unique semantic tags: {stats['unique_semantic_tags']}")
    print(f"   Most common tags: {stats['most_common_tags']}")
    
    # æ„å‘³çš„é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã®é¸æŠãƒ†ã‚¹ãƒˆ
    test_contexts = [
        "æœ€æ–°ã®SFå°èª¬ã«ã¤ã„ã¦è©±ã—ãŸã„",
        "ç¾å‘³ã—ã„æ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’æ•™ãˆã¦",
        "é¢ç™½ã„æ˜ ç”»ã‚’æ¨è–¦ã—ã¦ãã ã•ã„",
        "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®è³ªå•ãŒã‚ã‚Šã¾ã™"
    ]
    
    print(f"\nğŸ” Semantic-based piece selection test:")
    print("-" * 40)
    
    for context in test_contexts:
        print(f"\nğŸ“ Context: '{context}'")
        
        selected_pieces = pool.select_collaborative_pieces_semantic(
            user_context=context,
            top_k=3
        )
        
        print(f"   Selected pieces:")
        for i, (piece, score) in enumerate(selected_pieces):
            piece_id = piece.get('id', 'unknown')
            tags = ', '.join(piece.get('semantic_tags', []))
            print(f"     {i+1}. {piece_id} (score: {score:.4f}) - tags: {tags}")
    
    # å¾“æ¥æ‰‹æ³•ã¨ã®æ¯”è¼ƒ
    print(f"\nğŸ”„ Comparison with traditional method:")
    print("-" * 40)
    
    context = test_contexts[0]  # "æœ€æ–°ã®SFå°èª¬ã«ã¤ã„ã¦è©±ã—ãŸã„"
    
    # æ„å‘³çš„é¡ä¼¼åº¦
    semantic_selected = pool.select_collaborative_pieces_semantic(context, top_k=3)
    
    # å¾“æ¥çš„é¡ä¼¼åº¦ï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
    traditional_scores = np.random.uniform(0.1, 0.8, len(sample_pieces))
    traditional_selected = [(sample_pieces[i], traditional_scores[i]) 
                          for i in np.argsort(traditional_scores)[-3:]]
    
    print(f"Context: '{context}'")
    print(f"\nSemantic similarity results:")
    for piece, score in semantic_selected:
        print(f"   {piece['id']}: {score:.4f}")
    
    print(f"\nTraditional similarity results (mock):")
    for piece, score in traditional_selected:
        print(f"   {piece['id']}: {score:.4f}")
    
    # æ„å‘³çš„ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ã®ãƒ†ã‚¹ãƒˆ
    print(f"\nğŸ¯ Semantic collaborative editor test:")
    print("-" * 40)
    
    editor = SemanticCollaborativeChameleonEditor(
        semantic_config=semantic_config,
        pool_capacity=100
    )
    
    # ãƒ—ãƒ¼ãƒ«ã‚’ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ã«ç§»æ¤
    editor.semantic_pool = pool
    
    result = editor.generate_with_semantic_collaboration(
        prompt="SFå°èª¬ã®æ¨è–¦ã‚’ãŠé¡˜ã„ã—ã¾ã™",
        user_context="æœ€è¿‘ã®ç§‘å­¦æŠ€è¡“ã«èˆˆå‘³ãŒã‚ã‚Šã¾ã™",
        user_id="user_sci_fi"
    )
    
    print(f"Generated response: {result}")
    
    print("\nğŸ‰ æ„å‘³çš„é¡ä¼¼åº¦çµ±åˆãƒ‡ãƒ¢å®Œäº†!")

if __name__ == "__main__":
    demonstrate_semantic_integration()