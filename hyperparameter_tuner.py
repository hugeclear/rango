#!/usr/bin/env python3
"""
CFS-Chameleon ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ©Ÿèƒ½
ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã«ã‚ˆã‚‹æœ€é©Î±å€¤æ¢ç´¢
"""

import sys
import os
import argparse
import json
import time
import itertools
from pathlib import Path
from typing import List, Dict, Any, Tuple

sys.path.append('/home/nakata/master_thesis/rango')
os.chdir('/home/nakata/master_thesis/rango')

print('ğŸ¯ CFS-CHAMELEON HYPERPARAMETER TUNING')
print('='*80)

from chameleon_cfs_integrator import CollaborativeChameleonEditor

class HyperparameterTuner:
    """ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config_path: str = 'cfs_config.yaml'):
        self.config_path = config_path
        self.results = []
        self.best_params = None
        self.best_score = -float('inf')
    
    def load_test_data(self, max_samples: int = 6) -> List[Dict[str, Any]]:
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨å°‘æ•°ã‚µãƒ³ãƒ—ãƒ«ï¼‰"""
        try:
            data_path = Path("chameleon_prime_personalization/data/raw/LaMP-2/merged.json")
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã«å°‘æ•°ã‚µãƒ³ãƒ—ãƒ«ä½¿ç”¨
            test_samples = data[:max_samples]
            
            print(f"âœ… Test data loaded: {len(test_samples)} samples for tuning")
            return test_samples
            
        except Exception as e:
            error_msg = f"âŒ CRITICAL: Failed to load test data: {e}"
            print(error_msg)
            raise RuntimeError(error_msg)
    
    def evaluate_parameter_combination(self, alpha_p: float, alpha_n: float, 
                                     test_samples: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ„ã¿åˆã‚ã›ã®è©•ä¾¡"""
        print(f"\nğŸ”§ Testing Î±_p={alpha_p}, Î±_n={alpha_n}")
        
        try:
            # CFS-Chameleonã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ä½œæˆ
            editor = CollaborativeChameleonEditor(
                use_collaboration=True,
                config_path=self.config_path
            )
            
            results = []
            total_start = time.time()
            
            for i, sample in enumerate(test_samples):
                input_text = sample.get('input', '')[:120]
                expected = sample.get('output', 'N/A')
                user_id = f"tune_user_{i % 2}"  # 2ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ãƒ†ã‚¹ãƒˆ
                
                sample_start = time.time()
                
                try:
                    # ç”Ÿæˆå®Ÿè¡Œ
                    generated = editor.generate_with_collaborative_chameleon(
                        prompt=input_text,
                        user_id=user_id,
                        alpha_personal=alpha_p,
                        alpha_neutral=alpha_n,
                        max_length=100  # ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨çŸ­ã„ç”Ÿæˆ
                    )
                    
                    generation_time = time.time() - sample_start
                    
                    # ç°¡å˜ãªå“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹
                    quality_score = self._compute_quality_score(generated, expected, input_text)
                    
                    results.append({
                        'sample_id': i,
                        'user_id': user_id,
                        'input': input_text,
                        'expected': expected,
                        'generated': generated,
                        'generation_time': generation_time,
                        'quality_score': quality_score,
                        'status': 'success'
                    })
                    
                    print(f"     Sample {i+1}: Quality={quality_score:.3f}, Time={generation_time:.2f}s")
                    
                except Exception as e:
                    results.append({
                        'sample_id': i,
                        'user_id': user_id,
                        'input': input_text,
                        'expected': expected,
                        'generated': f'ERROR: {e}',
                        'generation_time': time.time() - sample_start,
                        'quality_score': 0.0,
                        'status': 'failed'
                    })
                    
                    print(f"     Sample {i+1}: FAILED - {e}")
            
            total_time = time.time() - total_start
            
            # çµ±è¨ˆè¨ˆç®—
            successful_results = [r for r in results if r['status'] == 'success']
            success_rate = len(successful_results) / len(results) if results else 0
            
            if successful_results:
                avg_quality = sum(r['quality_score'] for r in successful_results) / len(successful_results)
                avg_time = sum(r['generation_time'] for r in successful_results) / len(successful_results)
                
                # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®— (å“è³ªé‡è¦–ã€æ™‚é–“åŠ¹ç‡ã‚‚è€ƒæ…®)
                overall_score = (0.7 * avg_quality + 0.2 * success_rate - 0.1 * (avg_time / 10.0))
            else:
                avg_quality = 0.0
                avg_time = 0.0
                overall_score = 0.0
            
            evaluation = {
                'alpha_p': alpha_p,
                'alpha_n': alpha_n,
                'success_rate': success_rate,
                'avg_quality_score': avg_quality,
                'avg_generation_time': avg_time,
                'overall_score': overall_score,
                'total_evaluation_time': total_time,
                'sample_results': results
            }
            
            print(f"   ğŸ“Š Success Rate: {success_rate*100:.1f}%")
            print(f"   ğŸ“Š Avg Quality: {avg_quality:.3f}")
            print(f"   ğŸ“Š Avg Time: {avg_time:.2f}s")
            print(f"   ğŸ¯ Overall Score: {overall_score:.4f}")
            
            return evaluation
            
        except Exception as e:
            error_msg = f"âŒ Parameter evaluation failed: {e}"
            print(f"   {error_msg}")
            
            return {
                'alpha_p': alpha_p,
                'alpha_n': alpha_n,
                'success_rate': 0.0,
                'avg_quality_score': 0.0,
                'avg_generation_time': 0.0,
                'overall_score': -1.0,  # ãƒšãƒŠãƒ«ãƒ†ã‚£
                'total_evaluation_time': 0.0,
                'error': str(e),
                'sample_results': []
            }
    
    def _compute_quality_score(self, generated: str, expected: str, input_text: str) -> float:
        """ç°¡æ˜“å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—"""
        if not generated or len(generated.strip()) == 0:
            return 0.0
        
        # åŸºæœ¬å“è³ªãƒã‚§ãƒƒã‚¯
        quality = 0.0
        
        # 1. é•·ã•ãƒã‚§ãƒƒã‚¯ (10-300æ–‡å­—ãŒé©æ­£)
        length_score = 1.0
        if len(generated) < 10:
            length_score = len(generated) / 10.0
        elif len(generated) > 300:
            length_score = max(0.0, 1.0 - (len(generated) - 300) / 300.0)
        
        quality += 0.3 * length_score
        
        # 2. åå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º (å“è³ªä½ä¸‹æŒ‡æ¨™)
        repetition_penalty = 0.0
        words = generated.split()
        if len(words) > 5:
            unique_words = set(words)
            repetition_rate = 1.0 - (len(unique_words) / len(words))
            if repetition_rate > 0.5:  # 50%ä»¥ä¸Šã®åå¾©ã¯ãƒšãƒŠãƒ«ãƒ†ã‚£
                repetition_penalty = repetition_rate * 0.5
        
        quality += 0.3 * (1.0 - repetition_penalty)
        
        # 3. å…¥åŠ›ã¨ã®é–¢é€£æ€§ï¼ˆç°¡æ˜“ï¼‰
        input_words = set(input_text.lower().split())
        generated_words = set(generated.lower().split())
        if input_words and generated_words:
            relevance_score = len(input_words.intersection(generated_words)) / len(input_words.union(generated_words))
        else:
            relevance_score = 0.0
        
        quality += 0.2 * relevance_score
        
        # 4. æ–‡ç« ã¨ã—ã¦æˆç«‹ã—ã¦ã„ã‚‹ã‹
        structure_score = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        if any(char.isalpha() for char in generated):  # ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆå«æœ‰
            structure_score += 0.2
        if any(char in '.!?' for char in generated):  # å¥èª­ç‚¹å«æœ‰
            structure_score += 0.2
        if not any(char in r'/\*[]{}()' for char in generated[:50]):  # è¨˜å·ã®éåº¦å«æœ‰ãªã—
            structure_score += 0.1
        
        quality += 0.2 * min(structure_score, 1.0)
        
        return max(0.0, min(1.0, quality))
    
    def run_grid_search(self, alpha_p_values: List[float], alpha_n_values: List[float]) -> Dict[str, Any]:
        """ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒå®Ÿè¡Œ"""
        print(f"\nğŸ” Starting Grid Search...")
        print(f"   Î±_p values: {alpha_p_values}")
        print(f"   Î±_n values: {alpha_n_values}")
        print(f"   Total combinations: {len(alpha_p_values) * len(alpha_n_values)}")
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        test_samples = self.load_test_data(max_samples=6)
        
        grid_start = time.time()
        
        # å…¨çµ„ã¿åˆã‚ã›ã‚’è©•ä¾¡
        for i, (alpha_p, alpha_n) in enumerate(itertools.product(alpha_p_values, alpha_n_values)):
            print(f"\nğŸ“‹ Combination {i+1}/{len(alpha_p_values) * len(alpha_n_values)}")
            
            evaluation = self.evaluate_parameter_combination(alpha_p, alpha_n, test_samples)
            self.results.append(evaluation)
            
            # æœ€è‰¯ã‚¹ã‚³ã‚¢æ›´æ–°
            if evaluation['overall_score'] > self.best_score:
                self.best_score = evaluation['overall_score']
                self.best_params = {
                    'alpha_p': alpha_p,
                    'alpha_n': alpha_n
                }
                print(f"   ğŸ† NEW BEST: Score={self.best_score:.4f}")
        
        grid_time = time.time() - grid_start
        
        # çµæœåˆ†æ
        analysis = self._analyze_grid_results(grid_time)
        
        return analysis
    
    def _analyze_grid_results(self, total_time: float) -> Dict[str, Any]:
        """ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒçµæœåˆ†æ"""
        print(f"\n" + "="*80)
        print("ğŸ“Š GRID SEARCH RESULTS ANALYSIS")
        print("="*80)
        
        if not self.results:
            print("âŒ No results to analyze")
            return {'error': 'No results available'}
        
        # çµæœã‚½ãƒ¼ãƒˆ
        sorted_results = sorted(self.results, key=lambda x: x['overall_score'], reverse=True)
        
        print(f"\nğŸ† TOP 5 PARAMETER COMBINATIONS:")
        print("â”€" * 80)
        print(f"{'Rank':<4} {'Î±_p':<8} {'Î±_n':<8} {'Score':<8} {'Success%':<9} {'Quality':<8} {'AvgTime':<8}")
        print("â”€" * 80)
        
        for i, result in enumerate(sorted_results[:5]):
            print(f"{i+1:<4} {result['alpha_p']:<8.2f} {result['alpha_n']:<8.3f} "
                  f"{result['overall_score']:<8.4f} {result['success_rate']*100:<9.1f} "
                  f"{result['avg_quality_score']:<8.3f} {result['avg_generation_time']:<8.2f}")
        
        # çµ±è¨ˆåˆ†æ
        successful_evals = [r for r in self.results if r['overall_score'] >= 0]
        success_rate = len(successful_evals) / len(self.results) * 100
        
        print(f"\nğŸ“ˆ GRID SEARCH STATISTICS:")
        print(f"   â€¢ Total Evaluations: {len(self.results)}")
        print(f"   â€¢ Successful Evaluations: {len(successful_evals)} ({success_rate:.1f}%)")
        print(f"   â€¢ Best Overall Score: {self.best_score:.4f}")
        print(f"   â€¢ Total Search Time: {total_time:.1f}s ({total_time/60:.1f}min)")
        
        if self.best_params:
            print(f"\nğŸ¯ RECOMMENDED PARAMETERS:")
            print(f"   â€¢ Î±_p (alpha_personal): {self.best_params['alpha_p']}")
            print(f"   â€¢ Î±_n (alpha_neutral): {self.best_params['alpha_n']}")
            
            # æœ€è‰¯çµæœè©³ç´°
            best_result = sorted_results[0]
            print(f"\nğŸ“‹ BEST CONFIGURATION DETAILS:")
            print(f"   â€¢ Success Rate: {best_result['success_rate']*100:.1f}%")
            print(f"   â€¢ Average Quality Score: {best_result['avg_quality_score']:.3f}")
            print(f"   â€¢ Average Generation Time: {best_result['avg_generation_time']:.2f}s")
            print(f"   â€¢ Overall Performance Score: {best_result['overall_score']:.4f}")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        print(f"\nğŸ’¡ PERFORMANCE INSIGHTS:")
        
        # Î±_p ã®å½±éŸ¿åˆ†æ
        alpha_p_groups = {}
        for result in successful_evals:
            alpha_p = result['alpha_p']
            if alpha_p not in alpha_p_groups:
                alpha_p_groups[alpha_p] = []
            alpha_p_groups[alpha_p].append(result['overall_score'])
        
        if alpha_p_groups:
            best_alpha_p = max(alpha_p_groups.keys(), key=lambda x: sum(alpha_p_groups[x])/len(alpha_p_groups[x]))
            print(f"   â€¢ Best Î±_p value: {best_alpha_p} (avg score: {sum(alpha_p_groups[best_alpha_p])/len(alpha_p_groups[best_alpha_p]):.3f})")
        
        # Î±_n ã®å½±éŸ¿åˆ†æ
        alpha_n_groups = {}
        for result in successful_evals:
            alpha_n = result['alpha_n']
            if alpha_n not in alpha_n_groups:
                alpha_n_groups[alpha_n] = []
            alpha_n_groups[alpha_n].append(result['overall_score'])
        
        if alpha_n_groups:
            best_alpha_n = max(alpha_n_groups.keys(), key=lambda x: sum(alpha_n_groups[x])/len(alpha_n_groups[x]))
            print(f"   â€¢ Best Î±_n value: {best_alpha_n} (avg score: {sum(alpha_n_groups[best_alpha_n])/len(alpha_n_groups[best_alpha_n]):.3f})")
        
        return {
            'best_params': self.best_params,
            'best_score': self.best_score,
            'total_evaluations': len(self.results),
            'successful_evaluations': len(successful_evals),
            'total_time': total_time,
            'top_results': sorted_results[:5],
            'all_results': self.results
        }
    
    def save_results(self, output_file: str):
        """çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            analysis = {
                'best_params': self.best_params,
                'best_score': self.best_score,
                'timestamp': time.time(),
                'all_results': self.results
            }
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(analysis, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ Results saved to: {output_file}")
            
        except Exception as e:
            print(f"âŒ Failed to save results: {e}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    parser = argparse.ArgumentParser(description='CFS-Chameleon Hyperparameter Tuning')
    
    # CLIå¼•æ•°å®šç¾©
    parser.add_argument('--alpha_p', type=float, nargs='+', 
                       default=[0.01, 0.1, 0.2, 0.4], 
                       help='Alpha personal values to test')
    parser.add_argument('--alpha_n', type=float, nargs='+', 
                       default=[-0.02, -0.05, -0.1], 
                       help='Alpha neutral values to test')
    parser.add_argument('--grid-search', action='store_true', 
                       help='Enable grid search mode')
    parser.add_argument('--output', type=str, 
                       default='hyperparameter_results.json', 
                       help='Output JSON file')
    parser.add_argument('--config', type=str, 
                       default='cfs_config.yaml', 
                       help='CFS-Chameleon config file')
    
    args = parser.parse_args()
    
    print(f"ğŸš€ Starting Hyperparameter Tuning...")
    print(f"   Alpha Personal Values: {args.alpha_p}")
    print(f"   Alpha Neutral Values: {args.alpha_n}")
    print(f"   Grid Search: {'âœ… Enabled' if args.grid_search else 'âŒ Disabled'}")
    
    # GPUç¢ºèª
    import torch
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
    else:
        print("   GPU: Not available (using CPU)")
    
    if args.grid_search:
        # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒãƒ¢ãƒ¼ãƒ‰
        tuner = HyperparameterTuner(args.config)
        
        try:
            analysis = tuner.run_grid_search(args.alpha_p, args.alpha_n)
            
            # çµæœä¿å­˜
            tuner.save_results(args.output)
            
            print(f"\n" + "="*80)
            print("âœ¨ HYPERPARAMETER TUNING COMPLETED!")
            print("="*80)
            
            if tuner.best_params:
                print(f"ğŸ† Recommended Configuration:")
                print(f"   --alpha_p {tuner.best_params['alpha_p']}")
                print(f"   --alpha_n {tuner.best_params['alpha_n']}")
                print(f"ğŸ¯ Expected Performance Score: {tuner.best_score:.4f}")
            else:
                print("âŒ No optimal parameters found")
            
            return True
            
        except Exception as e:
            print(f"âŒ Grid search failed: {e}")
            return False
    
    else:
        # å˜ä¸€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ†ã‚¹ãƒˆ
        print("\nâš ï¸ Grid search not enabled. Use --grid-search for full optimization.")
        print("   Example: python hyperparameter_tuner.py --grid-search")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)