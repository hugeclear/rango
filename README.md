# rango
Personalized LLM via Chameleon approach

 ğŸ§  Chameleon + PriME: Personalized & Privacy-Preserving LLM Optimization

This repository contains code, configuration, and experimental framework for a three-month research project that integrates **Chameleon-style embedding personalization** and **PriME-style evolutionary model merging** to achieve **efficient, private, and high-performance LLM-based recommendation systems**.

## ğŸš€ Project Overview

This project aims to:
- Personalize LLMs using user-specific directions in embedding space (via Chameleon)
- Optimize merged models via evolutionary algorithms (PriME)
- Support multi-user inference with limited data and strong privacy constraints

## ğŸ“ Project Structure

chameleon_prime_personalization/
â”œâ”€â”€ configs/ # YAML/JSON configs for models, training, evaluation
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Raw datasets (LaMP-2, LaMP-3, etc.)
â”‚ â””â”€â”€ processed/ # Tokenized or embedded versions
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ base_model/ # Base LLM (e.g., LLaMA 2 7B or Mistral)
â”‚ â””â”€â”€ user_adapters/ # LoRA/PEFT modules for users
â”œâ”€â”€ notebooks/ # Experiment tracking or visualization notebooks
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ download_models.py
â”‚ â”œâ”€â”€ download_datasets.py
â”‚ â””â”€â”€ setup_environment.sh
â””â”€â”€ README.md




## ğŸ› ï¸ Setup

### 1. Environment
```bash
bash scripts/setup_environment.sh
source env/bin/activate
2. Download base models

python scripts/download_models.py --model mistralai/Mistral-7B-v0.1
3. Download datasets

python scripts/download_datasets.py --dataset lamp2
ğŸ§© Method Summary
Embedding Personalization (Chameleon)
SVD-based decomposition of user history embeddings

Extracts user-specific direction vs general direction

Adjust embeddings by shifting along user-personalized vector

Evolutionary Merge (PriME)
Each user gets LoRA/IA3 fine-tuned PEFT modules

Cosine similarity used to identify similar shared users

Evolutionary strategy (CMA-ES or NSGA-II) to merge modules

Objective: maximize user utility (F1, ROUGE), minimize privacy leakage

ğŸ“Š Evaluation Metrics
ROUGE / F1 / BLEU (text output quality)

Cosine similarity / KL divergence (privacy leakage)

Model diversity and memorization metrics (advanced)

## Strict Compliance Prompt Pack (LaMP-2)

**ç›®çš„**: å‡ºåŠ›ã‚’ `Answer: <TAG>` ã®å˜ä¸€è¡Œã«å¼·åˆ¶ã—ã€å½¢å¼æº–æ‹ ç‡ â‰¥ 0.95 ã‚’å®‰å®šé”æˆã€‚

### âœ… å®Ÿæ¸¬
- å½¢å¼æº–æ‹ ç‡: **98.0%**ï¼ˆç›®æ¨™ 95% è¶…ï¼‰
- ãƒ†ã‚¹ãƒˆè¦æ¨¡: **50 samples**
- å‡ºåŠ›å½¢å¼: å˜ä¸€è¡Œ `Answer: <TAG>`
- ãƒ‡ã‚³ãƒ¼ãƒ‰åˆ¶ç´„: `temperature=0, top_p=0, max_tokens=8, stop=["\n"]`
- å³æ ¼æ¤œè¨¼ãƒ‘ã‚¿ãƒ¼ãƒ³: `^Answer:\s*([A-Za-z0-9_\- ]+)\s*$`

### ğŸ”§ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
**SYSTEM**
```
You are a strict single-line tag classifier.

RULES (çµ¶å¯¾éµå®ˆ):
1. Output EXACTLY one line: Answer: <TAG>
2. No explanations, no extra words, no punctuation after <TAG>, no emojis.
3. NO NEWLINES. Output must be a single line only.
4. Choose ONE best tag from the allowed list, case-sensitive.
5. If uncertain, still pick the single best tag.

FORBIDDEN:
â€¢ Multiple lines or trailing spaces
â€¢ Any text before/after Answer: <TAG>

Allowed tags: {{ALLOWED_TAGS}}
Required output format: Answer: <TAG>
```

**USER**
```
Task

Classify the following movie description into exactly one tag from the allowed list.

Description

{{QUESTION}}

User Profile (optional)

{{USER_PROFILE}}

Allowed tags (pick ONE, case-sensitive)

{{ALLOWED_TAGS}}

Output constraints (çµ¶å¯¾éµå®ˆ)
â€¢ Single line only.
â€¢ EXACT string format: Answer: <TAG>
â€¢ Nothing else before or after.
â€¢ No newline characters.

Your response

Answer: <TAG>
```

### ğŸ§ª ä½¿ã„æ–¹
```bash
# åŸºæœ¬ãƒ†ã‚¹ãƒˆ
python run_strict_compliance_test.py --samples 10

# å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ†ã‚¹ãƒˆ
python run_strict_compliance_test.py --data path/to/lamp2_test.jsonl

# ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
python run_strict_compliance_test.py \
  --system-prompt prompts/lamp2_system_strict.txt \
  --user-template prompts/lamp2_user_template_strict.txt \
  --target-compliance 0.95
```

### ğŸ‰ åŠ¹æœ
â€¢ ç„¡é–¢ä¿‚å‡ºåŠ›ã®æ’é™¤ï¼ˆä¾‹: "Source: â€¦" ç­‰ï¼‰
â€¢ LaMP-2å‘ã‘ã«æœ€é©åŒ–ã•ã‚ŒãŸå˜ä¸€ã‚¿ã‚°é¸æŠ
â€¢ æ±ºå®šè«–çš„ãƒ‡ã‚³ãƒ¼ãƒ‰è¨­å®šã§å†ç¾æ€§æ‹…ä¿
â€¢ â‰¥95% æº–æ‹ ã§è©•ä¾¡ã®ä¿¡é ¼æ€§å‘ä¸Š

---

ğŸ“„ License
This project is licensed under the Apache License 2.0. See LICENSE for details.

ğŸ“š References
Chameleon: Personalized Prompt Editing for Large Language Models

PriME: Personalized Model Merging via Evolutionary Search

âœï¸ Acknowledgments
This repository was developed for academic research on privacy-preserving personalization using large language models, with support for datasets such as LaMP-2 and LaMP-3, and tested on NVIDIA A100Ã—2 environment.
