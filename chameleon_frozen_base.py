#!/usr/bin/env python3
"""
Chameleon on Frozen Base LLM Implementation
å®Œå…¨å‡çµãƒ™ãƒ¼ã‚¹LLMä¸Šã§ã®Chameleonå®Ÿè£…

åŸå‰‡: 
- ãƒ™ãƒ¼ã‚¹LLMã®é‡ã¿ã¯ä¸€åˆ‡è§¦ã‚‰ãªã„ï¼ˆå®Œå…¨å‡çµï¼‰
- å­¦ç¿’ãƒ»å¾®èª¿æ•´ãƒ»LoRAãƒ»PEFTç­‰ã®é‡ã„æ“ä½œã¯ä¸è¦
- æ¨è«–æ™‚ã®è»½ã„ç·¨é›†æ“ä½œï¼ˆå‰å‘ããƒ•ãƒƒã‚¯ + æŠ•å½±ç·¨é›†ï¼‰ã®ã¿

æ‰‹é †:
1. ãƒ¢ãƒ‡ãƒ«å‡çµ & è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰
2. A.3æº–æ‹ å›ºå®šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§è‡ªå·±ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
3. SVD+CCSã§æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«æ¨å®šï¼ˆã‚ªãƒ•ãƒ©ã‚¤ãƒ³è¨ˆç®—ï¼‰
4. å‰å‘ããƒ•ãƒƒã‚¯ç™»éŒ²ï¼ˆãƒ©ãƒ³ã‚¿ã‚¤ãƒ ç·¨é›†ï¼‰
"""

import torch
import torch.nn.functional as F
import numpy as np
import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
from transformers import AutoTokenizer, AutoModelForCausalLM
from functools import reduce
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# LaMP-2 15ã‚¿ã‚°ï¼ˆè«–æ–‡æº–æ‹ ï¼‰
LAMP2_15_TAGS = [
    'sci-fi', 'based on a book', 'comedy', 'action', 'twist ending',
    'dystopia', 'dark comedy', 'classic', 'psychology', 'fantasy', 
    'romance', 'thought-provoking', 'social commentary', 'violence', 'true story'
]

@dataclass
class FrozenChameleonConfig:
    """å®Œå…¨å‡çµChameleonè¨­å®š"""
    model_path: str
    device: str = "cuda"
    
    # è‡ªå·±ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
    self_gen_samples: int = 50  # ãƒšã‚¢ç”Ÿæˆæ•°
    max_gen_tokens: int = 50    # ç”Ÿæˆé•·
    
    # SVD+CCSæ–¹å‘æ¨å®š
    target_layers: List[str] = None  # ["model.layers.20.mlp", "model.layers.28.mlp"]
    svd_components: int = 1     # ç¬¬ä¸€ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«
    
    # ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ç·¨é›†
    alpha_personal: float = 0.6  # å€‹äººåŒ–å¼·åº¦
    beta_neutral: float = 0.4    # ä¸­ç«‹æŠ‘åˆ¶å¼·åº¦
    gamma_fakeit: float = 0.0    # Fake-itæ··åˆï¼ˆä»»æ„ï¼‰
    
    # ç·¨é›†åˆ¶å¾¡
    last_k_tokens: int = 0       # 0=å…¨ä½“, >0=æœ€å¾Œkãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿
    target_edit_ratio: float = 0.025  # 2.5%ç›®æ¨™
    edit_tolerance: float = 0.5       # Â±50%è¨±å®¹
    
    # ä¸ç¢ºå®Ÿæ€§ã‚²ãƒ¼ãƒˆï¼ˆä»»æ„ï¼‰
    uncertainty_gating: bool = False
    uncertainty_threshold: float = 0.6

class FrozenChameleon:
    """å®Œå…¨å‡çµãƒ™ãƒ¼ã‚¹LLMä¸Šã®Chameleon"""
    
    def __init__(self, config: FrozenChameleonConfig):
        self.config = config
        self.device = torch.device(config.device)
        
        logger.info(f"ğŸ”’ Initializing Frozen Chameleon on {config.model_path}")
        
        # Step 1: ãƒ¢ãƒ‡ãƒ«å‡çµ & è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰
        self._setup_frozen_model()
        
        # æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ä¿å­˜
        self.theta_vectors: Dict[str, Dict[str, torch.Tensor]] = {}
        self.active_hooks: List = []
        
        logger.info("âœ… Frozen Chameleon initialized - NO weight updates will occur")
    
    def _setup_frozen_model(self):
        """Step 1: ãƒ™ãƒ¼ã‚¹LLMå®Œå…¨å‡çµ"""
        logger.info("ğŸ”’ Setting up frozen base LLM...")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.model_path,
            torch_dtype=torch.float32,
            device_map='auto'
        )
        
        # å®Œå…¨å‡çµãƒ¢ãƒ¼ãƒ‰
        self.model.eval()  # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆãƒ»BatchNormç­‰ã‚’å›ºå®šï¼‰
        
        for param in self.model.parameters():
            param.requires_grad_(False)  # å‹¾é…å®Œå…¨OFF
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # å‡çµç¢ºèª
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        logger.info(f"âœ… Model frozen: {total_params:,} total params, {trainable_params:,} trainable (should be 0)")
        assert trainable_params == 0, f"Model not fully frozen! {trainable_params} trainable params found"
    
    def generate_self_data_pairs(self, sample_contexts: List[Dict]) -> List[Dict]:
        """Step 2: A.3æº–æ‹ å›ºå®šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§è‡ªå·±ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
        logger.info(f"ğŸ“ Generating {self.config.self_gen_samples} self-data pairs...")
        
        data_pairs = []
        
        # A.3æº–æ‹ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆå›ºå®šï¼‰
        personalized_template = """You are a personalized assistant. Based on this user's movie preferences:
{profile_context}

For the movie: "{query}"
Predict the most likely tag considering the user's personal taste patterns.
Available tags: {tags}

Answer with exactly one tag:"""

        neutral_template = """You are a neutral movie classifier. Analyze this movie objectively:

For the movie: "{query}"
Predict the most appropriate tag using standard film analysis.
Available tags: {tags}

Answer with exactly one tag:"""
        
        tags_str = ", ".join(LAMP2_15_TAGS)
        
        for i, context in enumerate(sample_contexts[:self.config.self_gen_samples]):
            if i % 10 == 0:
                logger.info(f"Generating pair {i+1}/{min(self.config.self_gen_samples, len(sample_contexts))}")
            
            query = context.get('input', 'A movie')
            profile = context.get('profile', [])
            
            # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æ–‡å­—åˆ—åŒ–
            profile_context = "\n".join([
                f"- {item.get('tag', 'unknown')}: {item.get('description', '')[:80]}..."
                for item in profile[:5]
            ])
            
            # å€‹äººåŒ–ç”Ÿæˆ
            p_prompt = personalized_template.format(
                profile_context=profile_context,
                query=query,
                tags=tags_str
            )
            
            # ä¸­ç«‹ç”Ÿæˆ
            n_prompt = neutral_template.format(
                query=query,
                tags=tags_str
            )
            
            # å‡çµãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆï¼ˆtorch.no_grad()ä¸‹ï¼‰
            with torch.no_grad():
                p_output = self._generate_frozen(p_prompt)
                n_output = self._generate_frozen(n_prompt)
            
            data_pairs.append({
                'query': query,
                'profile_context': profile_context,
                'personalized_output': p_output,
                'neutral_output': n_output,
                'context': context
            })
        
        logger.info(f"âœ… Generated {len(data_pairs)} data pairs (model weights unchanged)")
        return data_pairs
    
    def _generate_frozen(self, prompt: str) -> str:
        """å‡çµãƒ¢ãƒ‡ãƒ«ã§ã®ç”Ÿæˆ"""
        inputs = self.tokenizer(prompt, return_tensors='pt', truncation=True, max_length=512)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # å®Œå…¨no_gradä¸‹ã§ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.config.max_gen_tokens,
                do_sample=False,
                pad_token_id=self.tokenizer.eos_token_id,
                temperature=None,  # æ±ºå®šçš„ç”Ÿæˆ
                top_p=None
            )
        
        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = generated[len(prompt):].strip()
        
        return response
    
    def estimate_direction_vectors_svd_ccs(self, data_pairs: List[Dict]) -> Dict[str, Dict[str, torch.Tensor]]:
        """Step 3: SVD+CCSã§æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«æ¨å®šï¼ˆã‚ªãƒ•ãƒ©ã‚¤ãƒ³è¨ˆç®—ï¼‰"""
        logger.info("ğŸ§® Estimating direction vectors with SVD+CCS...")
        
        if self.config.target_layers is None:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå±¤é¸æŠï¼ˆLlamaç³»ã®ä¸­é–“ãƒ»å¾ŒåŠå±¤ï¼‰
            total_layers = len([n for n, _ in self.model.named_modules() if 'layers.' in n and '.mlp' in n])
            self.config.target_layers = [
                f"model.layers.{total_layers//2}.mlp",      # ä¸­é–“å±¤
                f"model.layers.{3*total_layers//4}.mlp"    # å¾ŒåŠå±¤
            ]
        
        results = {}
        
        for layer_name in self.config.target_layers:
            logger.info(f"Processing layer: {layer_name}")
            
            # åŸ‹ã‚è¾¼ã¿æŠ½å‡º
            personalized_embeds = []
            neutral_embeds = []
            
            for pair in data_pairs:
                # å€‹äººåŒ–åŸ‹ã‚è¾¼ã¿
                p_embed = self._extract_layer_embedding(pair['personalized_output'], layer_name)
                n_embed = self._extract_layer_embedding(pair['neutral_output'], layer_name)
                
                if p_embed is not None and n_embed is not None:
                    personalized_embeds.append(p_embed.cpu().numpy())
                    neutral_embeds.append(n_embed.cpu().numpy())
            
            if len(personalized_embeds) < 3:
                logger.warning(f"Insufficient embeddings for {layer_name}, skipping")
                continue
            
            # HP_l,u, HN_l,u æ§‹ç¯‰
            HP = np.array(personalized_embeds)  # [num_pairs, hidden_size]
            HN = np.array(neutral_embeds)
            
            # SVDã«ã‚ˆã‚‹Î¸_pæ¨å®šï¼ˆç¬¬ä¸€ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«ï¼‰
            U_p, S_p, Vt_p = np.linalg.svd(HP, full_matrices=False)
            theta_p = torch.tensor(Vt_p[0], dtype=torch.float32, device=self.device)
            
            # CCSã«ã‚ˆã‚‹Î¸_næ¨å®šï¼ˆå·®åˆ†ã®ä¸»æˆåˆ†ï¼‰
            diff_matrix = HP - HN  # å€‹äººåŒ– - ä¸­ç«‹ ã®å·®åˆ†
            U_diff, S_diff, Vt_diff = np.linalg.svd(diff_matrix, full_matrices=False)
            theta_n = torch.tensor(Vt_diff[0], dtype=torch.float32, device=self.device)
            
            # æ­£è¦åŒ–
            theta_p = F.normalize(theta_p, dim=0)
            theta_n = F.normalize(theta_n, dim=0)
            
            results[layer_name] = {
                'theta_p': theta_p,
                'theta_n': theta_n,
                'explained_variance_p': float(S_p[0] / S_p.sum()),
                'explained_variance_n': float(S_diff[0] / S_diff.sum()),
                'num_samples': len(personalized_embeds)
            }
            
            logger.info(f"âœ… {layer_name}: Î¸_p var={results[layer_name]['explained_variance_p']:.3f}, "
                       f"Î¸_n var={results[layer_name]['explained_variance_n']:.3f}")
        
        self.theta_vectors = results
        logger.info(f"âœ… Direction vectors estimated for {len(results)} layers (no model weights changed)")
        return results
    
    def _extract_layer_embedding(self, text: str, layer_name: str) -> Optional[torch.Tensor]:
        """æŒ‡å®šå±¤ã®åŸ‹ã‚è¾¼ã¿æŠ½å‡º"""
        embeddings = []
        
        def extraction_hook(module, input, output):
            if isinstance(output, tuple):
                embeddings.append(output[0].detach().clone())
            else:
                embeddings.append(output.detach().clone())
        
        # ãƒ•ãƒƒã‚¯ç™»éŒ²
        layer = self._get_layer_by_name(layer_name)
        hook = layer.register_forward_hook(extraction_hook)
        
        try:
            # é †ä¼æ’­å®Ÿè¡Œï¼ˆno_gradä¸‹ï¼‰
            inputs = self.tokenizer(text, return_tensors='pt', truncation=True)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                _ = self.model(**inputs)
            
            if embeddings:
                return embeddings[0][:, -1, :].mean(dim=0)  # æœ€å¾Œãƒˆãƒ¼ã‚¯ãƒ³ã®å¹³å‡
            return None
                
        finally:
            hook.remove()
    
    def _get_layer_by_name(self, layer_name: str):
        """å±¤åã‹ã‚‰å®Ÿéš›ã®å±¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå–å¾—"""
        return reduce(getattr, layer_name.split('.'), self.model)
    
    def register_runtime_editing_hooks(self) -> List:
        """Step 4: å‰å‘ããƒ•ãƒƒã‚¯ç™»éŒ²ï¼ˆãƒ©ãƒ³ã‚¿ã‚¤ãƒ ç·¨é›†ï¼‰"""
        logger.info("ğŸ£ Registering runtime editing hooks...")
        
        if not self.theta_vectors:
            logger.error("No direction vectors available. Run estimate_direction_vectors_svd_ccs() first.")
            return []
        
        hooks = []
        
        for layer_name, vectors in self.theta_vectors.items():
            theta_p = vectors['theta_p']
            theta_n = vectors['theta_n']
            
            def make_hook(tp, tn, ln):
                def editing_hook(module, input, output):
                    # å‡ºåŠ›å–å¾—
                    x = output[0] if isinstance(output, tuple) else output  # [B, T, H]
                    
                    # æŠ•å½±è¨ˆç®—
                    def projection(tensor, vector):
                        """proj(x, v) = (âŸ¨x,vâŸ© / ||v||Â²) v"""
                        v_norm = vector.norm() + 1e-8
                        v_normalized = vector / v_norm
                        dot_product = torch.sum(tensor * v_normalized, dim=-1, keepdim=True)
                        return dot_product * v_normalized
                    
                    # å€‹äººåŒ–ãƒ»ä¸­ç«‹æŠ•å½±
                    proj_p = projection(x, tp)
                    proj_n = projection(x, tn)
                    
                    # åŸºæœ¬ç·¨é›†
                    edit = self.config.alpha_personal * proj_p - abs(self.config.beta_neutral) * proj_n
                    
                    # last-kãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™
                    if x.ndim == 3 and self.config.last_k_tokens > 0:
                        B, T, H = x.shape
                        mask = x.new_zeros(B, T, 1)
                        mask[:, -min(T, self.config.last_k_tokens):, :] = 1
                        edit = edit * mask
                    
                    # edit-ratioåˆ¶å¾¡
                    if self.config.target_edit_ratio > 0:
                        original_norm = x.norm(dim=-1)
                        edit_norm = edit.norm(dim=-1)
                        current_ratio = (edit_norm / (original_norm + 1e-8)).mean().item()
                        
                        target_ratio = self.config.target_edit_ratio
                        if current_ratio > 0 and (current_ratio < 0.8 * target_ratio or current_ratio > 1.2 * target_ratio):
                            scale = target_ratio / current_ratio
                            scale = max(1.0 - self.config.edit_tolerance, 
                                       min(1.0 + self.config.edit_tolerance, scale))
                            edit = edit * scale
                            
                            logger.debug(f"{ln}: edit-ratio {current_ratio:.4f} -> {target_ratio:.4f} (scale: {scale:.3f})")
                    
                    # æœ€çµ‚ç·¨é›†é©ç”¨
                    x_edited = x + edit
                    
                    return (x_edited,) + output[1:] if isinstance(output, tuple) else x_edited
                
                return editing_hook
            
            # ãƒ•ãƒƒã‚¯ç™»éŒ²
            layer = self._get_layer_by_name(layer_name)
            hook = layer.register_forward_hook(make_hook(theta_p, theta_n, layer_name))
            hooks.append(hook)
            
            logger.info(f"âœ… Registered editing hook on {layer_name}")
        
        self.active_hooks = hooks
        logger.info(f"âœ… Runtime editing activated on {len(hooks)} layers")
        return hooks
    
    def remove_editing_hooks(self):
        """ç·¨é›†ãƒ•ãƒƒã‚¯å‰Šé™¤"""
        for hook in self.active_hooks:
            hook.remove()
        self.active_hooks = []
        logger.info("ğŸ”“ Runtime editing hooks removed")
    
    def generate_with_chameleon(self, prompt: str, max_new_tokens: int = 50) -> str:
        """Chameleonç·¨é›†ä»˜ãç”Ÿæˆ"""
        if not self.active_hooks:
            logger.warning("No editing hooks active. Use register_runtime_editing_hooks() first.")
        
        inputs = self.tokenizer(prompt, return_tensors='pt', truncation=True, max_length=512)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = generated[len(prompt):].strip()
        
        return response
    
    def run_full_pipeline(self, sample_contexts: List[Dict]) -> Dict[str, Any]:
        """å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        logger.info("ğŸš€ Running full Frozen Chameleon pipeline...")
        
        start_time = time.time()
        
        # Step 2: è‡ªå·±ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
        data_pairs = self.generate_self_data_pairs(sample_contexts)
        
        # Step 3: æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«æ¨å®š
        theta_vectors = self.estimate_direction_vectors_svd_ccs(data_pairs)
        
        # Step 4: ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ç·¨é›†ãƒ•ãƒƒã‚¯ç™»éŒ²
        hooks = self.register_runtime_editing_hooks()
        
        end_time = time.time()
        
        logger.info(f"âœ… Full pipeline completed in {end_time - start_time:.1f}s")
        logger.info(f"ğŸ“Š Results: {len(data_pairs)} pairs, {len(theta_vectors)} layers, {len(hooks)} hooks")
        
        return {
            'data_pairs': data_pairs,
            'theta_vectors': theta_vectors,
            'active_hooks': len(hooks),
            'setup_time': end_time - start_time,
            'model_frozen': True,
            'trainable_params': 0
        }

def demo_frozen_chameleon():
    """ãƒ‡ãƒ¢å®Ÿè¡Œ"""
    print("ğŸ§ª Frozen Chameleon Demo")
    print("=" * 50)
    
    # ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
    sample_contexts = [
        {
            'input': 'A romantic story about two lovers in Paris',
            'profile': [
                {'tag': 'romance', 'description': 'Love story between two people'},
                {'tag': 'classic', 'description': 'Classic Hollywood film'}
            ]
        },
        {
            'input': 'Futuristic space exploration adventure',
            'profile': [
                {'tag': 'sci-fi', 'description': 'Space battles and alien encounters'},
                {'tag': 'action', 'description': 'High-energy adventure movie'}
            ]
        }
    ]
    
    # è¨­å®š
    config = FrozenChameleonConfig(
        model_path="./chameleon_prime_personalization/models/base_model",
        self_gen_samples=len(sample_contexts),
        target_layers=["model.layers.20.mlp", "model.layers.27.mlp"],
        alpha_personal=0.4,
        beta_neutral=0.05,
        target_edit_ratio=0.025
    )
    
    # ChameleonåˆæœŸåŒ–
    chameleon = FrozenChameleon(config)
    
    # å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    results = chameleon.run_full_pipeline(sample_contexts)
    
    # ãƒ†ã‚¹ãƒˆç”Ÿæˆ
    print("\nğŸ¯ Testing Chameleon generation...")
    test_prompts = [
        "For the movie 'A thriller about psychological manipulation', the most appropriate tag is:",
        "For the movie 'A funny comedy about workplace situations', the most appropriate tag is:"
    ]
    
    for prompt in test_prompts:
        response = chameleon.generate_with_chameleon(prompt, max_new_tokens=10)
        print(f"Prompt: {prompt}")
        print(f"Response: {response}")
        print()
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    chameleon.remove_editing_hooks()
    
    print("âœ… Demo completed!")
    print(f"ğŸ“‹ Summary:")
    print(f"  - Model weights: COMPLETELY FROZEN")
    print(f"  - Trainable parameters: {results['trainable_params']}")
    print(f"  - Data pairs generated: {len(results['data_pairs'])}")
    print(f"  - Direction vectors: {len(results['theta_vectors'])}")
    print(f"  - Active hooks: {results['active_hooks']}")
    print(f"  - Setup time: {results['setup_time']:.1f}s")

if __name__ == "__main__":
    demo_frozen_chameleon()