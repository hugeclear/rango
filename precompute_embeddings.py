#!/usr/bin/env python3
"""
GraphRAGå‹•çš„CFS-Chameleonå‘ã‘å…±é€šåŸ‹ã‚è¾¼ã¿å‰è¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ 
LaMP-2/Tenrecãƒ¦ãƒ¼ã‚¶ãƒ¼/ã‚¢ã‚¤ãƒ†ãƒ ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰é«˜å“è³ªåŸ‹ã‚è¾¼ã¿ã‚’ä¸¦åˆ—ç”Ÿæˆ

## æ©Ÿèƒ½:
- sentence-transformers: all-MiniLM-L6-v2 (384d) / bge-base-en-v1.5 (768d) ç­‰
- GPUåˆ©ç”¨ãƒ»ã‚ªãƒ¼ãƒˆã‚­ãƒ£ã‚¹ãƒˆ(fp16)ãƒ»ãƒãƒƒãƒå‡¦ç†ãƒ»OOMå›é¿
- è¤‡æ•°åˆ—çµåˆ([SEP]åŒºåˆ‡ã‚Š)ãƒ»æ¬ æè€æ€§ãƒ»L2æ­£è¦åŒ–(opt)
- å‡ºåŠ›: Parquet + NPY + index.json
- å†ç¾æ€§ç¢ºä¿(seed=42)ãƒ»é€²æ—è¡¨ç¤º(tqdm)

## å¯¾å¿œãƒ‡ãƒ¼ã‚¿å½¢å¼:
- CSV/Parquet/JSONLå…¥åŠ›
- æŒ‡å®šåˆ—ã®æŸ”è»Ÿãªçµ„ã¿åˆã‚ã›
- å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿(10,000+è¡Œ)å¯¾å¿œ
"""

import os
import sys
import json
import argparse
import random
import warnings
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
import numpy as np
import pandas as pd
import torch
from torch.cuda.amp import autocast
from tqdm import tqdm
import time
import logging

# ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯
try:
    from sentence_transformers import SentenceTransformer
    import pyarrow.parquet as pq
    DEPENDENCIES_OK = True
except ImportError as e:
    print(f"âŒ ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒä¸è¶³: {e}")
    print("pip install sentence-transformers pyarrow pandas torch tqdm ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    sys.exit(1)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# è­¦å‘ŠæŠ‘åˆ¶
warnings.filterwarnings("ignore", category=FutureWarning)

class EmbeddingPrecomputer:
    """é«˜æ€§èƒ½åŸ‹ã‚è¾¼ã¿å‰è¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, 
                 model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
                 device: str = "auto",
                 batch_size: int = 32,
                 normalize_l2: bool = True,
                 fp16: bool = True,
                 seed: int = 42):
        """
        åˆæœŸåŒ–
        
        Args:
            model_name: sentence-transformersãƒ¢ãƒ‡ãƒ«å
            device: å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹("auto", "cuda", "cpu")
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            normalize_l2: L2æ­£è¦åŒ–ã‚’è¡Œã†ã‹
            fp16: FP16(half precision)ã‚’ä½¿ç”¨ã™ã‚‹ã‹
            seed: ä¹±æ•°ã‚·ãƒ¼ãƒ‰
        """
        self.model_name = model_name
        self.batch_size = batch_size
        self.normalize_l2 = normalize_l2
        self.fp16 = fp16
        self.seed = seed
        
        # å†ç¾æ€§ç¢ºä¿
        self._set_seed(seed)
        
        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        if device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
            
        logger.info(f"ğŸš€ EmbeddingPrecomputeråˆæœŸåŒ–")
        logger.info(f"   Model: {model_name}")
        logger.info(f"   Device: {self.device}")
        logger.info(f"   Batch size: {batch_size}")
        logger.info(f"   FP16: {fp16}")
        logger.info(f"   L2 normalize: {normalize_l2}")
        
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        try:
            self.model = SentenceTransformer(model_name, device=self.device)
            if fp16 and self.device == "cuda":
                self.model.half()
            logger.info(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {model_name}")
            
            # åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°å–å¾—
            test_embedding = self.model.encode("test", convert_to_numpy=True)
            self.embedding_dim = len(test_embedding)
            logger.info(f"ğŸ“ åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°: {self.embedding_dim}")
            
        except Exception as e:
            logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            raise
            
    def _set_seed(self, seed: int):
        """ä¹±æ•°ã‚·ãƒ¼ãƒ‰å›ºå®š"""
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
        
    def load_data(self, 
                  data_path: str,
                  text_cols: List[str],
                  id_col: str = "id",
                  sep_token: str = " [SEP] ",
                  max_rows: Optional[int] = None) -> pd.DataFrame:
        """
        ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†
        
        Args:
            data_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (CSV/Parquet/JSONL)
            text_cols: ãƒ†ã‚­ã‚¹ãƒˆåˆ—åãƒªã‚¹ãƒˆ
            id_col: IDåˆ—å
            sep_token: åˆ—çµåˆç”¨ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿
            max_rows: æœ€å¤§èª­ã¿è¾¼ã¿è¡Œæ•°
            
        Returns:
            å‰å‡¦ç†æ¸ˆã¿DataFrame
        """
        logger.info(f"ğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹: {data_path}")
        
        path = Path(data_path)
        if not path.exists():
            raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_path}")
            
        # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼åˆ¤å®šã¨èª­ã¿è¾¼ã¿
        try:
            if path.suffix.lower() == '.csv':
                df = pd.read_csv(data_path, nrows=max_rows)
            elif path.suffix.lower() == '.parquet':
                df = pd.read_parquet(data_path)
                if max_rows:
                    df = df.head(max_rows)
            elif path.suffix.lower() in ['.jsonl', '.json']:
                df = pd.read_json(data_path, lines=True, nrows=max_rows)
            else:
                raise ValueError(f"æœªå¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {path.suffix}")
                
            logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,} è¡Œ")
            
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            raise
            
        # å¿…è¦åˆ—ãƒã‚§ãƒƒã‚¯
        missing_cols = [col for col in [id_col] + text_cols if col not in df.columns]
        if missing_cols:
            logger.error(f"âŒ å¿…è¦ãªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_cols}")
            logger.info(f"åˆ©ç”¨å¯èƒ½ãªåˆ—: {list(df.columns)}")
            raise ValueError(f"å¿…è¦åˆ—ä¸è¶³: {missing_cols}")
            
        # ãƒ†ã‚­ã‚¹ãƒˆåˆ—çµåˆã¨å‰å‡¦ç†
        logger.info(f"ğŸ”„ ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†é–‹å§‹: {text_cols}")
        
        def combine_text_cols(row):
            """è¤‡æ•°ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã‚’çµåˆ"""
            texts = []
            for col in text_cols:
                val = row[col]
                if pd.isna(val) or val == "":
                    texts.append("[MISSING]")
                else:
                    texts.append(str(val).strip())
            return sep_token.join(texts)
            
        df['combined_text'] = df.apply(combine_text_cols, axis=1)
        
        # çµ±è¨ˆæƒ…å ±
        avg_length = df['combined_text'].str.len().mean()
        max_length = df['combined_text'].str.len().max()
        empty_count = df['combined_text'].str.contains(r'^\[MISSING\]').sum()
        
        logger.info(f"ğŸ“Š ãƒ†ã‚­ã‚¹ãƒˆçµ±è¨ˆ:")
        logger.info(f"   å¹³å‡é•·: {avg_length:.1f} æ–‡å­—")
        logger.info(f"   æœ€å¤§é•·: {max_length:,} æ–‡å­—")
        logger.info(f"   æ¬ æè¡Œ: {empty_count:,} / {len(df):,} ({empty_count/len(df)*100:.1f}%)")
        
        return df[[id_col, 'combined_text']].rename(columns={id_col: 'id'})
        
    def encode_batch(self, texts: List[str]) -> np.ndarray:
        """
        ãƒãƒƒãƒåŸ‹ã‚è¾¼ã¿ç”Ÿæˆï¼ˆGPU + FP16å¯¾å¿œï¼‰
        
        Args:
            texts: ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
            
        Returns:
            åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«é…åˆ— (N, embedding_dim)
        """
        try:
            if self.fp16 and self.device == "cuda":
                with autocast():
                    embeddings = self.model.encode(
                        texts, 
                        batch_size=self.batch_size,
                        convert_to_numpy=True,
                        normalize_embeddings=self.normalize_l2,
                        show_progress_bar=False
                    )
            else:
                embeddings = self.model.encode(
                    texts,
                    batch_size=self.batch_size,
                    convert_to_numpy=True,
                    normalize_embeddings=self.normalize_l2,
                    show_progress_bar=False
                )
                
            return embeddings.astype(np.float32)
            
        except Exception as e:
            logger.error(f"âŒ ãƒãƒƒãƒåŸ‹ã‚è¾¼ã¿ç”Ÿæˆå¤±æ•—: {e}")
            raise
            
    def compute_embeddings(self, 
                          df: pd.DataFrame,
                          chunk_size: Optional[int] = None) -> Dict[str, np.ndarray]:
        """
        å…¨ãƒ‡ãƒ¼ã‚¿ã®åŸ‹ã‚è¾¼ã¿è¨ˆç®—ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªé€æ¬¡å‡¦ç†ï¼‰
        
        Args:
            df: å‰å‡¦ç†æ¸ˆã¿DataFrame
            chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆNoneãªã‚‰è‡ªå‹•è¨­å®šï¼‰
            
        Returns:
            {id: embedding} ã®è¾æ›¸
        """
        if chunk_size is None:
            # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’è€ƒæ…®ã—ãŸè‡ªå‹•ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºè¨­å®š
            if len(df) <= 1000:
                chunk_size = len(df)
            elif len(df) <= 10000:
                chunk_size = 500
            else:
                chunk_size = 1000
                
        logger.info(f"ğŸ§  åŸ‹ã‚è¾¼ã¿è¨ˆç®—é–‹å§‹:")
        logger.info(f"   ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(df):,}")
        logger.info(f"   ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {chunk_size}")
        logger.info(f"   äºˆæƒ³ãƒãƒ£ãƒ³ã‚¯æ•°: {(len(df) + chunk_size - 1) // chunk_size}")
        
        embeddings_dict = {}
        start_time = time.time()
        
        # é€æ¬¡ãƒãƒƒãƒå‡¦ç†
        with tqdm(total=len(df), desc="Embeddingç”Ÿæˆ", unit="samples") as pbar:
            for i in range(0, len(df), chunk_size):
                chunk = df.iloc[i:i + chunk_size]
                
                try:
                    # ãƒãƒƒãƒåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
                    chunk_embeddings = self.encode_batch(chunk['combined_text'].tolist())
                    
                    # çµæœã‚’è¾æ›¸ã«ä¿å­˜
                    for j, (_, row) in enumerate(chunk.iterrows()):
                        embeddings_dict[str(row['id'])] = chunk_embeddings[j]
                        
                    pbar.update(len(chunk))
                    
                    # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
                    if self.device == "cuda":
                        torch.cuda.empty_cache()
                        
                except Exception as e:
                    logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯ {i//chunk_size + 1} å‡¦ç†å¤±æ•—: {e}")
                    raise
                    
        elapsed_time = time.time() - start_time
        samples_per_sec = len(df) / elapsed_time
        
        logger.info(f"âœ… åŸ‹ã‚è¾¼ã¿è¨ˆç®—å®Œäº†:")
        logger.info(f"   å‡¦ç†æ™‚é–“: {elapsed_time:.2f}s")
        logger.info(f"   å‡¦ç†é€Ÿåº¦: {samples_per_sec:.1f} samples/sec")
        logger.info(f"   ç”Ÿæˆæ¸ˆã¿åŸ‹ã‚è¾¼ã¿æ•°: {len(embeddings_dict):,}")
        
        return embeddings_dict
        
    def save_embeddings(self, 
                       embeddings_dict: Dict[str, np.ndarray],
                       output_dir: str,
                       prefix: str = "embeddings") -> Dict[str, str]:
        """
        åŸ‹ã‚è¾¼ã¿çµæœã‚’è¤‡æ•°å½¢å¼ã§ä¿å­˜
        
        Args:
            embeddings_dict: åŸ‹ã‚è¾¼ã¿è¾æ›¸
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            prefix: ãƒ•ã‚¡ã‚¤ãƒ«åãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
            
        Returns:
            ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è¾æ›¸
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ’¾ åŸ‹ã‚è¾¼ã¿ä¿å­˜é–‹å§‹: {output_dir}")
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        ids = list(embeddings_dict.keys())
        embeddings_matrix = np.array(list(embeddings_dict.values()), dtype=np.float32)
        
        saved_files = {}
        
        # 1. Parquetå½¢å¼ä¿å­˜
        try:
            df_embeddings = pd.DataFrame({
                'id': ids,
                'embedding': [emb.tolist() for emb in embeddings_matrix]
            })
            parquet_path = output_path / f"{prefix}.parquet"
            df_embeddings.to_parquet(parquet_path, index=False)
            saved_files['parquet'] = str(parquet_path)
            logger.info(f"âœ… Parquetä¿å­˜å®Œäº†: {parquet_path}")
            
        except Exception as e:
            logger.error(f"âŒ Parquetä¿å­˜å¤±æ•—: {e}")
            
        # 2. NPY + index.jsonå½¢å¼ä¿å­˜
        try:
            npy_path = output_path / f"{prefix}.npy"
            np.save(npy_path, embeddings_matrix)
            saved_files['npy'] = str(npy_path)
            
            index_path = output_path / f"{prefix}_index.json"
            with open(index_path, 'w', encoding='utf-8') as f:
                json.dump(ids, f, ensure_ascii=False, indent=2)
            saved_files['index'] = str(index_path)
            
            logger.info(f"âœ… NPY+Indexä¿å­˜å®Œäº†: {npy_path}, {index_path}")
            
        except Exception as e:
            logger.error(f"âŒ NPY+Indexä¿å­˜å¤±æ•—: {e}")
            
        # 3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        try:
            metadata = {
                'model_name': self.model_name,
                'embedding_dim': self.embedding_dim,
                'total_samples': len(embeddings_dict),
                'normalize_l2': self.normalize_l2,
                'fp16': self.fp16,
                'seed': self.seed,
                'created_at': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            metadata_path = output_path / f"{prefix}_metadata.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            saved_files['metadata'] = str(metadata_path)
            
            logger.info(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {metadata_path}")
            
        except Exception as e:
            logger.error(f"âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å¤±æ•—: {e}")
            
        logger.info(f"ğŸ¯ ä¿å­˜å®Œäº†: {len(saved_files)} ãƒ•ã‚¡ã‚¤ãƒ«")
        return saved_files
        

def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    parser = argparse.ArgumentParser(
        description="GraphRAGå‹•çš„CFS-Chameleonå‘ã‘å…±é€šåŸ‹ã‚è¾¼ã¿å‰è¨ˆç®—",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # LaMP-2ãƒ¦ãƒ¼ã‚¶ãƒ¼åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
  python precompute_embeddings.py \\
    --data_path /path/to/lamp2_users.csv \\
    --text_cols profile history preferences \\
    --id_col user_id \\
    --output_dir ./embeddings/lamp2_users \\
    --model sentence-transformers/all-MiniLM-L6-v2

  # Tenrecã‚¢ã‚¤ãƒ†ãƒ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆï¼ˆå¤§è¦æ¨¡ãƒ»GPUï¼‰
  python precompute_embeddings.py \\
    --data_path /path/to/tenrec_items.parquet \\
    --text_cols title description tags category \\
    --id_col item_id \\
    --output_dir ./embeddings/tenrec_items \\
    --model sentence-transformers/bge-base-en-v1.5 \\
    --batch_size 64 \\
    --fp16
        """
    )
    
    # å¿…é ˆå¼•æ•°
    parser.add_argument('--data_path', type=str, required=True,
                        help='å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ (CSV/Parquet/JSONL)')
    parser.add_argument('--text_cols', type=str, nargs='+', required=True,
                        help='ãƒ†ã‚­ã‚¹ãƒˆåˆ—åãƒªã‚¹ãƒˆ (ä¾‹: profile history tags)')
    parser.add_argument('--output_dir', type=str, required=True,
                        help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³å¼•æ•°
    parser.add_argument('--id_col', type=str, default='id',
                        help='IDåˆ—å (default: id)')
    parser.add_argument('--model', type=str, default='sentence-transformers/all-MiniLM-L6-v2',
                        help='sentence-transformersãƒ¢ãƒ‡ãƒ«å')
    parser.add_argument('--device', type=str, default='auto',
                        choices=['auto', 'cuda', 'cpu'],
                        help='å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹ (default: auto)')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='ãƒãƒƒãƒã‚µã‚¤ã‚º (default: 32)')
    parser.add_argument('--chunk_size', type=int, default=None,
                        help='ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º (default: auto)')
    parser.add_argument('--max_rows', type=int, default=None,
                        help='æœ€å¤§èª­ã¿è¾¼ã¿è¡Œæ•° (default: all)')
    parser.add_argument('--prefix', type=str, default='embeddings',
                        help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ (default: embeddings)')
    parser.add_argument('--sep_token', type=str, default=' [SEP] ',
                        help='åˆ—çµåˆã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ (default: " [SEP] ")')
    parser.add_argument('--seed', type=int, default=42,
                        help='ä¹±æ•°ã‚·ãƒ¼ãƒ‰ (default: 42)')
    
    # ãƒ•ãƒ©ã‚°
    parser.add_argument('--no_normalize', action='store_true',
                        help='L2æ­£è¦åŒ–ã‚’ç„¡åŠ¹åŒ–')
    parser.add_argument('--no_fp16', action='store_true',
                        help='FP16ã‚’ç„¡åŠ¹åŒ–')
    parser.add_argument('--verbose', action='store_true',
                        help='è©³ç´°ãƒ­ã‚°è¡¨ç¤º')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        
    try:
        # åŸ‹ã‚è¾¼ã¿å‰è¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        precomputer = EmbeddingPrecomputer(
            model_name=args.model,
            device=args.device,
            batch_size=args.batch_size,
            normalize_l2=not args.no_normalize,
            fp16=not args.no_fp16,
            seed=args.seed
        )
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†
        df = precomputer.load_data(
            data_path=args.data_path,
            text_cols=args.text_cols,
            id_col=args.id_col,
            sep_token=args.sep_token,
            max_rows=args.max_rows
        )
        
        # åŸ‹ã‚è¾¼ã¿è¨ˆç®—
        embeddings_dict = precomputer.compute_embeddings(
            df=df,
            chunk_size=args.chunk_size
        )
        
        # çµæœä¿å­˜
        saved_files = precomputer.save_embeddings(
            embeddings_dict=embeddings_dict,
            output_dir=args.output_dir,
            prefix=args.prefix
        )
        
        # å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ
        print(f"\nğŸ‰ åŸ‹ã‚è¾¼ã¿å‰è¨ˆç®—å®Œäº†!")
        print(f"ğŸ“Š å‡¦ç†çµ±è¨ˆ:")
        print(f"   ãƒ¢ãƒ‡ãƒ«: {args.model}")
        print(f"   å‡¦ç†æ•°: {len(embeddings_dict):,} samples")
        print(f"   æ¬¡å…ƒæ•°: {precomputer.embedding_dim}")
        print(f"   å‡ºåŠ›å…ˆ: {args.output_dir}")
        
        print(f"\nğŸ“ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«:")
        for file_type, file_path in saved_files.items():
            print(f"   {file_type}: {file_path}")
            
    except Exception as e:
        logger.error(f"âŒ å®Ÿè¡Œå¤±æ•—: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()