#!/usr/bin/env python3
"""
CFS-Chameleonå‘ã‘æ„å‘³çš„é¡ä¼¼åº¦è¨ˆç®—ã‚¨ãƒ³ã‚¸ãƒ³
å¾“æ¥ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‹ã‚‰æ„å‘³çš„ã«ãƒªãƒƒãƒãªåŸ‹ã‚è¾¼ã¿ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼åº¦è¨ˆç®—ã¸ã®é«˜åº¦åŒ–
"""

import numpy as np
import torch
from typing import Union, List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import logging
import time
import json
from pathlib import Path
import hashlib

# åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    from sentence_transformers import SentenceTransformer
    import openai
    EMBEDDING_LIBS_AVAILABLE = True
except ImportError:
    print("âš ï¸ Embedding libraries not available. Using mock implementations.")
    EMBEDDING_LIBS_AVAILABLE = False

# CFS-Chameleoné–¢é€£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
try:
    from cfs_chameleon_extension import DirectionPiece
    CFS_AVAILABLE = True
except ImportError:
    print("âš ï¸ CFS modules not available. Using mock implementations.")
    CFS_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class SemanticSimilarityConfig:
    """æ„å‘³çš„é¡ä¼¼åº¦è¨ˆç®—è¨­å®š"""
    primary_model: str = "sentence-transformers"  # sentence-transformers, openai, hybrid
    model_name: str = "all-MiniLM-L6-v2"
    openai_model: str = "text-embedding-ada-002"
    device: str = "cuda"
    cache_embeddings: bool = True
    batch_size: int = 32
    similarity_threshold: float = 0.1
    context_weight: float = 0.7  # ãƒ¦ãƒ¼ã‚¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®é‡ã¿
    semantic_tag_weight: float = 0.3  # semantic_tagã®é‡ã¿
    normalize_embeddings: bool = True
    max_cache_size: int = 10000

@dataclass
class EmbeddingCache:
    """åŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    text_to_embedding: Dict[str, np.ndarray] = None
    piece_to_embedding: Dict[str, np.ndarray] = None
    access_count: Dict[str, int] = None
    
    def __post_init__(self):
        if self.text_to_embedding is None:
            self.text_to_embedding = {}
        if self.piece_to_embedding is None:
            self.piece_to_embedding = {}
        if self.access_count is None:
            self.access_count = {}

class SemanticSimilarityEngine:
    """æ„å‘³çš„é¡ä¼¼åº¦è¨ˆç®—ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, config: SemanticSimilarityConfig = None):
        """
        åˆæœŸåŒ–
        
        Args:
            config: æ„å‘³çš„é¡ä¼¼åº¦è¨ˆç®—è¨­å®š
        """
        self.config = config or SemanticSimilarityConfig()
        self.embedding_cache = EmbeddingCache()
        
        # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self._initialize_embedding_models()
        
        logger.info("âœ… SemanticSimilarityEngine initialized")
        logger.info(f"   Primary model: {self.config.primary_model}")
        logger.info(f"   Model name: {self.config.model_name}")
        logger.info(f"   Cache enabled: {self.config.cache_embeddings}")
    
    def _initialize_embedding_models(self):
        """åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–"""
        if EMBEDDING_LIBS_AVAILABLE:
            try:
                # SentenceTransformeråˆæœŸåŒ–
                if self.config.primary_model in ["sentence-transformers", "hybrid"]:
                    self.sentence_model = SentenceTransformer(
                        self.config.model_name,
                        device=self.config.device
                    )
                    logger.info(f"âœ… SentenceTransformer loaded: {self.config.model_name}")
                else:
                    self.sentence_model = None
                
                # OpenAI APIè¨­å®šï¼ˆå®Ÿéš›ã®ã‚­ãƒ¼ãŒå¿…è¦ï¼‰
                if self.config.primary_model in ["openai", "hybrid"]:
                    # æ³¨æ„: å®Ÿéš›ã®ä½¿ç”¨æ™‚ã«ã¯openai.api_keyã‚’è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
                    self.openai_available = True
                    logger.info(f"âœ… OpenAI Embedding configured: {self.config.openai_model}")
                else:
                    self.openai_available = False
                
            except Exception as e:
                logger.error(f"âŒ Embedding model initialization error: {e}")
                self.sentence_model = None
                self.openai_available = False
        else:
            # ãƒ¢ãƒƒã‚¯è¨­å®š
            self.sentence_model = None
            self.openai_available = False
            logger.warning("âš ï¸ Using mock embedding implementations")
    
    def _get_text_hash(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”¨ï¼‰"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def _encode_with_sentence_transformer(self, texts: List[str]) -> np.ndarray:
        """SentenceTransformerã«ã‚ˆã‚‹åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""
        if self.sentence_model is None:
            # ãƒ¢ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿ï¼ˆ384æ¬¡å…ƒï¼‰
            return np.random.randn(len(texts), 384).astype(np.float32)
        
        try:
            embeddings = self.sentence_model.encode(
                texts,
                batch_size=self.config.batch_size,
                show_progress_bar=False,
                convert_to_numpy=True,
                normalize_embeddings=self.config.normalize_embeddings
            )
            return embeddings.astype(np.float32)
        
        except Exception as e:
            logger.error(f"SentenceTransformer encoding error: {e}")
            return np.random.randn(len(texts), 384).astype(np.float32)
    
    def _encode_with_openai(self, texts: List[str]) -> np.ndarray:
        """OpenAI Embeddingã«ã‚ˆã‚‹åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""
        if not self.openai_available:
            # ãƒ¢ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿ï¼ˆ1536æ¬¡å…ƒï¼‰
            return np.random.randn(len(texts), 1536).astype(np.float32)
        
        try:
            # ãƒãƒƒãƒã‚µã‚¤ã‚ºã®åˆ¶é™ã‚’è€ƒæ…®
            embeddings = []
            batch_size = min(self.config.batch_size, 100)  # OpenAIã®åˆ¶é™
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                
                response = openai.Embedding.create(
                    input=batch_texts,
                    engine=self.config.openai_model
                )
                
                batch_embeddings = [item['embedding'] for item in response['data']]
                embeddings.extend(batch_embeddings)
            
            embeddings_array = np.array(embeddings, dtype=np.float32)
            
            if self.config.normalize_embeddings:
                embeddings_array = embeddings_array / np.linalg.norm(
                    embeddings_array, axis=1, keepdims=True
                )
            
            return embeddings_array
            
        except Exception as e:
            logger.error(f"OpenAI embedding error: {e}")
            return np.random.randn(len(texts), 1536).astype(np.float32)
    
    def encode_texts(self, texts: List[str]) -> np.ndarray:
        """
        ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        
        Args:
            texts: åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
            
        Returns:
            åŸ‹ã‚è¾¼ã¿è¡Œåˆ— (num_texts, embedding_dim)
        """
        if not texts:
            return np.array([]).reshape(0, 384)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if self.config.cache_embeddings:
            cached_embeddings = []
            uncached_texts = []
            uncached_indices = []
            
            for i, text in enumerate(texts):
                text_hash = self._get_text_hash(text)
                if text_hash in self.embedding_cache.text_to_embedding:
                    cached_embeddings.append((i, self.embedding_cache.text_to_embedding[text_hash]))
                    self.embedding_cache.access_count[text_hash] = \
                        self.embedding_cache.access_count.get(text_hash, 0) + 1
                else:
                    uncached_texts.append(text)
                    uncached_indices.append(i)
            
            # æ–°è¦åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            if uncached_texts:
                if self.config.primary_model == "openai":
                    new_embeddings = self._encode_with_openai(uncached_texts)
                elif self.config.primary_model == "hybrid":
                    # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰: SentenceTransformerã‚’ãƒ¡ã‚¤ãƒ³ã«ä½¿ç”¨
                    new_embeddings = self._encode_with_sentence_transformer(uncached_texts)
                else:
                    new_embeddings = self._encode_with_sentence_transformer(uncached_texts)
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                for i, (text, embedding) in enumerate(zip(uncached_texts, new_embeddings)):
                    text_hash = self._get_text_hash(text)
                    self.embedding_cache.text_to_embedding[text_hash] = embedding
                    self.embedding_cache.access_count[text_hash] = 1
                    cached_embeddings.append((uncached_indices[i], embedding))
            
            # çµæœã‚’å…ƒã®é †åºã§ä¸¦ã³æ›¿ãˆ
            cached_embeddings.sort(key=lambda x: x[0])
            final_embeddings = np.array([emb for _, emb in cached_embeddings])
            
        else:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ã§ç›´æ¥ç”Ÿæˆ
            if self.config.primary_model == "openai":
                final_embeddings = self._encode_with_openai(texts)
            else:
                final_embeddings = self._encode_with_sentence_transformer(texts)
        
        return final_embeddings
    
    def extract_piece_semantic_info(self, piece: Any) -> str:
        """
        DirectionPieceã‹ã‚‰æ„å‘³çš„æƒ…å ±ã‚’æŠ½å‡º
        
        Args:
            piece: DirectionPiece (CFS_AVAILABLEã§ãªã„å ´åˆã¯dict)
            
        Returns:
            æŠ½å‡ºã•ã‚ŒãŸæ„å‘³çš„ãƒ†ã‚­ã‚¹ãƒˆ
        """
        semantic_info_parts = []
        
        if hasattr(piece, 'semantic_tags') and piece.semantic_tags:
            semantic_info_parts.extend(piece.semantic_tags)
        elif isinstance(piece, dict) and 'semantic_tags' in piece:
            if isinstance(piece['semantic_tags'], list):
                semantic_info_parts.extend(piece['semantic_tags'])
            else:
                semantic_info_parts.append(str(piece['semantic_tags']))
        
        # context_embeddingã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºï¼ˆå¯èƒ½ãªå ´åˆï¼‰
        if hasattr(piece, 'context_embedding') and hasattr(piece, 'original_text'):
            if hasattr(piece, 'original_text'):
                semantic_info_parts.append(piece.original_text)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ”ãƒ¼ã‚¹IDã‚„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æƒ…å ±
        if not semantic_info_parts:
            if hasattr(piece, 'piece_id'):
                semantic_info_parts.append(f"piece_{piece.piece_id}")
            elif isinstance(piece, dict) and 'id' in piece:
                semantic_info_parts.append(f"piece_{piece['id']}")
            else:
                semantic_info_parts.append("generic_direction_piece")
        
        return " ".join(semantic_info_parts)

def compute_semantic_similarity_rich(
    text_or_vector: Union[str, np.ndarray],
    piece: Any,
    embedding_model: Optional[SemanticSimilarityEngine] = None,
    config: Optional[SemanticSimilarityConfig] = None
) -> float:
    """
    æ„å‘³çš„ã«ãƒªãƒƒãƒãªé¡ä¼¼åº¦ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°
    
    Args:
        text_or_vector: ãƒ¦ãƒ¼ã‚¶å±¥æ­´ãƒ†ã‚­ã‚¹ãƒˆ or ã™ã§ã«å¾—ã‚‰ã‚ŒãŸåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
        piece: è©•ä¾¡å¯¾è±¡ã® DirectionPiece
        embedding_model: SemanticSimilarityEngine ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        config: è¨­å®šï¼ˆembedding_modelãŒNoneã®å ´åˆã«ä½¿ç”¨ï¼‰
        
    Returns:
        float: 0.0-1.0 ã®æ„å‘³çš„é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢
    """
    # åŸ‹ã‚è¾¼ã¿ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–
    if embedding_model is None:
        embedding_model = SemanticSimilarityEngine(config or SemanticSimilarityConfig())
    
    try:
        # ãƒ¦ãƒ¼ã‚¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿å–å¾—
        if isinstance(text_or_vector, str):
            user_embeddings = embedding_model.encode_texts([text_or_vector])
            user_embedding = user_embeddings[0]
        elif isinstance(text_or_vector, np.ndarray):
            user_embedding = text_or_vector
            if embedding_model.config.normalize_embeddings:
                user_embedding = user_embedding / (np.linalg.norm(user_embedding) + 1e-8)
        else:
            logger.error(f"Invalid input type: {type(text_or_vector)}")
            return 0.0
        
        # ãƒ”ãƒ¼ã‚¹ã®æ„å‘³çš„æƒ…å ±æŠ½å‡º
        piece_semantic_text = embedding_model.extract_piece_semantic_info(piece)
        
        # ãƒ”ãƒ¼ã‚¹ã®åŸ‹ã‚è¾¼ã¿å–å¾—
        piece_embeddings = embedding_model.encode_texts([piece_semantic_text])
        piece_embedding = piece_embeddings[0]
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
        # æ¬¡å…ƒãŒç•°ãªã‚‹å ´åˆã®å¯¾å‡¦
        min_dim = min(len(user_embedding), len(piece_embedding))
        user_emb_truncated = user_embedding[:min_dim]
        piece_emb_truncated = piece_embedding[:min_dim]
        
        # æ­£è¦åŒ–
        user_emb_norm = user_emb_truncated / (np.linalg.norm(user_emb_truncated) + 1e-8)
        piece_emb_norm = piece_emb_truncated / (np.linalg.norm(piece_emb_truncated) + 1e-8)
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
        similarity = np.dot(user_emb_norm, piece_emb_norm)
        
        # 0-1ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—
        similarity = max(0.0, min(1.0, (similarity + 1.0) / 2.0))
        
        logger.debug(f"Semantic similarity computed: {similarity:.4f}")
        logger.debug(f"User text: {text_or_vector if isinstance(text_or_vector, str) else 'vector'}")
        logger.debug(f"Piece semantic: {piece_semantic_text}")
        
        return float(similarity)
        
    except Exception as e:
        logger.error(f"âŒ Semantic similarity computation error: {e}")
        return 0.0

def compute_batch_semantic_similarity(
    user_contexts: List[Union[str, np.ndarray]],
    pieces: List[Any],
    embedding_model: Optional[SemanticSimilarityEngine] = None,
    config: Optional[SemanticSimilarityConfig] = None
) -> np.ndarray:
    """
    ãƒãƒƒãƒã§ã®æ„å‘³çš„é¡ä¼¼åº¦è¨ˆç®—
    
    Args:
        user_contexts: ãƒ¦ãƒ¼ã‚¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
        pieces: DirectionPieceãƒªã‚¹ãƒˆ
        embedding_model: SemanticSimilarityEngine ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        config: è¨­å®š
        
    Returns:
        é¡ä¼¼åº¦è¡Œåˆ— (len(user_contexts), len(pieces))
    """
    if embedding_model is None:
        embedding_model = SemanticSimilarityEngine(config or SemanticSimilarityConfig())
    
    logger.info(f"ğŸ”„ Computing batch semantic similarity: {len(user_contexts)} contexts Ã— {len(pieces)} pieces")
    
    try:
        # ãƒ¦ãƒ¼ã‚¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿å–å¾—
        user_texts = []
        user_vectors = []
        
        for ctx in user_contexts:
            if isinstance(ctx, str):
                user_texts.append(ctx)
                user_vectors.append(None)
            else:
                user_texts.append(None)
                user_vectors.append(ctx)
        
        # ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        text_embeddings = []
        if any(t is not None for t in user_texts):
            valid_texts = [t for t in user_texts if t is not None]
            if valid_texts:
                text_embeddings = embedding_model.encode_texts(valid_texts)
        
        # æœ€çµ‚çš„ãªãƒ¦ãƒ¼ã‚¶åŸ‹ã‚è¾¼ã¿é…åˆ—æ§‹ç¯‰
        final_user_embeddings = []
        text_idx = 0
        
        for i, (text, vector) in enumerate(zip(user_texts, user_vectors)):
            if text is not None:
                final_user_embeddings.append(text_embeddings[text_idx])
                text_idx += 1
            else:
                final_user_embeddings.append(vector)
        
        final_user_embeddings = np.array(final_user_embeddings)
        
        # ãƒ”ãƒ¼ã‚¹ã®åŸ‹ã‚è¾¼ã¿å–å¾—
        piece_texts = [embedding_model.extract_piece_semantic_info(piece) for piece in pieces]
        piece_embeddings = embedding_model.encode_texts(piece_texts)
        
        # ãƒãƒƒãƒé¡ä¼¼åº¦è¨ˆç®—
        # æ¬¡å…ƒèª¿æ•´
        min_dim = min(final_user_embeddings.shape[1], piece_embeddings.shape[1])
        user_emb_truncated = final_user_embeddings[:, :min_dim]
        piece_emb_truncated = piece_embeddings[:, :min_dim]
        
        # æ­£è¦åŒ–
        user_emb_norm = user_emb_truncated / (np.linalg.norm(user_emb_truncated, axis=1, keepdims=True) + 1e-8)
        piece_emb_norm = piece_emb_truncated / (np.linalg.norm(piece_emb_truncated, axis=1, keepdims=True) + 1e-8)
        
        # é¡ä¼¼åº¦è¡Œåˆ—è¨ˆç®—
        similarity_matrix = np.dot(user_emb_norm, piece_emb_norm.T)
        
        # 0-1ç¯„å›²ã«æ­£è¦åŒ–
        similarity_matrix = (similarity_matrix + 1.0) / 2.0
        similarity_matrix = np.clip(similarity_matrix, 0.0, 1.0)
        
        logger.info(f"âœ… Batch similarity computation completed")
        logger.info(f"   Similarity range: {similarity_matrix.min():.4f} - {similarity_matrix.max():.4f}")
        logger.info(f"   Average similarity: {similarity_matrix.mean():.4f}")
        
        return similarity_matrix
        
    except Exception as e:
        logger.error(f"âŒ Batch semantic similarity error: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ©ãƒ³ãƒ€ãƒ é¡ä¼¼åº¦
        return np.random.uniform(0.1, 0.9, (len(user_contexts), len(pieces)))

class HybridSimilarityCalculator:
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰é¡ä¼¼åº¦è¨ˆç®—å™¨ï¼ˆè¤‡æ•°æ‰‹æ³•ã®çµ„ã¿åˆã‚ã›ï¼‰"""
    
    def __init__(self, 
                 semantic_engine: SemanticSimilarityEngine,
                 semantic_weight: float = 0.8,
                 vector_weight: float = 0.2):
        """
        åˆæœŸåŒ–
        
        Args:
            semantic_engine: æ„å‘³çš„é¡ä¼¼åº¦ã‚¨ãƒ³ã‚¸ãƒ³
            semantic_weight: æ„å‘³çš„é¡ä¼¼åº¦ã®é‡ã¿
            vector_weight: ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦ã®é‡ã¿
        """
        self.semantic_engine = semantic_engine
        self.semantic_weight = semantic_weight
        self.vector_weight = vector_weight
        
        logger.info("âœ… HybridSimilarityCalculator initialized")
        logger.info(f"   Semantic weight: {semantic_weight}")
        logger.info(f"   Vector weight: {vector_weight}")
    
    def compute_hybrid_similarity(self,
                                text_or_vector: Union[str, np.ndarray],
                                piece: Any) -> float:
        """
        ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰é¡ä¼¼åº¦è¨ˆç®—
        
        Args:
            text_or_vector: ãƒ¦ãƒ¼ã‚¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            piece: DirectionPiece
            
        Returns:
            ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢
        """
        try:
            # æ„å‘³çš„é¡ä¼¼åº¦
            semantic_sim = compute_semantic_similarity_rich(
                text_or_vector, piece, self.semantic_engine
            )
            
            # ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦ï¼ˆå¾“æ¥æ‰‹æ³•ï¼‰
            if isinstance(text_or_vector, str):
                # ãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã¯æ„å‘³çš„é¡ä¼¼åº¦ã®ã¿ä½¿ç”¨
                vector_sim = 0.5  # ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«
            else:
                # ãƒ™ã‚¯ãƒˆãƒ«ã®å ´åˆã¯ç›´æ¥è¨ˆç®—
                if hasattr(piece, 'u_component'):
                    piece_vector = piece.u_component
                elif hasattr(piece, 'vector'):
                    piece_vector = piece.vector
                elif isinstance(piece, dict) and 'u_component' in piece:
                    piece_vector = np.array(piece['u_component'])
                else:
                    piece_vector = np.random.randn(len(text_or_vector))
                
                # æ¬¡å…ƒèª¿æ•´
                min_dim = min(len(text_or_vector), len(piece_vector))
                user_vec = text_or_vector[:min_dim]
                piece_vec = piece_vector[:min_dim]
                
                # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
                dot_product = np.dot(user_vec, piece_vec)
                magnitude = np.linalg.norm(user_vec) * np.linalg.norm(piece_vec)
                vector_sim = dot_product / (magnitude + 1e-8)
                vector_sim = max(0.0, min(1.0, (vector_sim + 1.0) / 2.0))
            
            # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚³ã‚¢
            hybrid_score = (
                self.semantic_weight * semantic_sim + 
                self.vector_weight * vector_sim
            )
            
            logger.debug(f"Hybrid similarity - Semantic: {semantic_sim:.4f}, Vector: {vector_sim:.4f}, Final: {hybrid_score:.4f}")
            
            return hybrid_score
            
        except Exception as e:
            logger.error(f"âŒ Hybrid similarity error: {e}")
            return 0.5

def demonstrate_semantic_similarity():
    """æ„å‘³çš„é¡ä¼¼åº¦è¨ˆç®—ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("ğŸ” æ„å‘³çš„é¡ä¼¼åº¦è¨ˆç®—ã‚¨ãƒ³ã‚¸ãƒ³ãƒ‡ãƒ¢")
    print("=" * 60)
    
    # è¨­å®šã¨ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–
    config = SemanticSimilarityConfig(
        primary_model="sentence-transformers",
        model_name="all-MiniLM-L6-v2",
        cache_embeddings=True
    )
    
    engine = SemanticSimilarityEngine(config)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    user_contexts = [
        "æœ€è¿‘ã®SFå°èª¬ã«ã¤ã„ã¦èªã‚ŠãŸã„",
        "æ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’æ•™ãˆã¦ãã ã•ã„", 
        "æ˜ ç”»ã®æ„Ÿæƒ³ã‚’èã‹ã›ã¦",
        "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®è³ªå•ãŒã‚ã‚Šã¾ã™"
    ]
    
    # ãƒ¢ãƒƒã‚¯DirectionPieces
    mock_pieces = [
        {"id": "sci_fi", "semantic_tags": ["science_fiction", "literature"], "u_component": np.random.randn(768)},
        {"id": "cooking", "semantic_tags": ["cooking", "recipes"], "u_component": np.random.randn(768)},
        {"id": "movies", "semantic_tags": ["entertainment", "films"], "u_component": np.random.randn(768)},
        {"id": "tech", "semantic_tags": ["programming", "technology"], "u_component": np.random.randn(768)}
    ]
    
    print(f"\nğŸ“ Testing individual similarity computation:")
    print("-" * 40)
    
    # å€‹åˆ¥é¡ä¼¼åº¦è¨ˆç®—ãƒ†ã‚¹ãƒˆ
    for i, context in enumerate(user_contexts):
        print(f"\nğŸ”¸ User Context: '{context}'")
        
        for j, piece in enumerate(mock_pieces):
            similarity = compute_semantic_similarity_rich(context, piece, engine)
            piece_info = f"Piece {piece['id']} ({', '.join(piece['semantic_tags'])})"
            print(f"   vs {piece_info:<30}: {similarity:.4f}")
    
    print(f"\n\nğŸ”„ Testing batch similarity computation:")
    print("-" * 40)
    
    # ãƒãƒƒãƒé¡ä¼¼åº¦è¨ˆç®—ãƒ†ã‚¹ãƒˆ
    similarity_matrix = compute_batch_semantic_similarity(
        user_contexts, mock_pieces, engine
    )
    
    print(f"\nSimilarity Matrix ({len(user_contexts)}Ã—{len(mock_pieces)}):")
    print("Context\\Piece", end="")
    for piece in mock_pieces:
        print(f"{piece['id'][:8]:>10}", end="")
    print()
    
    for i, context in enumerate(user_contexts):
        context_short = context[:15] + "..." if len(context) > 15 else context
        print(f"{context_short:<13}", end="")
        for j in range(len(mock_pieces)):
            print(f"{similarity_matrix[i,j]:>10.4f}", end="")
        print()
    
    print(f"\n\nğŸ”— Testing hybrid similarity:")
    print("-" * 40)
    
    # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰é¡ä¼¼åº¦ãƒ†ã‚¹ãƒˆ
    hybrid_calc = HybridSimilarityCalculator(engine, semantic_weight=0.7, vector_weight=0.3)
    
    context = user_contexts[0]  # "æœ€è¿‘ã®SFå°èª¬ã«ã¤ã„ã¦èªã‚ŠãŸã„"
    piece = mock_pieces[0]      # sci_fi piece
    
    hybrid_sim = hybrid_calc.compute_hybrid_similarity(context, piece)
    print(f"Hybrid similarity for '{context}' vs sci_fi piece: {hybrid_sim:.4f}")
    
    print(f"\nğŸ“Š Cache statistics:")
    print(f"   Cached embeddings: {len(engine.embedding_cache.text_to_embedding)}")
    print(f"   Cache hits: {sum(engine.embedding_cache.access_count.values())}")
    
    print("\nğŸ‰ æ„å‘³çš„é¡ä¼¼åº¦è¨ˆç®—ãƒ‡ãƒ¢å®Œäº†!")

if __name__ == "__main__":
    demonstrate_semantic_similarity()