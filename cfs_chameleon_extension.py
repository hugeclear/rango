#!/usr/bin/env python3
"""
CFS-Chameleon Extension: Collaborative Feature Sharing Integration
ä¸–ç•Œåˆã®è»½é‡å­¦ç¿’å”èª¿çš„åŸ‹ã‚è¾¼ã¿ç·¨é›†ã‚·ã‚¹ãƒ†ãƒ 

ç‰¹å¾´:
- æ—¢å­˜Chameleonå®Ÿè£…ã®å®Œå…¨äº’æ›æ€§ä¿æŒ
- æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«åˆ†è§£ãƒ»ãƒ—ãƒ¼ãƒ«åŒ–ãƒ»å”èª¿çš„é¸æŠ
- ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·ä¸‹ã§ã®å”èª¿å­¦ç¿’
- ãƒ•ãƒ©ã‚°ã«ã‚ˆã‚‹å”èª¿æ©Ÿèƒ½ã®ON/OFFåˆ¶å¾¡
"""

import json
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Any, Optional, Union
from dataclasses import dataclass
from pathlib import Path
import logging
import time
from collections import defaultdict
import hashlib

logger = logging.getLogger(__name__)

@dataclass
class DirectionPiece:
    """åˆ†è§£ã•ã‚ŒãŸæ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒ”ãƒ¼ã‚¹"""
    u_component: np.ndarray      # Uæˆåˆ† (å·¦ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«)
    v_component: np.ndarray      # Væˆåˆ† (å³ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«)  
    singular_value: float        # ç‰¹ç•°å€¤
    importance: float            # é‡è¦åº¦ã‚¹ã‚³ã‚¢
    semantic_tag: str            # æ„å‘³çš„ã‚¿ã‚°
    user_id: str                 # è²¢çŒ®ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
    quality_score: float         # å“è³ªã‚¹ã‚³ã‚¢
    creation_time: float         # ä½œæˆæ™‚åˆ»
    usage_count: int             # ä½¿ç”¨å›æ•°

@dataclass
class UserContext:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±"""
    user_id: str
    preference_vector: np.ndarray  # ãƒ¦ãƒ¼ã‚¶ãƒ¼å—œå¥½ãƒ™ã‚¯ãƒˆãƒ«
    history_embedding: np.ndarray  # å±¥æ­´åŸ‹ã‚è¾¼ã¿
    activity_level: float          # ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãƒ¬ãƒ™ãƒ«
    similarity_cache: Dict[str, float]  # é¡ä¼¼åº¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥

class CollaborativeDirectionPool:
    """
    å”èª¿çš„æ–¹å‘ãƒ—ãƒ¼ãƒ«
    
    æ©Ÿèƒ½:
    1. å€‹äººæ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ã®åˆ†è§£ãƒ»ä¿å­˜
    2. æ„å‘³çš„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
    3. é«˜é€Ÿé¡ä¼¼åº¦æ¤œç´¢
    4. ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·æ©Ÿæ§‹
    """
    
    def __init__(self, pool_size: int = 1000, rank_reduction: int = 32):
        self.pool_size = pool_size
        self.rank_reduction = rank_reduction
        
        # ãƒ”ãƒ¼ã‚¹ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
        self.pieces: List[DirectionPiece] = []
        self.piece_index: Dict[str, int] = {}  # ãƒãƒƒã‚·ãƒ¥ â†’ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        self.user_contexts: Dict[str, UserContext] = {}
        
        # æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        self.semantic_index = defaultdict(list)  # ã‚¿ã‚° â†’ ãƒ”ãƒ¼ã‚¹ãƒªã‚¹ãƒˆ
        self.similarity_matrix = None  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨é¡ä¼¼åº¦è¡Œåˆ—
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'total_contributions': 0,
            'active_users': 0,
            'avg_quality_score': 0.0,
            'cache_hit_rate': 0.0
        }
        
        logger.info(f"CollaborativeDirectionPool initialized (capacity: {pool_size}, rank: {rank_reduction})")
    
    def add_direction_vector(self, direction_vector: np.ndarray, user_id: str, 
                           semantic_context: str = "") -> List[DirectionPiece]:
        """
        å€‹äººæ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ã‚’åˆ†è§£ã—ã¦ãƒ—ãƒ¼ãƒ«ã«è¿½åŠ 
        
        Args:
            direction_vector: å€‹äººæ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«
            user_id: è²¢çŒ®ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            semantic_context: æ„å‘³çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ”ãƒ¼ã‚¹ãƒªã‚¹ãƒˆ
        """
        # SVDåˆ†è§£ã«ã‚ˆã‚‹æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«åˆ†å‰²
        pieces = self._decompose_direction_vector(direction_vector, user_id, semantic_context)
        
        # ãƒ—ãƒ¼ãƒ«ã«è¿½åŠ 
        for piece in pieces:
            if len(self.pieces) < self.pool_size:
                self.pieces.append(piece)
                piece_hash = self._compute_piece_hash(piece)
                self.piece_index[piece_hash] = len(self.pieces) - 1
                
                # æ„å‘³çš„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°
                self.semantic_index[piece.semantic_tag].append(len(self.pieces) - 1)
            else:
                # ãƒ—ãƒ¼ãƒ«æº€æ¯æ™‚ã¯å“è³ªã‚¹ã‚³ã‚¢ã«åŸºã¥ã„ã¦ç½®æ›
                self._replace_lowest_quality_piece(piece)
        
        # çµ±è¨ˆæ›´æ–°
        self._update_statistics(user_id)
        
        logger.info(f"Added {len(pieces)} pieces from user {user_id}")
        return pieces
    
    def _decompose_direction_vector(self, direction_vector: np.ndarray, user_id: str, 
                                  semantic_context: str) -> List[DirectionPiece]:
        """SVDåˆ†è§£ã«ã‚ˆã‚‹æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«åˆ†å‰²"""
        try:
            # æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¡Œåˆ—ã«å¤‰å½¢ï¼ˆè¤‡æ•°ã®åŸºåº•ã‚’æƒ³å®šï¼‰
            if len(direction_vector.shape) == 1:
                # 1Dãƒ™ã‚¯ãƒˆãƒ«ã®å ´åˆã€å¤–ç©è¡Œåˆ—ã‚’ä½œæˆ
                matrix = np.outer(direction_vector, direction_vector)
            else:
                matrix = direction_vector
            
            # SVDåˆ†è§£å®Ÿè¡Œ
            U, S, Vt = np.linalg.svd(matrix, full_matrices=False)
            
            # ãƒ©ãƒ³ã‚¯å‰Šæ¸›
            rank = min(self.rank_reduction, len(S))
            U_reduced = U[:, :rank]
            S_reduced = S[:rank]
            Vt_reduced = Vt[:rank, :]
            
            pieces = []
            for i in range(rank):
                # é‡è¦åº¦è¨ˆç®—ï¼ˆç‰¹ç•°å€¤ãƒ™ãƒ¼ã‚¹ï¼‰
                importance = S_reduced[i] / np.sum(S_reduced)
                
                # æ„å‘³çš„ã‚¿ã‚°ç”Ÿæˆ
                semantic_tag = self._generate_semantic_tag(U_reduced[:, i], semantic_context)
                
                # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
                quality_score = self._calculate_quality_score(U_reduced[:, i], S_reduced[i])
                
                piece = DirectionPiece(
                    u_component=U_reduced[:, i],
                    v_component=Vt_reduced[i, :],
                    singular_value=S_reduced[i],
                    importance=importance,
                    semantic_tag=semantic_tag,
                    user_id=user_id,
                    quality_score=quality_score,
                    creation_time=time.time(),
                    usage_count=0
                )
                pieces.append(piece)
            
            return pieces
            
        except Exception as e:
            logger.error(f"Direction decomposition failed: {e}")
            return []
    
    def _generate_semantic_tag(self, component: np.ndarray, context: str) -> str:
        """æ„å‘³çš„ã‚¿ã‚°ã®è‡ªå‹•ç”Ÿæˆ"""
        # æˆåˆ†ã®ç‰¹å¾´ã«åŸºã¥ã„ã¦ã‚¿ã‚°ç”Ÿæˆ
        norm = np.linalg.norm(component)
        sparsity = np.sum(np.abs(component) < 0.01) / len(component)
        
        if norm > 0.8:
            tag = "high_impact"
        elif norm > 0.5:
            tag = "medium_impact"
        else:
            tag = "subtle_adjustment"
        
        if sparsity > 0.7:
            tag += "_sparse"
        elif sparsity < 0.3:
            tag += "_dense"
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’è¿½åŠ 
        if context:
            context_hash = hashlib.md5(context.encode()).hexdigest()[:8]
            tag += f"_{context_hash}"
        
        return tag
    
    def _calculate_quality_score(self, component: np.ndarray, singular_value: float) -> float:
        """å“è³ªã‚¹ã‚³ã‚¢ã®çµ±è¨ˆçš„è¨ˆç®—"""
        # è¤‡æ•°ã®å“è³ªæŒ‡æ¨™ã‚’çµ„ã¿åˆã‚ã›
        norm_score = min(np.linalg.norm(component), 1.0)
        singular_score = min(singular_value, 1.0)
        stability_score = 1.0 - (np.std(component) / (np.mean(np.abs(component)) + 1e-8))
        
        return (norm_score + singular_score + stability_score) / 3.0
    
    def select_collaborative_pieces(self, user_context: UserContext, query_embedding: np.ndarray, 
                                  top_k: int = 10, strategy: str = "analytical") -> List[DirectionPiece]:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã‚¯ã‚¨ãƒªã«åŸºã¥ãæœ€é©ãƒ”ãƒ¼ã‚¹é¸æŠ
        
        Args:
            user_context: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            query_embedding: ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿
            top_k: é¸æŠã™ã‚‹ãƒ”ãƒ¼ã‚¹æ•°
            strategy: é¸æŠæˆ¦ç•¥ ("analytical", "learned")
            
        Returns:
            é¸æŠã•ã‚ŒãŸãƒ”ãƒ¼ã‚¹ãƒªã‚¹ãƒˆ
        """
        if not self.pieces:
            return []
        
        if strategy == "analytical":
            return self._analytical_selection(user_context, query_embedding, top_k)
        elif strategy == "learned":
            return self._learned_selection(user_context, query_embedding, top_k)
        else:
            raise ValueError(f"Unknown selection strategy: {strategy}")
    
    def _analytical_selection(self, user_context: UserContext, query_embedding: np.ndarray, 
                            top_k: int) -> List[DirectionPiece]:
        """è§£æçš„é¸æŠã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆå­¦ç¿’ãªã—ï¼‰"""
        scores = []
        
        for piece in self.pieces:
            # 1. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦
            context_sim = self._compute_context_similarity(user_context, piece)
            
            # 2. æ„å‘³çš„é–¢é€£æ€§
            semantic_sim = self._compute_semantic_similarity(query_embedding, piece)
            
            # 3. å“è³ªã‚¹ã‚³ã‚¢
            quality_score = piece.quality_score
            
            # 4. ãƒ¦ãƒ¼ã‚¶ãƒ¼é¡ä¼¼åº¦ï¼ˆå”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çš„ï¼‰
            user_sim = self._compute_user_similarity(user_context.user_id, piece.user_id)
            
            # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆé‡ã¿ä»˜ãå’Œï¼‰
            total_score = (0.3 * context_sim + 0.3 * semantic_sim + 
                          0.2 * quality_score + 0.2 * user_sim)
            
            scores.append((total_score, piece))
        
        # Top-Ké¸æŠ
        scores.sort(key=lambda x: x[0], reverse=True)
        selected_pieces = [piece for _, piece in scores[:top_k]]
        
        # ä½¿ç”¨å›æ•°æ›´æ–°
        for piece in selected_pieces:
            piece.usage_count += 1
        
        return selected_pieces
    
    def _compute_context_similarity(self, user_context: UserContext, piece: DirectionPiece) -> float:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦è¨ˆç®—"""
        try:
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼å—œå¥½ãƒ™ã‚¯ãƒˆãƒ«ã¨ãƒ”ãƒ¼ã‚¹æˆåˆ†ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
            if len(user_context.preference_vector) != len(piece.u_component):
                return 0.0
            
            dot_product = np.dot(user_context.preference_vector, piece.u_component)
            norm_product = (np.linalg.norm(user_context.preference_vector) * 
                          np.linalg.norm(piece.u_component))
            
            if norm_product == 0:
                return 0.0
            
            return abs(dot_product / norm_product)
        except:
            return 0.0
    
    def _compute_semantic_similarity(self, query_embedding: np.ndarray, piece: DirectionPiece) -> float:
        """æ„å‘³çš„é–¢é€£æ€§è¨ˆç®—"""
        try:
            # ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ã¨ãƒ”ãƒ¼ã‚¹æˆåˆ†ã®é¡ä¼¼åº¦
            if len(query_embedding) != len(piece.u_component):
                return 0.0
            
            dot_product = np.dot(query_embedding, piece.u_component)
            norm_product = np.linalg.norm(query_embedding) * np.linalg.norm(piece.u_component)
            
            if norm_product == 0:
                return 0.0
            
            return abs(dot_product / norm_product)
        except:
            return 0.0
    
    def _compute_user_similarity(self, user_id1: str, user_id2: str) -> float:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼é¡ä¼¼åº¦è¨ˆç®—ï¼ˆå”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰"""
        if user_id1 == user_id2:
            return 1.0
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
        cache_key = f"{user_id1}_{user_id2}"
        if user_id1 in self.user_contexts and cache_key in self.user_contexts[user_id1].similarity_cache:
            return self.user_contexts[user_id1].similarity_cache[cache_key]
        
        # ç°¡å˜ãªæ–‡å­—åˆ—é¡ä¼¼åº¦ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ã‚ˆã‚Šè¤‡é›‘ãªæ‰‹æ³•ã‚’ä½¿ç”¨ï¼‰
        similarity = 1.0 - (abs(hash(user_id1)) % 1000 - abs(hash(user_id2)) % 1000) / 1000.0
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        if user_id1 in self.user_contexts:
            self.user_contexts[user_id1].similarity_cache[cache_key] = similarity
        
        return max(0.0, similarity)
    
    def fuse_selected_directions(self, selected_pieces: List[DirectionPiece], 
                               fusion_strategy: str = "analytical") -> np.ndarray:
        """
        é¸æŠã•ã‚ŒãŸæ–¹å‘ãƒ”ãƒ¼ã‚¹ã‚’çµ±åˆã—ã¦æœ€çµ‚çš„ãªå”èª¿æ–¹å‘ã‚’ç”Ÿæˆ
        
        Args:
            selected_pieces: é¸æŠã•ã‚ŒãŸãƒ”ãƒ¼ã‚¹ãƒªã‚¹ãƒˆ
            fusion_strategy: çµ±åˆæˆ¦ç•¥
            
        Returns:
            çµ±åˆã•ã‚ŒãŸå”èª¿æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«
        """
        if not selected_pieces:
            return np.zeros(768)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¬¡å…ƒ
        
        if fusion_strategy == "analytical":
            return self._analytical_fusion(selected_pieces)
        elif fusion_strategy == "weighted_attention":
            return self._attention_fusion(selected_pieces)
        else:
            raise ValueError(f"Unknown fusion strategy: {fusion_strategy}")
    
    def _analytical_fusion(self, pieces: List[DirectionPiece]) -> np.ndarray:
        """è§£æçš„çµ±åˆï¼ˆé‡ã¿ä»˜ãå¹³å‡ï¼‰"""
        if not pieces:
            return np.zeros(768)
        
        # é‡è¦åº¦ã«åŸºã¥ãé‡ã¿è¨ˆç®—
        weights = np.array([piece.importance * piece.quality_score for piece in pieces])
        weights = weights / np.sum(weights)  # æ­£è¦åŒ–
        
        # é‡ã¿ä»˜ãå¹³å‡ã«ã‚ˆã‚‹çµ±åˆ
        fused_direction = np.zeros_like(pieces[0].u_component)
        for i, piece in enumerate(pieces):
            fused_direction += weights[i] * piece.u_component
        
        # æ­£è¦åŒ–
        norm = np.linalg.norm(fused_direction)
        if norm > 0:
            fused_direction = fused_direction / norm
        
        return fused_direction
    
    def _attention_fusion(self, pieces: List[DirectionPiece]) -> np.ndarray:
        """ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹ã«ã‚ˆã‚‹çµ±åˆ"""
        if not pieces:
            return np.zeros(768)
        
        # ç°¡å˜ãªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿è¨ˆç®—
        attention_scores = []
        for piece in pieces:
            # å“è³ªã‚¹ã‚³ã‚¢ã¨é‡è¦åº¦ã®çµ„ã¿åˆã‚ã›
            score = piece.quality_score * piece.importance
            attention_scores.append(score)
        
        # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹æ­£è¦åŒ–
        attention_weights = np.exp(attention_scores)
        attention_weights = attention_weights / np.sum(attention_weights)
        
        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ä»˜ãçµ±åˆ
        fused_direction = np.zeros_like(pieces[0].u_component)
        for i, piece in enumerate(pieces):
            fused_direction += attention_weights[i] * piece.u_component
        
        # æ­£è¦åŒ–
        norm = np.linalg.norm(fused_direction)
        if norm > 0:
            fused_direction = fused_direction / norm
        
        return fused_direction
    
    def _compute_piece_hash(self, piece: DirectionPiece) -> str:
        """ãƒ”ãƒ¼ã‚¹ã®ãƒãƒƒã‚·ãƒ¥å€¤è¨ˆç®—ï¼ˆé‡è¤‡æ¤œå‡ºç”¨ï¼‰"""
        content = f"{piece.user_id}_{piece.semantic_tag}_{piece.singular_value:.6f}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _replace_lowest_quality_piece(self, new_piece: DirectionPiece):
        """æœ€ä½å“è³ªãƒ”ãƒ¼ã‚¹ã‚’æ–°ã—ã„ãƒ”ãƒ¼ã‚¹ã§ç½®æ›"""
        if not self.pieces:
            return
        
        # æœ€ä½å“è³ªãƒ”ãƒ¼ã‚¹ã‚’ç‰¹å®š
        min_quality_idx = min(range(len(self.pieces)), 
                            key=lambda i: self.pieces[i].quality_score)
        
        if new_piece.quality_score > self.pieces[min_quality_idx].quality_score:
            # å¤ã„ãƒ”ãƒ¼ã‚¹ã‚’å‰Šé™¤
            old_piece = self.pieces[min_quality_idx]
            old_hash = self._compute_piece_hash(old_piece)
            
            # æ–°ã—ã„ãƒ”ãƒ¼ã‚¹ã§ç½®æ›
            self.pieces[min_quality_idx] = new_piece
            new_hash = self._compute_piece_hash(new_piece)
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°
            if old_hash in self.piece_index:
                del self.piece_index[old_hash]
            self.piece_index[new_hash] = min_quality_idx
            
            # æ„å‘³çš„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°
            self.semantic_index[old_piece.semantic_tag].remove(min_quality_idx)
            self.semantic_index[new_piece.semantic_tag].append(min_quality_idx)
    
    def _update_statistics(self, user_id: str):
        """çµ±è¨ˆæƒ…å ±æ›´æ–°"""
        self.stats['total_contributions'] += 1
        
        if user_id not in self.user_contexts:
            self.stats['active_users'] += 1
        
        # å¹³å‡å“è³ªã‚¹ã‚³ã‚¢æ›´æ–°
        if self.pieces:
            total_quality = sum(piece.quality_score for piece in self.pieces)
            self.stats['avg_quality_score'] = total_quality / len(self.pieces)
    
    def get_statistics(self) -> Dict[str, Any]:
        """ãƒ—ãƒ¼ãƒ«çµ±è¨ˆæƒ…å ±å–å¾—"""
        return {
            **self.stats,
            'pool_utilization': len(self.pieces) / self.pool_size,
            'unique_semantic_tags': len(self.semantic_index),
            'avg_piece_usage': np.mean([piece.usage_count for piece in self.pieces]) if self.pieces else 0
        }
    
    def get_collaborative_directions(self, user_context: UserContext, query_embedding: np.ndarray) -> Dict[str, np.ndarray]:
        """
        å”èª¿çš„æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ
        
        Args:
            user_context: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            query_embedding: ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿
            
        Returns:
            'personal'ã¨'neutral'æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«ã®è¾æ›¸
        """
        try:
            # æœ€é©ãƒ”ãƒ¼ã‚¹é¸æŠ
            selected_pieces = self.select_collaborative_pieces(user_context, query_embedding, top_k=8)
            
            if not selected_pieces:
                logger.warning(f"No collaborative pieces found for user {user_context.user_id}")
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ–¹å‘ã‚’è¿”ã™ï¼ˆã‚¨ãƒ©ãƒ¼ã§ãªãè­¦å‘Šãƒ¬ãƒ™ãƒ«ï¼‰
                return {
                    'personal': user_context.preference_vector[:min(len(user_context.preference_vector), 3072)],
                    'neutral': np.zeros(3072, dtype=np.float32)
                }
            
            # å”èª¿çš„æ–¹å‘çµ±åˆ
            collaborative_personal = self._attention_fusion(selected_pieces)
            
            # å€‹äººæ–¹å‘ã¨ã®ãƒ–ãƒ¬ãƒ³ãƒ‰ï¼ˆ70%å”èª¿ã€30%å€‹äººï¼‰
            personal_component = user_context.preference_vector[:min(len(user_context.preference_vector), len(collaborative_personal))]
            if len(personal_component) < len(collaborative_personal):
                # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                padded_personal = np.zeros_like(collaborative_personal)
                padded_personal[:len(personal_component)] = personal_component
                personal_component = padded_personal
            
            blended_personal = 0.7 * collaborative_personal + 0.3 * personal_component
            
            # ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æ–¹å‘ç”Ÿæˆï¼ˆé€†æ–¹å‘ï¼‰
            collaborative_neutral = -0.5 * blended_personal + 0.3 * np.random.randn(*blended_personal.shape) * 0.1
            
            # æ­£è¦åŒ–
            if np.linalg.norm(blended_personal) > 0:
                blended_personal = blended_personal / np.linalg.norm(blended_personal)
            if np.linalg.norm(collaborative_neutral) > 0:
                collaborative_neutral = collaborative_neutral / np.linalg.norm(collaborative_neutral)
            
            # 3072æ¬¡å…ƒã«èª¿æ•´ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
            if len(blended_personal) != 3072:
                if len(blended_personal) > 3072:
                    blended_personal = blended_personal[:3072]
                    collaborative_neutral = collaborative_neutral[:3072]
                else:
                    padded_personal = np.zeros(3072, dtype=np.float32)
                    padded_neutral = np.zeros(3072, dtype=np.float32)
                    padded_personal[:len(blended_personal)] = blended_personal
                    padded_neutral[:len(collaborative_neutral)] = collaborative_neutral
                    blended_personal = padded_personal
                    collaborative_neutral = padded_neutral
            
            logger.debug(f"Generated collaborative directions for user {user_context.user_id}: P={blended_personal.shape}, N={collaborative_neutral.shape}")
            
            return {
                'personal': blended_personal.astype(np.float32),
                'neutral': collaborative_neutral.astype(np.float32)
            }
            
        except Exception as e:
            logger.error(f"Failed to generate collaborative directions: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ—¢å­˜æ–¹å‘ã‚’è¿”ã™
            return {
                'personal': user_context.preference_vector[:min(len(user_context.preference_vector), 3072)],
                'neutral': np.zeros(3072, dtype=np.float32)
            }
    
    def save_pool(self, filepath: str):
        """ãƒ—ãƒ¼ãƒ«ã®çŠ¶æ…‹ã‚’ä¿å­˜"""
        pool_data = {
            'pieces': [],
            'user_contexts': {},
            'stats': self.stats,
            'config': {
                'pool_size': self.pool_size,
                'rank_reduction': self.rank_reduction
            }
        }
        
        # ãƒ”ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
        for piece in self.pieces:
            piece_data = {
                'u_component': piece.u_component.tolist(),
                'v_component': piece.v_component.tolist(),
                'singular_value': piece.singular_value,
                'importance': piece.importance,
                'semantic_tag': piece.semantic_tag,
                'user_id': piece.user_id,
                'quality_score': piece.quality_score,
                'creation_time': piece.creation_time,
                'usage_count': piece.usage_count
            }
            pool_data['pieces'].append(piece_data)
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
        for user_id, context in self.user_contexts.items():
            context_data = {
                'user_id': context.user_id,
                'preference_vector': context.preference_vector.tolist(),
                'history_embedding': context.history_embedding.tolist(),
                'activity_level': context.activity_level,
                'similarity_cache': context.similarity_cache
            }
            pool_data['user_contexts'][user_id] = context_data
        
        with open(filepath, 'w') as f:
            json.dump(pool_data, f, indent=2)
        
        logger.info(f"Pool saved to {filepath}")
    
    def load_pool(self, filepath: str):
        """ä¿å­˜ã•ã‚ŒãŸãƒ—ãƒ¼ãƒ«çŠ¶æ…‹ã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(filepath, 'r') as f:
                pool_data = json.load(f)
            
            # è¨­å®šå¾©å…ƒ
            config = pool_data.get('config', {})
            self.pool_size = config.get('pool_size', 1000)
            self.rank_reduction = config.get('rank_reduction', 32)
            
            # ãƒ”ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å¾©å…ƒ
            self.pieces = []
            self.piece_index = {}
            self.semantic_index = defaultdict(list)
            
            for i, piece_data in enumerate(pool_data['pieces']):
                piece = DirectionPiece(
                    u_component=np.array(piece_data['u_component']),
                    v_component=np.array(piece_data['v_component']),
                    singular_value=piece_data['singular_value'],
                    importance=piece_data['importance'],
                    semantic_tag=piece_data['semantic_tag'],
                    user_id=piece_data['user_id'],
                    quality_score=piece_data['quality_score'],
                    creation_time=piece_data['creation_time'],
                    usage_count=piece_data['usage_count']
                )
                self.pieces.append(piece)
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å¾©å…ƒ
                piece_hash = self._compute_piece_hash(piece)
                self.piece_index[piece_hash] = i
                self.semantic_index[piece.semantic_tag].append(i)
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¾©å…ƒ
            self.user_contexts = {}
            for user_id, context_data in pool_data.get('user_contexts', {}).items():
                context = UserContext(
                    user_id=context_data['user_id'],
                    preference_vector=np.array(context_data['preference_vector']),
                    history_embedding=np.array(context_data['history_embedding']),
                    activity_level=context_data['activity_level'],
                    similarity_cache=context_data['similarity_cache']
                )
                self.user_contexts[user_id] = context
            
            # çµ±è¨ˆå¾©å…ƒ
            self.stats = pool_data.get('stats', {})
            
            logger.info(f"Pool loaded from {filepath} ({len(self.pieces)} pieces, {len(self.user_contexts)} users)")
            
        except Exception as e:
            logger.error(f"Failed to load pool: {e}")

class LightweightGateNetwork(nn.Module):
    """è»½é‡ã‚²ãƒ¼ãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³å­¦ç¿’ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼‰"""
    
    def __init__(self, embedding_dim: int = 768, num_directions: int = 200, hidden_dim: int = 256):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.num_directions = num_directions
        
        # è»½é‡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹æˆ
        self.gate_network = nn.Sequential(
            nn.Linear(embedding_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, num_directions),
            nn.Sigmoid()
        )
        
        # ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: ~200K (768*256 + 256*200 = ~250K)
        logger.info(f"LightweightGateNetwork initialized ({self._count_parameters()} parameters)")
    
    def forward(self, user_embedding: torch.Tensor) -> torch.Tensor:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼åŸ‹ã‚è¾¼ã¿ã‹ã‚‰æ–¹å‘é¸æŠã‚²ãƒ¼ãƒˆã‚’è¨ˆç®—
        
        Args:
            user_embedding: ãƒ¦ãƒ¼ã‚¶ãƒ¼åŸ‹ã‚è¾¼ã¿ (batch_size, embedding_dim)
            
        Returns:
            æ–¹å‘é¸æŠã‚²ãƒ¼ãƒˆ (batch_size, num_directions)
        """
        return self.gate_network(user_embedding)
    
    def _count_parameters(self) -> int:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

if __name__ == "__main__":
    # åŸºæœ¬å‹•ä½œãƒ†ã‚¹ãƒˆ
    print("ğŸ”§ CFS-Chameleon Extension Test")
    
    # ãƒ—ãƒ¼ãƒ«åˆæœŸåŒ–
    pool = CollaborativeDirectionPool(pool_size=100, rank_reduction=16)
    
    # ã‚µãƒ³ãƒ—ãƒ«æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«è¿½åŠ 
    sample_direction = np.random.randn(768)
    pieces = pool.add_direction_vector(sample_direction, "user_001", "movie_preferences")
    
    print(f"Generated {len(pieces)} pieces")
    print(f"Pool statistics: {pool.get_statistics()}")
    
    # è»½é‡ã‚²ãƒ¼ãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ
    gate_net = LightweightGateNetwork()
    sample_embedding = torch.randn(1, 768)
    gates = gate_net(sample_embedding)
    
    print(f"Gate network output shape: {gates.shape}")
    print("âœ… CFS-Chameleon Extension test completed")