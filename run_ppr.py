#!/usr/bin/env python3
"""
GraphRAGå‹•çš„CFS-Chameleonå‘ã‘Personalized PageRank (PPR) ã‚·ã‚¹ãƒ†ãƒ  (Step-3)
Step-2ã®ç–ã‚°ãƒ©ãƒ•ã‚¨ãƒƒã‚¸ã‹ã‚‰CSRè¡Œåˆ—ã‚’æ§‹ç¯‰ã—ã€Power Iterationã§è¿‘ä¼¼PPRã‚’è¨ˆç®—

## æ©Ÿèƒ½:
- å…¥åŠ›: Step-2ã®graph_edges.parquetï¼ˆsrc_id, dst_id, scoreï¼‰
- å‰å‡¦ç†: IDå†ãƒãƒƒãƒ”ãƒ³ã‚°ã€é‡è¤‡ã‚¨ãƒƒã‚¸çµ±åˆã€è‡ªå·±ãƒ«ãƒ¼ãƒ—é™¤å»
- CSRç–éš£æ¥è¡Œåˆ—æ§‹ç¯‰ã€è¡Œæ­£è¦åŒ–ã§ç¢ºç‡é·ç§»è¡Œåˆ—åŒ–
- Power Iterationã«ã‚ˆã‚‹PPRè¿‘ä¼¼ï¼ˆp = Î±Â·e + (1-Î±)Â·Páµ€Â·pï¼‰
- åæŸåˆ¤å®šã€ä¸Šä½Lä¿æŒã€è¤‡æ•°ã‚·ãƒ¼ãƒ‰å¯¾å¿œ

## å¯¾è±¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯:
- LaMP-2ï¼ˆç”Ÿæˆã‚¿ã‚¹ã‚¯ï¼‰+ Tenrecï¼ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ï¼‰
- GraphRAGå‹•çš„CFS-Chameleonã®å”èª¿ãƒ¦ãƒ¼ã‚¶ãƒ¼é¸æŠ
"""

import os
import sys
import json
import argparse
import warnings
import time
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Union
import numpy as np
import pandas as pd
import logging
from tqdm import tqdm

# ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯
try:
    from scipy.sparse import csr_matrix, save_npz, load_npz
    from scipy.sparse.csgraph import connected_components
    import pyarrow.parquet as pq
    DEPENDENCIES_OK = True
except ImportError as e:
    print(f"âŒ ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒä¸è¶³: {e}")
    print("pip install numpy scipy pandas pyarrow tqdm ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    sys.exit(1)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# è­¦å‘ŠæŠ‘åˆ¶
warnings.filterwarnings("ignore", category=FutureWarning)

class PPRCalculator:
    """Personalized PageRank è¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, 
                 alpha: float = 0.15,
                 eps: float = 1e-6,
                 max_iter: int = 50,
                 normalize_edge: bool = True,
                 undirected: bool = False,
                 seed: int = 42):
        """
        åˆæœŸåŒ–
        
        Args:
            alpha: å†å§‹å‹•ç¢ºç‡ï¼ˆå…¸å‹çš„ã«0.1-0.2ï¼‰
            eps: åæŸåˆ¤å®šé–¾å€¤
            max_iter: æœ€å¤§åå¾©å›æ•°
            normalize_edge: ã‚¨ãƒƒã‚¸é‡ã¿è¡Œæ­£è¦åŒ–ã™ã‚‹ã‹
            undirected: ç„¡å‘ã‚°ãƒ©ãƒ•åŒ–ã™ã‚‹ã‹
            seed: ä¹±æ•°ã‚·ãƒ¼ãƒ‰
        """
        self.alpha = alpha
        self.eps = eps
        self.max_iter = max_iter
        self.normalize_edge = normalize_edge
        self.undirected = undirected
        self.seed = seed
        
        # ä¹±æ•°ã‚·ãƒ¼ãƒ‰å›ºå®š
        np.random.seed(seed)
        
        # å†…éƒ¨çŠ¶æ…‹
        self.adj_matrix = None
        self.id_to_idx = None
        self.idx_to_id = None
        self.n_nodes = 0
        
        logger.info(f"ğŸ¯ PPRCalculatoråˆæœŸåŒ–")
        logger.info(f"   å†å§‹å‹•ç¢ºç‡: {alpha}")
        logger.info(f"   åæŸé–¾å€¤: {eps}")
        logger.info(f"   æœ€å¤§åå¾©: {max_iter}")
        logger.info(f"   ã‚¨ãƒƒã‚¸æ­£è¦åŒ–: {normalize_edge}")
        logger.info(f"   ç„¡å‘ã‚°ãƒ©ãƒ•: {undirected}")
        
    def load_graph_edges(self, edges_path: str) -> pd.DataFrame:
        """
        ã‚°ãƒ©ãƒ•ã‚¨ãƒƒã‚¸èª­ã¿è¾¼ã¿
        
        Args:
            edges_path: graph_edges.parquetã®ãƒ‘ã‚¹
            
        Returns:
            ã‚¨ãƒƒã‚¸DataFrame
        """
        logger.info(f"ğŸ“‚ ã‚°ãƒ©ãƒ•ã‚¨ãƒƒã‚¸èª­ã¿è¾¼ã¿: {edges_path}")
        
        if not Path(edges_path).exists():
            raise RuntimeError(f"âŒ CRITICAL: ã‚¨ãƒƒã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {edges_path}")
            
        try:
            df_edges = pd.read_parquet(edges_path)
            
            # å¿…è¦åˆ—ãƒã‚§ãƒƒã‚¯
            required_cols = ['src_id', 'dst_id', 'score']
            missing_cols = [col for col in required_cols if col not in df_edges.columns]
            if missing_cols:
                raise RuntimeError(f"âŒ CRITICAL: å¿…è¦ãªåˆ—ãŒä¸è¶³: {missing_cols}")
            
            # ãƒ‡ãƒ¼ã‚¿å‹ç¢ºä¿
            df_edges['src_id'] = df_edges['src_id'].astype(str)
            df_edges['dst_id'] = df_edges['dst_id'].astype(str)
            df_edges['score'] = df_edges['score'].astype(float)
            
            logger.info(f"âœ… ã‚¨ãƒƒã‚¸èª­ã¿è¾¼ã¿å®Œäº†: {len(df_edges):,} ã‚¨ãƒƒã‚¸")
            
            # åŸºæœ¬çµ±è¨ˆ
            unique_nodes = len(set(df_edges['src_id'].tolist() + df_edges['dst_id'].tolist()))
            score_stats = df_edges['score'].describe()
            
            logger.info(f"ğŸ“Š ã‚°ãƒ©ãƒ•çµ±è¨ˆ:")
            logger.info(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒãƒ¼ãƒ‰æ•°: {unique_nodes:,}")
            logger.info(f"   ã‚¹ã‚³ã‚¢çµ±è¨ˆ: min={score_stats['min']:.6f}, max={score_stats['max']:.6f}, mean={score_stats['mean']:.6f}")
            
            return df_edges
            
        except Exception as e:
            raise RuntimeError(f"âŒ CRITICAL: ã‚¨ãƒƒã‚¸èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            
    def load_user_ids(self, ids_path: str) -> List[str]:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿
        
        Args:
            ids_path: Step-1ã®index.jsonãƒ‘ã‚¹
            
        Returns:
            IDãƒªã‚¹ãƒˆ
        """
        logger.info(f"ğŸ“‚ ãƒ¦ãƒ¼ã‚¶ãƒ¼IDèª­ã¿è¾¼ã¿: {ids_path}")
        
        if not Path(ids_path).exists():
            raise RuntimeError(f"âŒ CRITICAL: IDãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {ids_path}")
            
        try:
            with open(ids_path, 'r', encoding='utf-8') as f:
                ids_data = json.load(f)
            
            # ãƒªã‚¹ãƒˆå½¢å¼ã‹ç¢ºèª
            if isinstance(ids_data, list):
                # å˜ç´”ãªãƒªã‚¹ãƒˆå½¢å¼ (ä¾‹: ["100", "101", ...])
                if len(ids_data) > 0 and isinstance(ids_data[0], (str, int)):
                    ids = [str(id_val) for id_val in ids_data]
                # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒªã‚¹ãƒˆå½¢å¼ (ä¾‹: [{"user_id": "100", "n_docs": 1}, ...])
                elif len(ids_data) > 0 and isinstance(ids_data[0], dict):
                    if 'user_id' in ids_data[0]:
                        ids = [str(item['user_id']) for item in ids_data]
                    else:
                        raise RuntimeError(f"âŒ CRITICAL: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå½¢å¼ã®å ´åˆã€'user_id'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒå¿…è¦ã§ã™")
                else:
                    raise RuntimeError(f"âŒ CRITICAL: ç©ºã®IDãƒªã‚¹ãƒˆã¾ãŸã¯æœªçŸ¥ã®å½¢å¼ã§ã™")
            else:
                raise RuntimeError(f"âŒ CRITICAL: IDãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒªã‚¹ãƒˆå½¢å¼ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
                
            logger.info(f"âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼IDèª­ã¿è¾¼ã¿å®Œäº†: {len(ids):,} ãƒ¦ãƒ¼ã‚¶ãƒ¼")
            return ids
            
        except Exception as e:
            raise RuntimeError(f"âŒ CRITICAL: ãƒ¦ãƒ¼ã‚¶ãƒ¼IDèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            
    def preprocess_edges(self, df_edges: pd.DataFrame) -> pd.DataFrame:
        """
        ã‚¨ãƒƒã‚¸å‰å‡¦ç†ï¼ˆé‡è¤‡çµ±åˆã€è‡ªå·±ãƒ«ãƒ¼ãƒ—é™¤å»ã€æ¤œè¨¼ï¼‰
        
        Args:
            df_edges: ç”Ÿã‚¨ãƒƒã‚¸DataFrame
            
        Returns:
            å‰å‡¦ç†æ¸ˆã¿ã‚¨ãƒƒã‚¸DataFrame
        """
        logger.info("ğŸ”„ ã‚¨ãƒƒã‚¸å‰å‡¦ç†é–‹å§‹")
        
        # å…ƒã®çµ±è¨ˆ
        original_edges = len(df_edges)
        
        # 1. ã‚¹ã‚³ã‚¢è² å€¤ãƒã‚§ãƒƒã‚¯
        negative_scores = df_edges['score'] < 0
        if negative_scores.any():
            n_negative = negative_scores.sum()
            logger.warning(f"âš ï¸ è² ã®ã‚¹ã‚³ã‚¢ã‚’æ¤œå‡º: {n_negative} ã‚¨ãƒƒã‚¸ã€0ã«ã‚¯ãƒªãƒƒãƒ—")
            df_edges.loc[negative_scores, 'score'] = 0.0
            
        # 2. è‡ªå·±ãƒ«ãƒ¼ãƒ—é™¤å»
        self_loops = df_edges['src_id'] == df_edges['dst_id']
        if self_loops.any():
            n_self_loops = self_loops.sum()
            logger.info(f"   è‡ªå·±ãƒ«ãƒ¼ãƒ—é™¤å»: {n_self_loops} ã‚¨ãƒƒã‚¸")
            df_edges = df_edges[~self_loops]
            
        # 3. é‡è¤‡ã‚¨ãƒƒã‚¸çµ±åˆï¼ˆæœ€å¤§ã‚¹ã‚³ã‚¢æ¡ç”¨ï¼‰
        logger.info("   é‡è¤‡ã‚¨ãƒƒã‚¸çµ±åˆä¸­...")
        df_edges = df_edges.groupby(['src_id', 'dst_id'], as_index=False)['score'].max()
        
        # 4. ç„¡å‘ã‚°ãƒ©ãƒ•åŒ–ï¼ˆæŒ‡å®šæ™‚ï¼‰
        if self.undirected:
            logger.info("   ç„¡å‘ã‚°ãƒ©ãƒ•åŒ–å®Ÿè¡Œä¸­...")
            
            # é€†å‘ãã‚¨ãƒƒã‚¸ç”Ÿæˆ
            df_reverse = df_edges.copy()
            df_reverse = df_reverse.rename(columns={'src_id': 'dst_id', 'dst_id': 'src_id'})
            
            # çµåˆãƒ»é‡è¤‡å‡¦ç†ï¼ˆå¹³å‡ã‚¹ã‚³ã‚¢æ¡ç”¨ï¼‰
            df_combined = pd.concat([df_edges, df_reverse], ignore_index=True)
            df_edges = df_combined.groupby(['src_id', 'dst_id'], as_index=False)['score'].mean()
            
        # 5. ã‚¼ãƒ­ã‚¹ã‚³ã‚¢ã‚¨ãƒƒã‚¸é™¤å»
        zero_scores = df_edges['score'] == 0.0
        if zero_scores.any():
            n_zero = zero_scores.sum()
            logger.info(f"   ã‚¼ãƒ­ã‚¹ã‚³ã‚¢ã‚¨ãƒƒã‚¸é™¤å»: {n_zero} ã‚¨ãƒƒã‚¸")
            df_edges = df_edges[~zero_scores]
            
        # å‰å‡¦ç†å¾Œçµ±è¨ˆ
        processed_edges = len(df_edges)
        unique_nodes = len(set(df_edges['src_id'].tolist() + df_edges['dst_id'].tolist()))
        
        logger.info(f"âœ… ã‚¨ãƒƒã‚¸å‰å‡¦ç†å®Œäº†:")
        logger.info(f"   å…ƒã‚¨ãƒƒã‚¸æ•°: {original_edges:,}")
        logger.info(f"   å‡¦ç†å¾Œã‚¨ãƒƒã‚¸æ•°: {processed_edges:,}")
        logger.info(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒãƒ¼ãƒ‰æ•°: {unique_nodes:,}")
        
        return df_edges
        
    def build_csr_matrix(self, df_edges: pd.DataFrame, user_ids: List[str]) -> Tuple[csr_matrix, Dict[str, int], Dict[int, str]]:
        """
        CSRç–éš£æ¥è¡Œåˆ—æ§‹ç¯‰
        
        Args:
            df_edges: å‰å‡¦ç†æ¸ˆã¿ã‚¨ãƒƒã‚¸DataFrame
            user_ids: å…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãƒªã‚¹ãƒˆ
            
        Returns:
            (CSRéš£æ¥è¡Œåˆ—, IDâ†’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒãƒƒãƒ—, ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹â†’IDãƒãƒƒãƒ—)
        """
        logger.info("ğŸ”§ CSRç–éš£æ¥è¡Œåˆ—æ§‹ç¯‰é–‹å§‹")
        
        # IDâ†’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒãƒƒãƒ”ãƒ³ã‚°æ§‹ç¯‰
        all_node_ids = set(user_ids)
        edge_node_ids = set(df_edges['src_id'].tolist() + df_edges['dst_id'].tolist())
        
        # ã‚¨ãƒƒã‚¸ã«å­˜åœ¨ã—ãªã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚‚å«ã‚ã‚‹ï¼ˆå­¤ç«‹ãƒãƒ¼ãƒ‰å¯¾å¿œï¼‰
        missing_in_edges = all_node_ids - edge_node_ids
        if missing_in_edges:
            logger.info(f"   ã‚¨ãƒƒã‚¸ã«å­˜åœ¨ã—ãªã„å­¤ç«‹ãƒãƒ¼ãƒ‰: {len(missing_in_edges):,}")
            
        # çµ±åˆãƒãƒ¼ãƒ‰ãƒªã‚¹ãƒˆä½œæˆ
        final_node_ids = sorted(list(all_node_ids))
        self.n_nodes = len(final_node_ids)
        
        # IDâ†â†’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒãƒƒãƒ”ãƒ³ã‚°
        self.id_to_idx = {node_id: i for i, node_id in enumerate(final_node_ids)}
        self.idx_to_id = {i: node_id for node_id, i in self.id_to_idx.items()}
        
        logger.info(f"   ãƒãƒ¼ãƒ‰ç·æ•°: {self.n_nodes:,}")
        logger.info(f"   ã‚¨ãƒƒã‚¸æ•°: {len(df_edges):,}")
        
        # CSRè¡Œåˆ—ãƒ‡ãƒ¼ã‚¿æº–å‚™
        rows = []
        cols = []
        data = []
        
        for _, edge in tqdm(df_edges.iterrows(), total=len(df_edges), desc="CSRæ§‹ç¯‰", unit="edges", leave=False):
            src_idx = self.id_to_idx.get(edge['src_id'])
            dst_idx = self.id_to_idx.get(edge['dst_id'])
            
            if src_idx is not None and dst_idx is not None:
                rows.append(src_idx)
                cols.append(dst_idx)
                data.append(edge['score'])
            else:
                logger.debug(f"æœªçŸ¥ãƒãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—: {edge['src_id']} -> {edge['dst_id']}")
                
        # CSRè¡Œåˆ—æ§‹ç¯‰
        try:
            adj_matrix = csr_matrix((data, (rows, cols)), shape=(self.n_nodes, self.n_nodes), dtype=np.float32)
            
            # è¡Œæ­£è¦åŒ–ï¼ˆç¢ºç‡é·ç§»è¡Œåˆ—åŒ–ï¼‰
            if self.normalize_edge:
                logger.info("   è¡Œæ­£è¦åŒ–å®Ÿè¡Œä¸­...")
                row_sums = np.array(adj_matrix.sum(axis=1)).flatten()
                
                # ã‚¼ãƒ­è¡Œï¼ˆå‡ºæ¬¡æ•°ãªã—ï¼‰ã®å‡¦ç†
                zero_rows = row_sums == 0
                n_zero_rows = zero_rows.sum()
                if n_zero_rows > 0:
                    logger.info(f"   å‡ºæ¬¡æ•°ã‚¼ãƒ­ãƒãƒ¼ãƒ‰: {n_zero_rows} (å…¨ãƒãƒ¼ãƒ‰ã«ç­‰ç¢ºç‡é·ç§»)")
                    # ã‚¼ãƒ­è¡Œã¯å‡ç­‰åˆ†å¸ƒã§ç½®ãæ›ãˆ
                    row_sums[zero_rows] = 1.0
                    
                # è¡Œæ­£è¦åŒ–
                row_sums_inv = 1.0 / row_sums
                row_sums_inv[~np.isfinite(row_sums_inv)] = 0.0
                
                # è¡Œæ­£è¦åŒ–é©ç”¨
                adj_matrix = adj_matrix.multiply(row_sums_inv[:, np.newaxis])
                
                # ã‚¼ãƒ­è¡Œã®å‡ç­‰åˆ†å¸ƒè¨­å®š
                if n_zero_rows > 0:
                    uniform_prob = 1.0 / self.n_nodes
                    # CSRå½¢å¼ã§ç›´æ¥è¨­å®šã¯ã§ããªã„ãŸã‚ã€COOå½¢å¼ã‚’ä½¿ç”¨
                    adj_matrix = adj_matrix.tocoo()
                    
                    # ã‚¼ãƒ­è¡Œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
                    zero_row_indices = np.where(zero_rows)[0]
                    
                    # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ
                    rows = adj_matrix.row.tolist()
                    cols = adj_matrix.col.tolist()
                    data = adj_matrix.data.tolist()
                    
                    # ã‚¼ãƒ­è¡Œã«å‡ç­‰åˆ†å¸ƒã‚’è¿½åŠ 
                    for row_idx in zero_row_indices:
                        for col_idx in range(self.n_nodes):
                            rows.append(row_idx)
                            cols.append(col_idx)
                            data.append(uniform_prob)
                    
                    # æ–°ã—ã„CSRè¡Œåˆ—ã‚’æ§‹ç¯‰
                    adj_matrix = csr_matrix((data, (rows, cols)), shape=(self.n_nodes, self.n_nodes), dtype=np.float32)
                        
        except Exception as e:
            raise RuntimeError(f"âŒ CRITICAL: CSRè¡Œåˆ—æ§‹ç¯‰å¤±æ•—: {e}")
            
        # è¡Œåˆ—çµ±è¨ˆ
        density = adj_matrix.nnz / (self.n_nodes * self.n_nodes)
        
        logger.info(f"âœ… CSRè¡Œåˆ—æ§‹ç¯‰å®Œäº†:")
        logger.info(f"   è¡Œåˆ—ã‚µã‚¤ã‚º: {adj_matrix.shape}")
        logger.info(f"   éã‚¼ãƒ­è¦ç´ : {adj_matrix.nnz:,}")
        logger.info(f"   å¯†åº¦: {density:.8f}")
        logger.info(f"   ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {adj_matrix.data.nbytes / 1024**2:.1f} MB")
        
        self.adj_matrix = adj_matrix
        return adj_matrix, self.id_to_idx, self.idx_to_id
        
    def compute_ppr(self, 
                   seed_nodes: Union[int, List[int]], 
                   topL: int = 50) -> Dict[int, np.ndarray]:
        """
        Power Iterationã«ã‚ˆã‚‹PPRè¨ˆç®—
        
        Args:
            seed_nodes: ã‚·ãƒ¼ãƒ‰ãƒãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆå˜ä¸€ã¾ãŸã¯ãƒªã‚¹ãƒˆï¼‰
            topL: ä¸Šä½Lå€‹ã‚’ä¿æŒ
            
        Returns:
            {seed_idx: ppr_vector} ã®è¾æ›¸
        """
        if self.adj_matrix is None:
            raise RuntimeError("âŒ CRITICAL: CSRè¡Œåˆ—ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
        # ã‚·ãƒ¼ãƒ‰ãƒãƒ¼ãƒ‰ãƒªã‚¹ãƒˆåŒ–
        if isinstance(seed_nodes, int):
            seed_nodes = [seed_nodes]
            
        logger.info(f"ğŸ§® PPRè¨ˆç®—é–‹å§‹: {len(seed_nodes)} ã‚·ãƒ¼ãƒ‰ãƒãƒ¼ãƒ‰")
        logger.info(f"   ä¸Šä½ä¿æŒæ•°: {topL}")
        
        results = {}
        
        for seed_idx in tqdm(seed_nodes, desc="PPRè¨ˆç®—", unit="seeds"):
            if seed_idx < 0 or seed_idx >= self.n_nodes:
                logger.warning(f"âš ï¸ ç„¡åŠ¹ã‚·ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {seed_idx}")
                continue
                
            # åˆæœŸåŒ–ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆã‚·ãƒ¼ãƒ‰ãƒãƒ¼ãƒ‰ã§1ã€ä»–ã¯0ï¼‰
            e = np.zeros(self.n_nodes, dtype=np.float32)
            e[seed_idx] = 1.0
            
            # PPRãƒ™ã‚¯ãƒˆãƒ«åˆæœŸåŒ–ï¼ˆå‡ç­‰åˆ†å¸ƒï¼‰
            p = np.ones(self.n_nodes, dtype=np.float32) / self.n_nodes
            
            # Power Iteration
            for iteration in range(self.max_iter):
                p_prev = p.copy()
                
                # p = Î± * e + (1 - Î±) * P^T * p
                # P^T * p ã¯è»¢ç½®è¡Œåˆ—ã¨ã®ç©ï¼ˆCSRã§ã¯ .T.dot() ã‚’ä½¿ç”¨ï¼‰
                p = self.alpha * e + (1 - self.alpha) * self.adj_matrix.T.dot(p)
                
                # L1æ­£è¦åŒ–ï¼ˆç¢ºç‡åˆ†å¸ƒã¨ã—ã¦ç¶­æŒï¼‰
                p = p / p.sum()
                
                # åæŸåˆ¤å®š
                l1_diff = np.sum(np.abs(p - p_prev))
                if l1_diff < self.eps:
                    logger.debug(f"  ã‚·ãƒ¼ãƒ‰ {seed_idx}: åå¾© {iteration+1} ã§åæŸ (L1å·®åˆ†: {l1_diff:.2e})")
                    break
            else:
                logger.warning(f"âš ï¸ ã‚·ãƒ¼ãƒ‰ {seed_idx}: æœ€å¤§åå¾©æ•° {self.max_iter} ã§æœªåæŸ")
                
            # ä¸Šä½Lå€‹ä¿æŒï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰
            if topL < self.n_nodes:
                top_indices = np.argpartition(p, -topL)[-topL:]
                top_indices = top_indices[np.argsort(p[top_indices])[::-1]]
                
                # ã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–
                p_sparse = np.zeros_like(p)
                p_sparse[top_indices] = p[top_indices]
                p = p_sparse
                
            results[seed_idx] = p
            
        logger.info(f"âœ… PPRè¨ˆç®—å®Œäº†: {len(results)} ã‚·ãƒ¼ãƒ‰å‡¦ç†æ¸ˆã¿")
        return results
        
    def compute_graph_statistics(self) -> Dict[str, Any]:
        """
        ã‚°ãƒ©ãƒ•çµ±è¨ˆæƒ…å ±è¨ˆç®—
        
        Returns:
            çµ±è¨ˆæƒ…å ±è¾æ›¸
        """
        if self.adj_matrix is None:
            raise RuntimeError("âŒ CRITICAL: CSRè¡Œåˆ—ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
        logger.info("ğŸ“Š ã‚°ãƒ©ãƒ•çµ±è¨ˆè¨ˆç®—ä¸­...")
        
        try:
            # åŸºæœ¬çµ±è¨ˆ
            n_nodes = self.adj_matrix.shape[0]
            n_edges = self.adj_matrix.nnz
            density = n_edges / (n_nodes * n_nodes) if n_nodes > 0 else 0.0
            
            # æ¬¡æ•°çµ±è¨ˆ
            out_degrees = np.array(self.adj_matrix.sum(axis=1)).flatten()
            in_degrees = np.array(self.adj_matrix.sum(axis=0)).flatten()
            
            # é€£çµæˆåˆ†æ•°
            n_components, labels = connected_components(
                self.adj_matrix, directed=not self.undirected, return_labels=True
            )
            
            # æœ€å¤§é€£çµæˆåˆ†ã‚µã‚¤ã‚º
            component_sizes = np.bincount(labels)
            largest_component_size = component_sizes.max() if len(component_sizes) > 0 else 0
            
            stats = {
                'n_nodes': int(n_nodes),
                'n_edges': int(n_edges),
                'density': float(density),
                'avg_out_degree': float(out_degrees.mean()),
                'avg_in_degree': float(in_degrees.mean()),
                'max_out_degree': int(out_degrees.max()),
                'max_in_degree': int(in_degrees.max()),
                'n_connected_components': int(n_components),
                'largest_component_size': int(largest_component_size),
                'largest_component_ratio': float(largest_component_size / n_nodes) if n_nodes > 0 else 0.0,
                'is_undirected': self.undirected,
                'is_normalized': self.normalize_edge,
            }
            
            logger.info(f"âœ… ã‚°ãƒ©ãƒ•çµ±è¨ˆå®Œäº†:")
            logger.info(f"   ãƒãƒ¼ãƒ‰æ•°: {stats['n_nodes']:,}")
            logger.info(f"   ã‚¨ãƒƒã‚¸æ•°: {stats['n_edges']:,}")
            logger.info(f"   å¹³å‡å‡ºæ¬¡æ•°: {stats['avg_out_degree']:.2f}")
            logger.info(f"   é€£çµæˆåˆ†æ•°: {stats['n_connected_components']}")
            logger.info(f"   æœ€å¤§æˆåˆ†æ¯”ç‡: {stats['largest_component_ratio']:.1%}")
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ ã‚°ãƒ©ãƒ•çµ±è¨ˆè¨ˆç®—å¤±æ•—: {e}")
            return {}
            
    def save_results(self, 
                    ppr_results: Dict[int, np.ndarray],
                    output_dir: str,
                    topL: int = 50,
                    graph_stats: Dict[str, Any] = None) -> Dict[str, str]:
        """
        çµæœä¿å­˜
        
        Args:
            ppr_results: PPRè¨ˆç®—çµæœ
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            topL: ä¸Šä½ä¿æŒæ•°
            graph_stats: ã‚°ãƒ©ãƒ•çµ±è¨ˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è¾æ›¸
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ’¾ çµæœä¿å­˜é–‹å§‹: {output_dir}")
        saved_files = {}
        
        # 1. CSRè¡Œåˆ—ä¿å­˜
        try:
            csr_path = output_path / "graph_csr.npz"
            save_npz(csr_path, self.adj_matrix)
            saved_files['csr'] = str(csr_path)
            logger.info(f"âœ… CSRè¡Œåˆ—ä¿å­˜å®Œäº†: {csr_path}")
        except Exception as e:
            logger.error(f"âŒ CSRè¡Œåˆ—ä¿å­˜å¤±æ•—: {e}")
            
        # 2. IDãƒãƒƒãƒ”ãƒ³ã‚°ä¿å­˜
        try:
            id_map_path = output_path / "id_map.json"
            id_mapping = {
                'id_to_idx': self.id_to_idx,
                'idx_to_id': self.idx_to_id,
                'n_nodes': self.n_nodes
            }
            with open(id_map_path, 'w', encoding='utf-8') as f:
                json.dump(id_mapping, f, ensure_ascii=False, indent=2)
            saved_files['id_map'] = str(id_map_path)
            logger.info(f"âœ… IDãƒãƒƒãƒ”ãƒ³ã‚°ä¿å­˜å®Œäº†: {id_map_path}")
        except Exception as e:
            logger.error(f"âŒ IDãƒãƒƒãƒ”ãƒ³ã‚°ä¿å­˜å¤±æ•—: {e}")
            
        # 3. PPRçµæœParquetä¿å­˜
        try:
            ppr_data = []
            
            for seed_idx, ppr_vector in ppr_results.items():
                seed_id = self.idx_to_id[seed_idx]
                
                # éã‚¼ãƒ­è¦ç´ ã®ã¿æŠ½å‡ºï¼ˆä¸Šä½Lå€‹ä¿æŒã®ãŸã‚ï¼‰
                nonzero_indices = np.nonzero(ppr_vector)[0]
                
                if len(nonzero_indices) == 0:
                    continue
                    
                # ã‚¹ã‚³ã‚¢é †ã‚½ãƒ¼ãƒˆ
                scores = ppr_vector[nonzero_indices]
                sorted_indices = np.argsort(scores)[::-1]
                
                for rank, idx in enumerate(sorted_indices[:topL]):
                    node_idx = nonzero_indices[idx]
                    node_id = self.idx_to_id[node_idx]
                    score = scores[idx]
                    
                    ppr_data.append({
                        'seed_id': seed_id,
                        'rank': rank + 1,
                        'node_id': node_id,
                        'score': float(score)
                    })
                    
            if ppr_data:
                df_ppr = pd.DataFrame(ppr_data)
                ppr_path = output_path / "ppr_topk.parquet"
                df_ppr.to_parquet(ppr_path, index=False)
                saved_files['ppr'] = str(ppr_path)
                logger.info(f"âœ… PPRçµæœä¿å­˜å®Œäº†: {ppr_path} ({len(ppr_data):,} ãƒ¬ã‚³ãƒ¼ãƒ‰)")
            else:
                logger.warning("âš ï¸ PPRçµæœãŒç©ºã®ãŸã‚ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                
        except Exception as e:
            logger.error(f"âŒ PPRçµæœä¿å­˜å¤±æ•—: {e}")
            
        # 4. ã‚°ãƒ©ãƒ•çµ±è¨ˆä¿å­˜
        if graph_stats:
            try:
                stats_path = output_path / "graph_stats.json"
                with open(stats_path, 'w', encoding='utf-8') as f:
                    json.dump(graph_stats, f, ensure_ascii=False, indent=2)
                saved_files['stats'] = str(stats_path)
                logger.info(f"âœ… ã‚°ãƒ©ãƒ•çµ±è¨ˆä¿å­˜å®Œäº†: {stats_path}")
            except Exception as e:
                logger.error(f"âŒ ã‚°ãƒ©ãƒ•çµ±è¨ˆä¿å­˜å¤±æ•—: {e}")
                
        logger.info(f"ğŸ¯ ä¿å­˜å®Œäº†: {len(saved_files)} ãƒ•ã‚¡ã‚¤ãƒ«")
        return saved_files


def parse_seed_nodes(seed_id: Optional[str] = None, 
                    seed_file: Optional[str] = None,
                    id_to_idx: Dict[str, int] = None) -> List[int]:
    """
    ã‚·ãƒ¼ãƒ‰ãƒãƒ¼ãƒ‰è§£æ
    
    Args:
        seed_id: å˜ä¸€ã‚·ãƒ¼ãƒ‰ID
        seed_file: ã‚·ãƒ¼ãƒ‰IDãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
        id_to_idx: IDâ†’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒãƒƒãƒ”ãƒ³ã‚°
        
    Returns:
        ã‚·ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚¹ãƒˆ
    """
    if seed_id:
        # å˜ä¸€ã‚·ãƒ¼ãƒ‰
        if seed_id not in id_to_idx:
            raise RuntimeError(f"âŒ CRITICAL: ã‚·ãƒ¼ãƒ‰ID '{seed_id}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return [id_to_idx[seed_id]]
        
    elif seed_file:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¤‡æ•°ã‚·ãƒ¼ãƒ‰
        if not Path(seed_file).exists():
            raise RuntimeError(f"âŒ CRITICAL: ã‚·ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {seed_file}")
            
        try:
            with open(seed_file, 'r', encoding='utf-8') as f:
                seed_ids = [line.strip() for line in f if line.strip()]
                
            seed_indices = []
            missing_ids = []
            
            for sid in seed_ids:
                if sid in id_to_idx:
                    seed_indices.append(id_to_idx[sid])
                else:
                    missing_ids.append(sid)
                    
            if missing_ids:
                logger.warning(f"âš ï¸ è¦‹ã¤ã‹ã‚‰ãªã„ã‚·ãƒ¼ãƒ‰ID: {len(missing_ids)} å€‹")
                for mid in missing_ids[:5]:  # æœ€åˆã®5å€‹è¡¨ç¤º
                    logger.warning(f"   - {mid}")
                    
            if not seed_indices:
                raise RuntimeError("âŒ CRITICAL: æœ‰åŠ¹ãªã‚·ãƒ¼ãƒ‰IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                
            return seed_indices
            
        except Exception as e:
            raise RuntimeError(f"âŒ CRITICAL: ã‚·ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
    else:
        # å…¨ãƒãƒ¼ãƒ‰ã‚’ã‚·ãƒ¼ãƒ‰ã¨ã™ã‚‹
        return list(range(len(id_to_idx)))


def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    parser = argparse.ArgumentParser(
        description="GraphRAGå‹•çš„CFS-Chameleonå‘ã‘Personalized PageRankè¨ˆç®— (Step-3)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # ç„¡å‘åŒ–ã—ã¦å…¨ä½“æ€§ã‚’å®‰å®šåŒ–ï¼ˆæ¨å¥¨ï¼‰
  python run_ppr.py \\
    --edges ./faiss/lamp2_users/graph_edges.parquet \\
    --ids ./embeddings/lamp2_users/embeddings_index.json \\
    --alpha 0.15 --eps 1e-6 --max_iter 50 --topL 50 \\
    --undirected --normalize_edge \\
    --output ./ppr_results

  # ç‰¹å®šãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ã‚·ãƒ¼ãƒ‰ã«å€‹åˆ¥PPR
  python run_ppr.py \\
    --edges ./faiss/lamp2_users/graph_edges.parquet \\
    --ids ./embeddings/lamp2_users/embeddings_index.json \\
    --alpha 0.2 --eps 1e-6 --max_iter 50 --topL 50 \\
    --undirected --normalize_edge \\
    --seed_id user_123 \\
    --output ./ppr_results_user123

  # è¤‡æ•°ã‚·ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®š
  python run_ppr.py \\
    --edges ./faiss/tenrec_users/graph_edges.parquet \\
    --ids ./embeddings/tenrec_users/embeddings_index.json \\
    --alpha 0.15 --eps 1e-6 --max_iter 50 --topL 100 \\
    --undirected --normalize_edge \\
    --seed_file ./seed_users.txt \\
    --output ./ppr_results_multi
        """
    )
    
    # å¿…é ˆå¼•æ•°
    parser.add_argument('--edges', type=str, required=True,
                        help='Step-2ã®graph_edges.parquetãƒ‘ã‚¹')
    parser.add_argument('--ids', type=str, required=True,
                        help='Step-1ã®ID index.jsonãƒ‘ã‚¹')
    parser.add_argument('--output', type=str, required=True,
                        help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    # PPRãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument('--alpha', type=float, default=0.15,
                        help='å†å§‹å‹•ç¢ºç‡ (default: 0.15)')
    parser.add_argument('--eps', type=float, default=1e-6,
                        help='åæŸåˆ¤å®šé–¾å€¤ (default: 1e-6)')
    parser.add_argument('--max_iter', type=int, default=50,
                        help='æœ€å¤§åå¾©å›æ•° (default: 50)')
    parser.add_argument('--topL', type=int, default=50,
                        help='å‡ºåŠ›ä¸Šä½Lå€‹ä¿æŒ (default: 50)')
    
    # ã‚°ãƒ©ãƒ•å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument('--normalize_edge', action='store_true', default=True,
                        help='ã‚¨ãƒƒã‚¸é‡ã¿è¡Œæ­£è¦åŒ–ï¼ˆç¢ºç‡é·ç§»è¡Œåˆ—åŒ–ï¼‰')
    parser.add_argument('--no_normalize_edge', action='store_false', dest='normalize_edge',
                        help='ã‚¨ãƒƒã‚¸é‡ã¿æ­£è¦åŒ–ã‚’ç„¡åŠ¹åŒ–')
    parser.add_argument('--undirected', action='store_true',
                        help='ç„¡å‘ã‚°ãƒ©ãƒ•åŒ–ï¼ˆåŒæ–¹å‘ã‚¨ãƒƒã‚¸ã€é‡ã¿å¹³å‡ï¼‰')
    
    # ã‚·ãƒ¼ãƒ‰æŒ‡å®š
    seed_group = parser.add_mutually_exclusive_group()
    seed_group.add_argument('--seed_id', type=str,
                           help='å˜ä¸€ã‚·ãƒ¼ãƒ‰ãƒãƒ¼ãƒ‰ID')
    seed_group.add_argument('--seed_file', type=str,
                           help='ã‚·ãƒ¼ãƒ‰IDãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ1è¡Œ1IDï¼‰')
    
    # ãã®ä»–
    parser.add_argument('--seed', type=int, default=42,
                        help='ä¹±æ•°ã‚·ãƒ¼ãƒ‰ (default: 42)')
    parser.add_argument('--verbose', action='store_true',
                        help='è©³ç´°ãƒ­ã‚°è¡¨ç¤º')
    
    args = parser.parse_args()
    
    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«è¨­å®š
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        
    try:
        # PPRè¨ˆç®—æ©ŸåˆæœŸåŒ–
        ppr_calc = PPRCalculator(
            alpha=args.alpha,
            eps=args.eps,
            max_iter=args.max_iter,
            normalize_edge=args.normalize_edge,
            undirected=args.undirected,
            seed=args.seed
        )
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        logger.info("ğŸ”„ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹")
        df_edges = ppr_calc.load_graph_edges(args.edges)
        user_ids = ppr_calc.load_user_ids(args.ids)
        
        # ã‚¨ãƒƒã‚¸å‰å‡¦ç†
        df_edges = ppr_calc.preprocess_edges(df_edges)
        
        # CSRè¡Œåˆ—æ§‹ç¯‰
        adj_matrix, id_to_idx, idx_to_id = ppr_calc.build_csr_matrix(df_edges, user_ids)
        
        # ã‚·ãƒ¼ãƒ‰ãƒãƒ¼ãƒ‰è§£æ
        seed_indices = parse_seed_nodes(args.seed_id, args.seed_file, id_to_idx)
        logger.info(f"ğŸ¯ ã‚·ãƒ¼ãƒ‰ãƒãƒ¼ãƒ‰: {len(seed_indices)} å€‹")
        
        # ã‚°ãƒ©ãƒ•çµ±è¨ˆè¨ˆç®—
        graph_stats = ppr_calc.compute_graph_statistics()
        
        # PPRè¨ˆç®—å®Ÿè¡Œ
        ppr_results = ppr_calc.compute_ppr(seed_indices, args.topL)
        
        # çµæœä¿å­˜
        saved_files = ppr_calc.save_results(
            ppr_results=ppr_results,
            output_dir=args.output,
            topL=args.topL,
            graph_stats=graph_stats
        )
        
        # å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ
        print(f"\nğŸ‰ PPRè¨ˆç®—å®Œäº†!")
        print(f"ğŸ“Š å‡¦ç†çµ±è¨ˆ:")
        print(f"   ã‚°ãƒ©ãƒ•: {graph_stats.get('n_nodes', 0):,} ãƒãƒ¼ãƒ‰, {graph_stats.get('n_edges', 0):,} ã‚¨ãƒƒã‚¸")
        print(f"   å†å§‹å‹•ç¢ºç‡: {args.alpha}")
        print(f"   ã‚·ãƒ¼ãƒ‰æ•°: {len(seed_indices):,}")
        print(f"   ä¸Šä½ä¿æŒ: {args.topL}")
        print(f"   ç„¡å‘ã‚°ãƒ©ãƒ•: {args.undirected}")
        print(f"   æ­£è¦åŒ–: {args.normalize_edge}")
        print(f"   å‡ºåŠ›å…ˆ: {args.output}")
        
        print(f"\nğŸ“ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«:")
        for file_type, file_path in saved_files.items():
            print(f"   {file_type}: {file_path}")
            
    except Exception as e:
        logger.error(f"âŒ å®Ÿè¡Œå¤±æ•—: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()