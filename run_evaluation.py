#!/usr/bin/env python3
"""
Chameleon LaMPè‡ªå‹•è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ  - å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Usage:
    python run_evaluation.py --mode demo      # ãƒ‡ãƒ¢å®Ÿè¡Œ (3ãƒ¦ãƒ¼ã‚¶ãƒ¼)
    python run_evaluation.py --mode full      # æœ¬æ ¼è©•ä¾¡ (10ãƒ¦ãƒ¼ã‚¶ãƒ¼)
    python run_evaluation.py --mode ablation  # ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶
    python run_evaluation.py --mode results   # çµæœç¢ºèª
"""

import argparse
import os
import sys
import time
import json
import subprocess
from pathlib import Path
from typing import Dict, List, Any
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('evaluation.log')
    ]
)
logger = logging.getLogger(__name__)

class EnvironmentChecker:
    """å®Ÿè¡Œç’°å¢ƒãƒã‚§ãƒƒã‚¯ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def check_python_version():
        """Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯"""
        version = sys.version_info
        if version.major < 3 or (version.major == 3 and version.minor < 8):
            raise RuntimeError(f"Python 3.8+ required, found {version.major}.{version.minor}")
        logger.info(f"âœ… Python version: {version.major}.{version.minor}.{version.micro}")
    
    @staticmethod
    def check_gpu():
        """CUDA/GPU åˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯"""
        try:
            import torch
            cuda_available = torch.cuda.is_available()
            if cuda_available:
                gpu_count = torch.cuda.device_count()
                gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else "Unknown"
                logger.info(f"âœ… CUDA available: {gpu_count} GPU(s) - {gpu_name}")
                return True
            else:
                logger.warning("âš ï¸  CUDA not available - using CPU (slower)")
                return False
        except ImportError:
            logger.error("âŒ PyTorch not installed")
            return False
    
    @staticmethod
    def check_memory():
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯"""
        try:
            import psutil
            memory = psutil.virtual_memory()
            available_gb = memory.available / (1024**3)
            total_gb = memory.total / (1024**3)
            
            logger.info(f"ğŸ’¾ Memory: {available_gb:.1f}GB available / {total_gb:.1f}GB total")
            
            if available_gb < 8:
                logger.warning("âš ï¸  Low memory available - consider closing other applications")
            
            return available_gb >= 4  # æœ€ä½4GBå¿…è¦
        except ImportError:
            logger.warning("Cannot check memory - psutil not installed")
            return True
    
    @staticmethod
    def check_dependencies():
        """ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯"""
        required_packages = [
            'torch', 'transformers', 'numpy', 'pandas', 'scikit-learn',
            'matplotlib', 'seaborn', 'scipy', 'yaml', 'nltk'
        ]
        
        missing_packages = []
        for package in required_packages:
            try:
                __import__(package)
                logger.info(f"âœ… {package} installed")
            except ImportError:
                missing_packages.append(package)
                logger.error(f"âŒ {package} not installed")
        
        if missing_packages:
            logger.error(f"Missing packages: {', '.join(missing_packages)}")
            logger.error("Install with: pip install " + " ".join(missing_packages))
            return False
        
        return True
    
    @staticmethod
    def check_data_files():
        """å¿…è¦ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª"""
        required_files = [
            "chameleon_prime_personalization/data/raw/LaMP-2/merged.json",
            "processed/LaMP-2/theta_p.json",
            "processed/LaMP-2/theta_n.json"
        ]
        
        missing_files = []
        for file_path in required_files:
            if not Path(file_path).exists():
                missing_files.append(file_path)
                logger.error(f"âŒ {file_path} not found")
            else:
                logger.info(f"âœ… {file_path} found")
        
        if missing_files:
            logger.error("Missing required data files:")
            for file_path in missing_files:
                logger.error(f"  - {file_path}")
            return False
        
        return True
    
    @classmethod
    def run_all_checks(cls):
        """å…¨ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""
        logger.info("ğŸ” Running environment checks...")
        
        checks = [
            ("Python version", cls.check_python_version),
            ("Dependencies", cls.check_dependencies),
            ("GPU/CUDA", cls.check_gpu),
            ("Memory", cls.check_memory),
            ("Data files", cls.check_data_files)
        ]
        
        for check_name, check_func in checks:
            try:
                result = check_func()
                if result is False:
                    logger.error(f"âŒ {check_name} check failed")
                    return False
            except Exception as e:
                logger.error(f"âŒ {check_name} check failed: {e}")
                return False
        
        logger.info("âœ… All environment checks passed!")
        return True

class EvaluationRunner:
    """è©•ä¾¡å®Ÿè¡Œç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.script_dir = Path(__file__).parent
        self.config_path = self.script_dir / "config.yaml"
        self.results_dir = self.script_dir / "results"
        self.results_dir.mkdir(exist_ok=True)
    
    def run_demo_evaluation(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¢è©•ä¾¡å®Ÿè¡Œ (3ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€ç´„5åˆ†)"""
        logger.info("ğŸš€ Starting demo evaluation...")
        logger.info("ğŸ“‹ Demo parameters:")
        logger.info("   - Users: 3")
        logger.info("   - Estimated time: 5-10 minutes")
        logger.info("   - Purpose: Quick system verification")
        
        return self._run_chameleon_evaluation(mode="demo")
    
    def run_full_evaluation(self) -> Dict[str, Any]:
        """æœ¬æ ¼è©•ä¾¡å®Ÿè¡Œ (10ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€ç´„30-60åˆ†)"""
        logger.info("ğŸš€ Starting full evaluation...")
        logger.info("ğŸ“‹ Full evaluation parameters:")
        logger.info("   - Users: 10")
        logger.info("   - Estimated time: 30-60 minutes")
        logger.info("   - Purpose: Complete research evaluation")
        
        # ç¢ºèªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        response = input("This will take 30-60 minutes. Continue? (y/N): ")
        if response.lower() != 'y':
            logger.info("Evaluation cancelled by user")
            return {}
        
        return self._run_chameleon_evaluation(mode="full")
    
    def run_ablation_study(self) -> Dict[str, Any]:
        """ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶å®Ÿè¡Œ"""
        logger.info("ğŸ”¬ Starting ablation study...")
        
        alpha_values = [0.5, 1.0, 1.5, 2.0]
        results = {}
        
        for alpha in alpha_values:
            logger.info(f"Running ablation with alpha_personal={alpha}")
            
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¸€æ™‚çš„ã«å¤‰æ›´
            config = self._load_config()
            config['chameleon']['alpha_personal'] = alpha
            self._save_temp_config(config)
            
            result = self._run_chameleon_evaluation(mode="ablation", config_override=True)
            results[f"alpha_{alpha}"] = result
        
        logger.info("âœ… Ablation study completed")
        return results
    
    def show_recent_results(self):
        """æœ€è¿‘ã®è©•ä¾¡çµæœã‚’è¡¨ç¤º"""
        logger.info("ğŸ“Š Recent evaluation results:")
        
        result_dirs = list(self.results_dir.glob("evaluation_*"))
        result_dirs.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        if not result_dirs:
            logger.info("No previous results found")
            return
        
        # æœ€æ–°ã®çµæœã‚’è¡¨ç¤º
        latest_result = result_dirs[0]
        result_file = latest_result / "results.json"
        
        if result_file.exists():
            with open(result_file, 'r') as f:
                results = json.load(f)
            
            self._display_results_summary(results)
        else:
            logger.info(f"Result file not found in {latest_result}")
    
    def _run_chameleon_evaluation(self, mode: str, config_override: bool = False) -> Dict[str, Any]:
        """Chameleonè©•ä¾¡å®Ÿè¡Œ"""
        try:
            from chameleon_evaluator import ChameleonEvaluator
            
            config_path = "temp_config.yaml" if config_override else str(self.config_path)
            evaluator = ChameleonEvaluator(config_path=config_path if Path(config_path).exists() else None)
            
            start_time = time.time()
            results = evaluator.run_evaluation(mode=mode)
            end_time = time.time()
            
            logger.info(f"âœ… Evaluation completed in {end_time - start_time:.1f} seconds")
            
            # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
            self._display_results_summary(results)
            
            return results
            
        except ImportError as e:
            logger.error(f"Failed to import chameleon_evaluator: {e}")
            return {}
        except Exception as e:
            logger.error(f"Evaluation failed: {e}")
            return {}
        finally:
            # ä¸€æ™‚è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            if config_override and Path("temp_config.yaml").exists():
                os.remove("temp_config.yaml")
    
    def _load_config(self) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        if self.config_path.exists():
            import yaml
            with open(self.config_path, 'r') as f:
                return yaml.safe_load(f)
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
            return {
                'model': {
                    'name': 'meta-llama/Llama-3.2-3B-Instruct',
                    'device': 'auto'
                },
                'chameleon': {
                    'alpha_personal': 1.5,
                    'alpha_general': -0.8
                },
                'evaluation': {
                    'max_users': 10
                }
            }
    
    def _save_temp_config(self, config: Dict[str, Any]):
        """ä¸€æ™‚è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜"""
        import yaml
        with open("temp_config.yaml", 'w') as f:
            yaml.dump(config, f)
    
    def _display_results_summary(self, results: Dict[str, Any]):
        """çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print("\n" + "=" * 60)
        print("ğŸ¯ Evaluation Results Summary")
        print("=" * 60)
        
        baseline = results.get('baseline', {})
        chameleon = results.get('chameleon', {})
        significance = results.get('significance', {})
        
        if baseline:
            print(f"\nğŸ“Š Baseline:")
            print(f"   Accuracy: {baseline.get('accuracy', 0):.3f}")
            print(f"   Exact Match: {baseline.get('exact_match', 0):.3f}")
            print(f"   BLEU Score: {baseline.get('bleu_score', 0):.3f}")
        
        if chameleon:
            print(f"\nğŸ¦ Chameleon:")
            print(f"   Accuracy: {chameleon.get('accuracy', 0):.3f}")
            print(f"   Exact Match: {chameleon.get('exact_match', 0):.3f}")
            print(f"   BLEU Score: {chameleon.get('bleu_score', 0):.3f}")
        
        if baseline and chameleon and significance:
            improvement = significance.get('improvement_rate', 0) * 100
            p_value = significance.get('p_value', 1.0)
            
            print(f"\nğŸ“ˆ Improvement:")
            print(f"   Rate: {improvement:+.1f}%")
            print(f"   p-value: {p_value:.4f}")
            
            if p_value < 0.05:
                print("   âœ… Statistically significant!")
            else:
                print("   âš ï¸  Not statistically significant")
        
        print("\n" + "=" * 60)

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="Chameleon LaMPè‡ªå‹•è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
å®Ÿè¡Œä¾‹:
  python run_evaluation.py --mode demo      # 3ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€5åˆ†ã®ãƒ‡ãƒ¢å®Ÿè¡Œ
  python run_evaluation.py --mode full      # 10ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€60åˆ†ã®æœ¬æ ¼è©•ä¾¡
  python run_evaluation.py --mode ablation  # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ„Ÿåº¦åˆ†æ
  python run_evaluation.py --mode results   # éå»ã®çµæœè¡¨ç¤º
        """
    )
    
    parser.add_argument(
        "--mode", 
        choices=["demo", "full", "ablation", "results"],
        default="demo",
        help="å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ (default: demo)"
    )
    
    parser.add_argument(
        "--skip-checks",
        action="store_true",
        help="ç’°å¢ƒãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—"
    )
    
    parser.add_argument(
        "--output-dir",
        type=str,
        default="./results",
        help="çµæœå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    
    args = parser.parse_args()
    
    print("ğŸ¦ Chameleon LaMPè‡ªå‹•è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 50)
    
    # ç’°å¢ƒãƒã‚§ãƒƒã‚¯
    if not args.skip_checks:
        if not EnvironmentChecker.run_all_checks():
            logger.error("âŒ Environment checks failed. Fix issues before proceeding.")
            sys.exit(1)
    
    # è©•ä¾¡å®Ÿè¡Œ
    runner = EvaluationRunner()
    
    try:
        if args.mode == "demo":
            results = runner.run_demo_evaluation()
        elif args.mode == "full":
            results = runner.run_full_evaluation()
        elif args.mode == "ablation":
            results = runner.run_ablation_study()
        elif args.mode == "results":
            runner.show_recent_results()
            return
        else:
            logger.error(f"Unknown mode: {args.mode}")
            sys.exit(1)
        
        if results:
            logger.info("ğŸ‰ Evaluation completed successfully!")
            logger.info(f"ğŸ“ Results saved in: {runner.results_dir}")
        else:
            logger.error("âŒ Evaluation failed or was cancelled")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  Evaluation interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ Unexpected error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()